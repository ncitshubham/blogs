{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Data Science Jobs on in.indeed.com\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- image: https://i.imgur.com/6zM7JBq.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed.com is a worldwide employment website for job listings. Globally, it hosts millions of job listings on thousands of jobs. In this project, we are interested in the 'Data Science' related job listings on https://in.indeed.com/ . Thus, we'll scrape the website for this information and save that to a csv file for future use. In order to do this, we'll use the following tools:\n",
    " - Python as the programming language\n",
    " - Requests library for downloading the webpage contents\n",
    " - BeautifulSoup library for finding and accessing the relevant information from the downloaded webpage.\n",
    " - Numpy library for handling missing values.\n",
    " - Pandas library for saving the accessed information to a csv file.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the steps we'll follow:\n",
    " \n",
    " \n",
    " - We'll scrape first 30 pages from https://in.indeed.com/jobs?q=data%20science\n",
    " - We'll get a list of all 15 jobs on each page.\n",
    " - For each job, we'll grab the Job Title, Salary, Location, Company Name, and Company Rating.\n",
    " - We'll save all of the information in a csv file in the following format:\n",
    " \n",
    " ```\n",
    " Title,Salary,Location,Company Name,Company Rating\n",
    " Data science,\"₹35,000 - ₹45,000 a month\",\"Mumbai, Maharashtra\",V Digitech Sevices\n",
    " Data Science ( 2 - 8 Yrs) - Remote,\"₹8,00,000 - ₹20,00,000 a year\",Remote,ProGrad\n",
    " Data Science Internship,\"₹10,000 a month\",\"Gurgaon, Haryana\",Zigram\n",
    " ```\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from numpy import nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up base URL and the user-agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The base_url is grabbed after searching 'Data Science' on in.indeed.com\n",
    "# The start value in the base_url will increment by 10 to access each following page.\n",
    "\n",
    "base_url = \"https://in.indeed.com/jobs?q=data%20science&start={}\"\n",
    "\n",
    "header = {\"User-Agent\": \"Mozilla/5.0\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary to save all information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = {\"Job Title\": [],\n",
    "        \"Salary\": [],\n",
    "        \"Location\": [],\n",
    "        \"Company Name\": [],\n",
    "        \"Company Rating\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the search result webpage.\n",
    "\n",
    "#### Download webpage and create a BeautifulSoup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    '''\n",
    "    This function will download the webpage for the url supplied as argument\n",
    "    and return the BeautifulSoup object for the webpage which can be used to \n",
    "    grab required information for the webpage.\n",
    "    '''\n",
    "    response = requests.get(url, \"html.parser\", headers = header)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(url))\n",
    "        \n",
    "    soup = BeautifulSoup(response.text)\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for `get_soup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = get_soup(base_url.format(10))\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a transform function\n",
    "Now, we'll create a transform function to grab the list of jobs from the result webpage.\n",
    "\n",
    "To do this, we'll pick `td` tags with the class: `resultContent`\n",
    "\n",
    "![](https://i.imgur.com/8MXbGpC.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(soup):\n",
    "    # find all the job listings on the webpage.\n",
    "    jobs_tags = soup.find_all(\"td\", class_ = \"resultContent\")\n",
    "    \n",
    "    # for each job, call helper functions to grab information about the job\n",
    "    # and save that to jobs dictionary.\n",
    "    for job in jobs_tags:\n",
    "        jobs[\"Job Title\"].append(get_job_title(job))\n",
    "        jobs[\"Salary\"].append(get_job_salary(job))\n",
    "        jobs[\"Location\"].append(get_company_location(job))\n",
    "        jobs[\"Company Name\"].append(get_company_name(job))\n",
    "        jobs[\"Company Rating\"].append(get_company_rating(job))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for finding the job tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "job_tags = soup.find_all(\"td\", class_ = \"resultContent\")\n",
    "print(len(job_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newData Analyst/ScientistSopra Steria3.5Noida, Uttar Pradesh\n"
     ]
    }
   ],
   "source": [
    "print(job_tags[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create helper functions\n",
    "Create helper functions to grab job information for each job and store that in the jobs dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First grab Job Title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Analyst/Scientist'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_job_title(job):\n",
    "    '''\n",
    "    Function to grab the job title.\n",
    "    Because some job titles have a prefix new in their job titles,\n",
    "    this function will automatically detect this prefix and return\n",
    "    the title sans 'new' in the job title.\n",
    "    '''\n",
    "    title = job.find(class_ = \"jobTitle\").text\n",
    "    if title[:3] == \"new\":\n",
    "        return title[3:]\n",
    "    else:\n",
    "        return title\n",
    "    \n",
    "get_job_title(job_tags[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll grab the job salary, if the listing has one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'₹500 an hour'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_job_salary(job):\n",
    "    \n",
    "    salary = job.find(\"div\", class_ = \"salary-snippet\")\n",
    "    if salary:\n",
    "        return salary.text\n",
    "    else:\n",
    "        return nan\n",
    "    \n",
    "get_job_salary(job_tags[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we'll grab the company name, location, and its rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sopra Steria\n",
      "Noida, Uttar Pradesh\n",
      "3.5\n"
     ]
    }
   ],
   "source": [
    "def get_company_name(job):\n",
    "    '''\n",
    "    Returns the company name for the supp'''\n",
    "    return job.find(class_ = \"companyName\").text\n",
    "\n",
    "    \n",
    "def get_company_location(job):\n",
    "    '''\n",
    "    Returns the company location for the supplied job tag\n",
    "    '''\n",
    "    return job.find(class_ = \"companyLocation\").text\n",
    "\n",
    "\n",
    "def get_company_rating(job):\n",
    "    '''\n",
    "    Returns the company rating for the supplied job tag\n",
    "    '''\n",
    "    rating = job.find(class_ = \"ratingNumber\")\n",
    "    if rating:\n",
    "        return float(rating.text)\n",
    "    else:\n",
    "        return nan\n",
    "\n",
    "# Example\n",
    "print(get_company_name(job_tags[0]), \n",
    "      get_company_location(job_tags[0]), \n",
    "      get_company_rating(job_tags[0]),\n",
    "     sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "We'll use a `for loop` to loop through 30 search result pages. Within this loop, we can apply the `get_soup` function to download these pages and the `transform` function to parse through all job listings from these pages and save the information in the `jobs` dictionary.\n",
    "We'll then use this dictionary to create a pandas DataFrame, which can then be saved to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 0...\n",
      "Scraping page 10...\n",
      "Scraping page 20...\n",
      "Scraping page 30...\n",
      "Scraping page 40...\n",
      "Scraping page 50...\n",
      "Scraping page 60...\n",
      "Scraping page 70...\n",
      "Scraping page 80...\n",
      "Scraping page 90...\n",
      "Scraping page 100...\n",
      "Scraping page 110...\n",
      "Scraping page 120...\n",
      "Scraping page 130...\n",
      "Scraping page 140...\n",
      "Scraping page 150...\n",
      "Scraping page 160...\n",
      "Scraping page 170...\n",
      "Scraping page 180...\n",
      "Scraping page 190...\n",
      "Scraping page 200...\n",
      "Scraping page 210...\n",
      "Scraping page 220...\n",
      "Scraping page 230...\n",
      "Scraping page 240...\n",
      "Scraping page 250...\n",
      "Scraping page 260...\n",
      "Scraping page 270...\n",
      "Scraping page 280...\n",
      "Scraping page 290...\n",
      "Scraping page 300...\n"
     ]
    }
   ],
   "source": [
    "for page in range(0, 310, 10):\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    soup = get_soup(base_url.format(page))\n",
    "    transform(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Company Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Technology Analyst: Data Science | Machine Lea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "      <td>Infosys Limited</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Analyst-Data Science</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gurgaon, Haryana+2 locations</td>\n",
       "      <td>Amex</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Junior Data Scientist Data Science Chennai, India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>Applied Data Finance</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Engineer – EPH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>India</td>\n",
       "      <td>Kyndryl</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Science Analyst</td>\n",
       "      <td>NaN</td>\n",
       "      <td>India</td>\n",
       "      <td>Helius Technologies</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title Salary  \\\n",
       "0  Technology Analyst: Data Science | Machine Lea...    NaN   \n",
       "1                               Analyst-Data Science    NaN   \n",
       "2  Junior Data Scientist Data Science Chennai, India    NaN   \n",
       "3                                Data Engineer – EPH    NaN   \n",
       "4                               Data Science Analyst    NaN   \n",
       "\n",
       "                       Location          Company Name  Company Rating  \n",
       "0          Bengaluru, Karnataka       Infosys Limited             3.9  \n",
       "1  Gurgaon, Haryana+2 locations                  Amex             NaN  \n",
       "2                    Tamil Nadu  Applied Data Finance             NaN  \n",
       "3                         India               Kyndryl             3.4  \n",
       "4                         India   Helius Technologies             NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# create a pandas DataFrame of the scraped data\n",
    "jobs_df = pd.DataFrame(jobs)\n",
    "\n",
    "jobs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save data to a csv file\n",
    "jobs_df.to_csv(\"Data_Science_jobs_from_indeed.com.csv\", index = None, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a short project, where we looked into how job listings can be scraped from Indeed.com. We craped 30 pages of job listings with tags `Data Science`. This gave us a total of 450 job listings with the details like the job title, salary, company, location, etc. We then saved this scraped data into a csv file for future use. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
