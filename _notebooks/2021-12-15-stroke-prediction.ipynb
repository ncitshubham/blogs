{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EDA of Stroke Dataset and Prediction of Strokes using Selected ML Algorithms\n\n- toc: true\n- badges: true","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n\nAccording to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\n\nIn this project, we'll try to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. To do this, we'll use the [Stroke Prediction Dataset](https://www.kaggle.com/fedesoriano/stroke-prediction-dataset) provided by [fedesoriano](https://www.kaggle.com/fedesoriano) on Kaggle.\n\nEach row in the dataset provides relavant information about the patient like age, smoking status, gender, heart disease, bmi, work type and in the end whether the patient suffered a stroke. This last parameter will be our target, which we'll try to predict using information from the other columns. \n\nThe steps that we'll take:\n- Setup and import the dataset\n- Perform basic EDA and prepare the dataset for training\n- Train and evaluate a baseline model\n- Train multiple ML models and make predictions.\n- Evaluate and compare their performance.","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{"execution":{"iopub.status.busy":"2022-03-12T02:15:43.698913Z","iopub.execute_input":"2022-03-12T02:15:43.699274Z","iopub.status.idle":"2022-03-12T02:15:43.721364Z","shell.execute_reply.started":"2022-03-12T02:15:43.699186Z","shell.execute_reply":"2022-03-12T02:15:43.720486Z"}}},{"cell_type":"code","source":"import numpy as np # Linear algebra\nimport pandas as pd # Data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Data Visualizatin\nimport seaborn as sns # Data Visualization\nfrom imblearn.over_sampling import SMOTE # Oversampling imbalanced classes\nfrom sklearn.impute import SimpleImputer, MissingIndicator # Handle missing values\nfrom sklearn.model_selection import train_test_split, cross_val_predict, cross_validate, GridSearchCV, StratifiedKFold \nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, RocCurveDisplay, PrecisionRecallDisplay, confusion_matrix, ConfusionMatrixDisplay # Metrics\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, FunctionTransformer\nfrom imblearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\n# Models for prediciton\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# get the file path\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-15T20:50:40.042661Z","iopub.execute_input":"2022-03-15T20:50:40.043627Z","iopub.status.idle":"2022-03-15T20:50:41.973113Z","shell.execute_reply.started":"2022-03-15T20:50:40.043529Z","shell.execute_reply":"2022-03-15T20:50:41.972092Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"file_path = \"/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\"","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:41.981397Z","iopub.execute_input":"2022-03-15T20:50:41.981643Z","iopub.status.idle":"2022-03-15T20:50:41.994241Z","shell.execute_reply.started":"2022-03-15T20:50:41.981613Z","shell.execute_reply":"2022-03-15T20:50:41.993181Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# file size of the dataset\n!ls -lh {file_path}","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:41.996950Z","iopub.execute_input":"2022-03-15T20:50:41.997529Z","iopub.status.idle":"2022-03-15T20:50:42.763582Z","shell.execute_reply.started":"2022-03-15T20:50:41.997478Z","shell.execute_reply":"2022-03-15T20:50:42.762717Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# look at the first few rows of the dataseet\n!head {file_path}","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:42.764939Z","iopub.execute_input":"2022-03-15T20:50:42.765798Z","iopub.status.idle":"2022-03-15T20:50:43.524142Z","shell.execute_reply.started":"2022-03-15T20:50:42.765757Z","shell.execute_reply":"2022-03-15T20:50:43.523102Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"**Observations**\n\n* File size is 310 KB, thus it is safe to import the whole dataset.\n* The delimiters are `,`\n* `id` is the index column\n* `stroke` is the prediction class in the last column","metadata":{}},{"cell_type":"code","source":"# Load the dataset into pandas DataFrame\ndf = pd.read_csv(file_path, index_col = [\"id\"])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:43.526483Z","iopub.execute_input":"2022-03-15T20:50:43.527534Z","iopub.status.idle":"2022-03-15T20:50:43.591090Z","shell.execute_reply.started":"2022-03-15T20:50:43.527479Z","shell.execute_reply":"2022-03-15T20:50:43.590393Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Basic EDA and Data Preparation\n\nFirst, it is really important to separate the test data from the train data at this point, so that the transformers and models cannot learn from the test data itself. Before making a split, it is worth looking at the distribution of prediction class, to see if there is an imbalance and whether we will need to stratify the split.","metadata":{"execution":{"iopub.status.busy":"2022-03-12T03:05:50.209851Z","iopub.execute_input":"2022-03-12T03:05:50.210124Z","iopub.status.idle":"2022-03-12T03:05:50.214502Z","shell.execute_reply.started":"2022-03-12T03:05:50.210095Z","shell.execute_reply":"2022-03-12T03:05:50.213376Z"}}},{"cell_type":"code","source":"# Distribution of prediction class\nstroke_val_count = df.stroke.value_counts()\nprint(f\"Value Count in the prediction class - Stroke:\\n{stroke_val_count}\\n\\n\")\n\nsns.barplot(x = stroke_val_count.index, y = stroke_val_count)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:43.592527Z","iopub.execute_input":"2022-03-15T20:50:43.592984Z","iopub.status.idle":"2022-03-15T20:50:43.799636Z","shell.execute_reply.started":"2022-03-15T20:50:43.592951Z","shell.execute_reply":"2022-03-15T20:50:43.798655Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the prediction class is highly imbalanced. Therefore, we'll need to stratify the split. Also, after making the split, it would be worth to generate artificial data in the training dataset to help the ML models distinguish better between the two categories of prediction class. ","metadata":{}},{"cell_type":"code","source":"# Separate the prediction class from the training features\nX = df.copy()\ny = X.pop(\"stroke\")\n\n# split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:43.800846Z","iopub.execute_input":"2022-03-15T20:50:43.801099Z","iopub.status.idle":"2022-03-15T20:50:43.816273Z","shell.execute_reply.started":"2022-03-15T20:50:43.801068Z","shell.execute_reply":"2022-03-15T20:50:43.815236Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"We'll perform EDA on the dataset and use the observations that we make for building a data preparation pipeline as the last step.\n\n#### Basic Inspection","metadata":{}},{"cell_type":"code","source":"# Look at the dataset\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:43.817689Z","iopub.execute_input":"2022-03-15T20:50:43.817935Z","iopub.status.idle":"2022-03-15T20:50:43.838479Z","shell.execute_reply.started":"2022-03-15T20:50:43.817904Z","shell.execute_reply":"2022-03-15T20:50:43.837781Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"X_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:43.841657Z","iopub.execute_input":"2022-03-15T20:50:43.842208Z","iopub.status.idle":"2022-03-15T20:50:43.863192Z","shell.execute_reply.started":"2022-03-15T20:50:43.842144Z","shell.execute_reply":"2022-03-15T20:50:43.862381Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**Observations**\n\n- There are 4088 entries in the train dataset.\n- There are total 10 features which we can use to predict the occurance of stroke.\n- There are some categorical features like `gender`, `work_type`, `Residence_type` of dtype `object`, which we'll need to be One Hot Encoded.\n- Numerical features will need to be scaled.\n- There are some missing values in the bmi column.","metadata":{}},{"cell_type":"markdown","source":"#### Null values","metadata":{}},{"cell_type":"code","source":"# Null values\nX_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:43.864565Z","iopub.execute_input":"2022-03-15T20:50:43.865055Z","iopub.status.idle":"2022-03-15T20:50:43.876311Z","shell.execute_reply.started":"2022-03-15T20:50:43.865021Z","shell.execute_reply":"2022-03-15T20:50:43.875407Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Distribution of null values in the dataset\nplt.figure(figsize = (10, 8))\nsns.heatmap(X_train.isnull(), cmap = \"rocket\", yticklabels = \"\", cbar = None)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:43.877897Z","iopub.execute_input":"2022-03-15T20:50:43.878205Z","iopub.status.idle":"2022-03-15T20:50:44.158595Z","shell.execute_reply.started":"2022-03-15T20:50:43.878149Z","shell.execute_reply":"2022-03-15T20:50:44.157554Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"null_bmi = y_train.loc[X_train.bmi.isna()]\nnot_null_bmi = y_train.loc[X_train.bmi.notnull()]\n\nprint(f\"\"\"Non null bmi values:-\n\nStroke-no stroke ratio: {null_bmi.sum()/len(null_bmi)}\n\n\nNull bmi values:-\n\nStroke-no stroke ratio: {not_null_bmi.sum()/len(not_null_bmi)}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:44.159783Z","iopub.execute_input":"2022-03-15T20:50:44.160035Z","iopub.status.idle":"2022-03-15T20:50:44.168686Z","shell.execute_reply.started":"2022-03-15T20:50:44.160005Z","shell.execute_reply":"2022-03-15T20:50:44.167759Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"**Observations**\n\nAlthough Null values look to be evenly distributed in the heatmap, the ratio for occurance of stroke is significatly different in the entries with null bmi values. Thus, instead of dropping the null values, it would be better to impute the null values with median bmi value, and also encode the presence of null values in a separate column. This may help in better prediction of stroke.","metadata":{}},{"cell_type":"markdown","source":"#### Categorical columns\n\nFirst it is worth inspecting the categories and their distributions in all categorical columns.","metadata":{}},{"cell_type":"code","source":"# Distribution of Categorical Features\ncat_cols = ['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n\nprint(\"Value Counts of all categorical columns...\\n\")\n\nfor i, col in enumerate(cat_cols):\n    val_count = X_train[col].value_counts()\n    print(f\"Values:-\\n{val_count}\\n\\n\")\n    plt.figure(i)\n    sns.barplot(x = val_count.index, y = val_count)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:44.169894Z","iopub.execute_input":"2022-03-15T20:50:44.170135Z","iopub.status.idle":"2022-03-15T20:50:45.476273Z","shell.execute_reply.started":"2022-03-15T20:50:44.170107Z","shell.execute_reply":"2022-03-15T20:50:45.475252Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"**Observation**\n\nThe categories in all the categorical features look ok, albeit most of the categories are unevenly distributed. Thus, we'll just one hot encode these columns.","metadata":{}},{"cell_type":"markdown","source":"#### Numerical Columns","metadata":{}},{"cell_type":"code","source":"# Look at the basic statistics\n\nnum_cols = [\"age\", \"avg_glucose_level\", \"bmi\"]\n\nX_train[num_cols].describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:45.477785Z","iopub.execute_input":"2022-03-15T20:50:45.478150Z","iopub.status.idle":"2022-03-15T20:50:45.504322Z","shell.execute_reply.started":"2022-03-15T20:50:45.478106Z","shell.execute_reply":"2022-03-15T20:50:45.503321Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"**Observations**\n\n- `Age` appears to be slightly negatively skewed\n- `avg_glucose_level` appears to be positively skewed\n- min age of 0.08 suggests that age is stored in fractions, which needs further inspection.","metadata":{}},{"cell_type":"code","source":"# Distribution of Numerical (Continuous) Features\n\nfor i, col in enumerate(num_cols):\n    plt.figure(i)\n    sns.violinplot(x = X_train[col])\n    plt.legend([f\"skewness: {X_train[col].skew():.2f}\\nkurtosis: {X_train[col].kurtosis():.2f}\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:45.505979Z","iopub.execute_input":"2022-03-15T20:50:45.506532Z","iopub.status.idle":"2022-03-15T20:50:46.088421Z","shell.execute_reply.started":"2022-03-15T20:50:45.506483Z","shell.execute_reply":"2022-03-15T20:50:46.087335Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"**Observations**\n\n- `age` is very slightly negatively skewed with negative kurtosis.\n- `avg_glucose_level` and `bmi` are positively skewed with sharp peaks(positive kurtosis)\n\nAlthough some ML models assume normal distribution of data, they can work fine with data with small skewness and kurtosis values. Therefore, we'll just scale this data using MinMaxScaler.","metadata":{}},{"cell_type":"code","source":"# inspect`age` column\nprint(sorted(X_train.age.unique()))","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:46.090087Z","iopub.execute_input":"2022-03-15T20:50:46.090446Z","iopub.status.idle":"2022-03-15T20:50:46.098745Z","shell.execute_reply.started":"2022-03-15T20:50:46.090409Z","shell.execute_reply":"2022-03-15T20:50:46.097729Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"**Observations**\n\nOnly values smaller than 2 are stored as floats. Thus, we can change the whole column to int datatype to make this feature uniform.","metadata":{}},{"cell_type":"markdown","source":"#### Build Column Transformer\n\nNow, we can use combine all the observations that we made to build a data preparation pipeline with a column transformer. We'll use the transformer to:\n- impute missing values\n- change `age` column dtype to `int`\n- add missing value indicator\n- onehotencode categorical columns\n- scale numerical features","metadata":{}},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:46.100347Z","iopub.execute_input":"2022-03-15T20:50:46.100944Z","iopub.status.idle":"2022-03-15T20:50:46.122553Z","shell.execute_reply.started":"2022-03-15T20:50:46.100896Z","shell.execute_reply":"2022-03-15T20:50:46.121599Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"X_train[20:]","metadata":{"execution":{"iopub.status.busy":"2022-03-15T21:31:25.550564Z","iopub.execute_input":"2022-03-15T21:31:25.550874Z","iopub.status.idle":"2022-03-15T21:31:25.581936Z","shell.execute_reply.started":"2022-03-15T21:31:25.550837Z","shell.execute_reply":"2022-03-15T21:31:25.580544Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"mi = MissingIndicator()\n\nmi.fit_transform(X_train)[:].sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T21:32:03.971761Z","iopub.execute_input":"2022-03-15T21:32:03.972405Z","iopub.status.idle":"2022-03-15T21:32:03.986198Z","shell.execute_reply.started":"2022-03-15T21:32:03.972340Z","shell.execute_reply":"2022-03-15T21:32:03.985298Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# Columns\nage_col = [\"age\"]\nnum_cols_without_age = [\"avg_glucose_level\", \"bmi\"]\ncat_cols = [\"gender\", \"ever_married\", \"work_type\", \"Residence_type\", \"smoking_status\"]\n\n# Scale the data\nscaler = MinMaxScaler()\n\n# handle missing values in `bmi`\nimputer = SimpleImputer(strategy = \"median\")\n\n# change dtype of `age`\ndef to_int(x):\n    return pd.DataFrame(x).astype(int)\n\nint_tr = FunctionTransformer(to_int)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:46.124237Z","iopub.execute_input":"2022-03-15T20:50:46.124824Z","iopub.status.idle":"2022-03-15T20:50:46.136692Z","shell.execute_reply.started":"2022-03-15T20:50:46.124776Z","shell.execute_reply":"2022-03-15T20:50:46.135524Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# build transformer for `age` separately\nage_transformer = make_pipeline(int_tr, scaler)\n\n# build transformer for numeric cols without age\nnum_transformer = make_pipeline(imputer, scaler)\n\n# build transformer for categorical cols\ncat_transformer = OneHotEncoder(drop = \"first\", handle_unknown = \"ignore\", sparse = False)\n\n# combine transformers to make a single column transformer\nct = make_column_transformer((age_transformer, age_col), (num_transformer, num_cols_without_age), (cat_transformer, cat_cols), remainder = \"passthrough\")","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:46.138039Z","iopub.execute_input":"2022-03-15T20:50:46.138392Z","iopub.status.idle":"2022-03-15T20:50:46.150409Z","shell.execute_reply.started":"2022-03-15T20:50:46.138361Z","shell.execute_reply":"2022-03-15T20:50:46.149429Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"#### SMOTE\n\nWe'll use imblearn's SMOTE over-sampling to balance the data. We'll implement this within the final pipeline with the training model.","metadata":{}},{"cell_type":"code","source":"sm = SMOTE(random_state = 42)\nsm","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:46.151510Z","iopub.execute_input":"2022-03-15T20:50:46.151736Z","iopub.status.idle":"2022-03-15T20:50:46.172102Z","shell.execute_reply.started":"2022-03-15T20:50:46.151703Z","shell.execute_reply":"2022-03-15T20:50:46.171086Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Train and Evaluate a Baseline Model","metadata":{}},{"cell_type":"markdown","source":"A baseline model which doesn't use any of the features to make predictions will give a baseline score, that the future ML models should at least beat. This score will help to identify errors in model training and evaluation if the models perform worse than the baseline score. ","metadata":{}},{"cell_type":"code","source":"# Build imblearn pipeline with the DummyClassifier\nbase_model = DummyClassifier(strategy = 'prior')\n\nbase_pipe = make_pipeline(ct, sm, base_model)\n\n# specify KFold strategy\ncv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:46.174096Z","iopub.execute_input":"2022-03-15T20:50:46.174454Z","iopub.status.idle":"2022-03-15T20:50:46.184149Z","shell.execute_reply.started":"2022-03-15T20:50:46.174409Z","shell.execute_reply":"2022-03-15T20:50:46.183203Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"#### Baseline scores","metadata":{}},{"cell_type":"code","source":"# Scores on multiple metrics from the DummyClassifier\ncross_validate(base_pipe, X_train, y_train, cv = cv, n_jobs = -1, scoring = [\"accuracy\", \"recall\", \"roc_auc\", \"f1\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-15T21:22:34.694798Z","iopub.execute_input":"2022-03-15T21:22:34.695632Z","iopub.status.idle":"2022-03-15T21:22:34.879706Z","shell.execute_reply.started":"2022-03-15T21:22:34.695583Z","shell.execute_reply":"2022-03-15T21:22:34.878949Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"The model accuracy comes out to be really great at about 0.95. Although, it would generally be a good score to achieve. in this domain, a good accuracy score can be decieving. We can actually confirm this by looking at the test recall scores. All the test recall scores are 0, which means that the model failed to catch even a single True Positive. This is what we almost never want in the medical sphere. If the testing isn't prohibitively expensive or risky for the patient, which in this case it isn't, the test should aim for a high recall score and not a high accuracy score. That is why we'll judge our models using the roc_auc score primarily, which shows the relation between the True Positive Rate(TPR/recall) and False Positive Rate(FPR) of the model. For this baseline model, the roc_auc score comes out to be 0.5, which means the model isn't able to distinguish between the prediction classes at any of the thresholds, which we would expect from a dummy model.\nWe can also look at the roc_auc_curve of the model.","metadata":{}},{"cell_type":"code","source":"# ROC curve of DummyClassifier\n\nbase_preds_prob = cross_val_predict(base_pipe, X_train, y_train, cv = cv, n_jobs = -1, method = \"predict_proba\")[:, 1]\n\nRocCurveDisplay.from_predictions(y_train, base_preds_prob)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T21:21:33.404402Z","iopub.execute_input":"2022-03-15T21:21:33.404689Z","iopub.status.idle":"2022-03-15T21:21:35.344506Z","shell.execute_reply.started":"2022-03-15T21:21:33.404660Z","shell.execute_reply":"2022-03-15T21:21:35.343213Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## Model Selection","metadata":{}},{"cell_type":"markdown","source":"In this section, we'll train multiple ML models typically used for binary classification and compare their performance using the scores from `cross_validate`","metadata":{}},{"cell_type":"code","source":"# Specify ML models\n\nmodels = {\"Logistic_Regression\": LogisticRegression(random_state = 42),\n        \"Ridge_Classification\": RidgeClassifier(random_state = 42),\n        \"SVC\": SVC(random_state = 42),\n        \"GaussianNB\": GaussianNB(),\n        \"KNClassifier\": KNeighborsClassifier(n_neighbors = 5),\n        \"RandomForestClassifier\": RandomForestClassifier(max_depth = 10, n_jobs = -1, random_state = 42),\n        \"XGBClassifier\": XGBClassifier(n_estimators = 50, learning_rate = 0.03, n_jobs = -1, \n                                       objective = \"binary:logistic\", eval_metric = \"auc\", tree_method = \"hist\", random_state = 42)}","metadata":{"execution":{"iopub.status.busy":"2022-03-15T21:20:27.896479Z","iopub.execute_input":"2022-03-15T21:20:27.896775Z","iopub.status.idle":"2022-03-15T21:20:27.904005Z","shell.execute_reply.started":"2022-03-15T21:20:27.896745Z","shell.execute_reply":"2022-03-15T21:20:27.903060Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Model comparison on multiple metrics\n\nscores = {}\n\nfor model_name, model in models.items():\n\n    model_pipe = make_pipeline(ct, sm, model)\n    \n    cross_val = cross_validate(model_pipe, X_train, y_train, cv = cv, scoring = [\"accuracy\", \"recall\", \"precision\", \"roc_auc\", \"f1\"])\n    del cross_val[\"fit_time\"]\n    del cross_val[\"score_time\"]\n    \n    print(model_name + \" :\\n\")\n    \n    for score_name, score_vals in cross_val.items():\n        score_mean = score_vals.mean()\n        score_std = score_vals.std()\n        cross_val[score_name] = score_mean\n        \n        print(f\"{score_name}: Mean: {(score_mean * 100):.2f} %   Std: {(score_std * 100):.2f}\\n\")\n        \n    print(\"\\n\", \"-\" * 50, \"\\n\")\n    \n    scores[model_name] = cross_val","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:50:48.497071Z","iopub.execute_input":"2022-03-15T20:50:48.497437Z","iopub.status.idle":"2022-03-15T20:51:04.959303Z","shell.execute_reply.started":"2022-03-15T20:50:48.497393Z","shell.execute_reply":"2022-03-15T20:51:04.958628Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"scores_df = pd.DataFrame.from_dict(scores, orient = 'index')\nscores_df.sort_values(by = \"test_roc_auc\", ascending = False)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:51:04.963407Z","iopub.execute_input":"2022-03-15T20:51:04.963840Z","iopub.status.idle":"2022-03-15T20:51:04.981574Z","shell.execute_reply.started":"2022-03-15T20:51:04.963807Z","shell.execute_reply":"2022-03-15T20:51:04.980557Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"From the scores we get, linear models although lacking in accuracy porformed way much better than the others in recall, auc_roc, and f1 metrics. Their roc_auc score of above 80s but low precision score suggests that they can do well with threshold tuning. The tree models like RandomForestClassifier and XGBClassifier also achieved satisfactory performance with high accuracy and moderate roc_auc scores. KNClassifier performed the worst and may require some hyperparameter tuning. But time would be better spent tuning the linear models to get even better performance than to tune the other ML models.","metadata":{}},{"cell_type":"markdown","source":"## Model Tuning\n\nWe'll train `LogisticRegression` and `RidgeClassifier` ML models with `GridSearchCV` to find the best model and its parameters. We can then train this model on the full train dataset and then make predictions on the test dataset in the next section. ","metadata":{}},{"cell_type":"code","source":"# Initialize models and specify param_grid for GridSearchCV\nmodels_and_params = {\"LogisticRegression\": [LogisticRegression(random_state = 42), \n                                            {\"logisticregression__class_weight\": [{0:1, 1: 1}, {0:1, 1:3}]}],\n                    \"RidgeClassification\": [RidgeClassifier(random_state = 42), \n                                            {\"ridgeclassifier__alpha\": [1, 2, 3], \"ridgeclassifier__class_weight\": [{0:1, 1: 1}, {0:1, 1:3}]}]}","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:51:04.983025Z","iopub.execute_input":"2022-03-15T20:51:04.983302Z","iopub.status.idle":"2022-03-15T20:51:04.989752Z","shell.execute_reply.started":"2022-03-15T20:51:04.983270Z","shell.execute_reply":"2022-03-15T20:51:04.988522Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Run GridSearchCV and store its results\ntuning_scores = []\nfor model_name in models_and_params:\n    model, params = models_and_params[model_name]\n    \n    model_pipe = make_pipeline(ct, sm, model)\n    \n    grid_cv = GridSearchCV(model_pipe, params, scoring = [\"recall\", \"precision\", \"roc_auc\", \"f1\"], n_jobs = -1, cv = cv, refit = False)\n    \n    grid_cv.fit(X_train, y_train)\n    \n    tuning_scores.append(grid_cv)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:51:04.991490Z","iopub.execute_input":"2022-03-15T20:51:04.991968Z","iopub.status.idle":"2022-03-15T20:51:06.339957Z","shell.execute_reply.started":"2022-03-15T20:51:04.991935Z","shell.execute_reply":"2022-03-15T20:51:06.338941Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Results for RidgeClassifier\nridge_classifier_grid_result = pd.DataFrame.from_dict(tuning_scores[1].cv_results_)\nridge_classifier_grid_result = ridge_classifier_grid_result[[\n                                                    \"param_ridgeclassifier__alpha\", \n                                                    \"param_ridgeclassifier__class_weight\", \n                                                    \"mean_test_precision\",\n                                                    \"mean_test_recall\",\n                                                    \"mean_test_roc_auc\",\n                                                    \"mean_test_f1\"]]\nridge_classifier_grid_result","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:51:06.342242Z","iopub.execute_input":"2022-03-15T20:51:06.342682Z","iopub.status.idle":"2022-03-15T20:51:06.364669Z","shell.execute_reply.started":"2022-03-15T20:51:06.342645Z","shell.execute_reply":"2022-03-15T20:51:06.364030Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Results for LogisticRegression\nlogistic_regression_grid_result = pd.DataFrame.from_dict(tuning_scores[0].cv_results_)\nlogistic_regression_grid_result = logistic_regression_grid_result[[\n                                                    \"param_logisticregression__class_weight\", \n                                                    \"mean_test_precision\",\n                                                    \"mean_test_recall\",\n                                                    \"mean_test_roc_auc\",\n                                                    \"mean_test_f1\"]]\nlogistic_regression_grid_result","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:51:06.365748Z","iopub.execute_input":"2022-03-15T20:51:06.366072Z","iopub.status.idle":"2022-03-15T20:51:06.386311Z","shell.execute_reply.started":"2022-03-15T20:51:06.366045Z","shell.execute_reply":"2022-03-15T20:51:06.385356Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"From these grid search, we have found that both the logistic regression and the ridge regression models are really close in performance on theire roc_auc score which is the primary metric, and that the changing class weights or alpha values doesn't help in improving the scores. From this, we can identify our ML model - RidgeClassifier with the default parameters of alpha 1 and class weghts 1:1. Now, we will train the final model and make predictions on the test dataset and then evaluate those predictions.","metadata":{}},{"cell_type":"markdown","source":"## Final Model and Predictions\n\nWe can now train a RidgeClassifier with the default parameters with a imblearn Pipeline and then make predictions on the test dataset.","metadata":{}},{"cell_type":"code","source":"# Model Training\nridge = RidgeClassifier(random_state = 42)\n\nridge_pipe = make_pipeline(ct, sm, ridge)\n\nridge_pipe.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:51:06.387448Z","iopub.execute_input":"2022-03-15T20:51:06.387668Z","iopub.status.idle":"2022-03-15T20:51:06.504838Z","shell.execute_reply.started":"2022-03-15T20:51:06.387643Z","shell.execute_reply":"2022-03-15T20:51:06.503588Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Predictions on test dataset\npreds = ridge_pipe.predict(X_test)\nConfusionMatrixDisplay.from_predictions(y_test, preds)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:51:06.506844Z","iopub.execute_input":"2022-03-15T20:51:06.509705Z","iopub.status.idle":"2022-03-15T20:51:06.830592Z","shell.execute_reply.started":"2022-03-15T20:51:06.509642Z","shell.execute_reply":"2022-03-15T20:51:06.829645Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Metrics for test predictions\nRocCurveDisplay.from_estimator(ridge_pipe, X_test, y_test, name = \"RidgeClassifier\")\nprint(\"Precision Score:\", precision_score(y_test, preds))\nprint(\"Recall Score:\", recall_score(y_test, preds))","metadata":{"execution":{"iopub.status.busy":"2022-03-15T20:59:46.791998Z","iopub.execute_input":"2022-03-15T20:59:46.792345Z","iopub.status.idle":"2022-03-15T20:59:47.048522Z","shell.execute_reply.started":"2022-03-15T20:59:46.792317Z","shell.execute_reply":"2022-03-15T20:59:47.047727Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"The classifier has predicted 712 True Negatives and 41 True Positives. There are 269 misclassifications too out of which 260 are False Positives. Considering that the predictions class was highly imbalanced to begin with, the model has worked really well in making correct classifications. Its AUC_ROC score on the test dataset is 0.84, similar to what we found on the train dataset. With more data, we can improve this score further. ","metadata":{}},{"cell_type":"markdown","source":"\n## Summary and Conclusion","metadata":{}},{"cell_type":"markdown","source":"In this project, we used the Stroke Dataset available on Kaggle to predict whether a patient would suffer from a stroke. First, we prepared the data for training and test by splitting it using train_test_split. Then we explored the data and understood where it needed some cleaning and preparation. With this knowledge, we developed imblearn's Pipelines to clean the data. We then explored multiple ML models and studied their performance through multiple metrics, primarily focusing on roc_auc scores. This choice of metrics was made with the knowledge that in the medicinal domain, the correct knowledge of True Positives is much more valuable than the wrong knowledge on False Positives.  We chose 2 linear models from this step for further model tuning and selection. RidgeClassifier turned out to be performing the best with its default parameters. We then trained this model and made predictions on the test data that we got from the split earlier. On the predictions, we achieved respectable scores on recall - 0.82 and on precision - 0.14. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}