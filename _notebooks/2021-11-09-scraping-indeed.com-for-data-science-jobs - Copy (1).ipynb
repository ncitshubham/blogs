{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wha fakdsfj  alsfjasl\n",
    "> \"Awesonononon sdfjls\"\n",
    "\n",
    "toc: true\n",
    "badges: true\n",
    "comments: true\n",
    "image: https://i.imgur.com/6zM7JBq.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Data Science Jobs on in.indeed.com\n",
    "\n",
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed.com is a worldwide employment website for job listings. Globally, it hosts millions of job listings on thousands of jobs. In this project, we are interested in the 'Data Science' related job listings on https://in.indeed.com/ . Thus, we'll scrape the website for this information and save that to a csv file for future use. In order to do this, we'll use the following tools:\n",
    " - Python as the programming language\n",
    " - Requests library for downloading the webpage contents\n",
    " - BeautifulSoup library for finding and accessing the relevant information from the downloaded webpage.\n",
    " - Numpy library for handling missing values.\n",
    " - Pandas library for saving the accessed information to a csv file.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the steps we'll follow:\n",
    " \n",
    " \n",
    " - We'll scrape first 10 pages from https://in.indeed.com/jobs?q=data%20science\n",
    " - We'll get a list of all 15 jobs on each page.\n",
    " - For each job, we'll grab the Job Title, Salary, Location, Company Name, and Company Rating.\n",
    " - We'll save all of the information in a csv file in the following format:\n",
    " \n",
    " ```\n",
    " Title,Salary,Location,Company Name,Company Rating\n",
    " Data science,\"₹35,000 - ₹45,000 a month\",\"Mumbai, Maharashtra\",V Digitech Sevices\n",
    " Data Science ( 2 - 8 Yrs) - Remote,\"₹8,00,000 - ₹20,00,000 a year\",Remote,ProGrad\n",
    " Data Science Internship,\"₹10,000 a month\",\"Gurgaon, Haryana\",Zigram\n",
    " ```\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from numpy import nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up base URL and the user-agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base_url is grabbed after searching 'Data Science' on in.indeed.com\n",
    "# The start value in the base_url will increment by 10 to access each following page.\n",
    "\n",
    "base_url = \"https://in.indeed.com/jobs?q=data%20science&start={}\"\n",
    "\n",
    "header = {\"User-Agent\": \"Mozilla/5.0\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary to save all information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = {\"Job Title\": [],\n",
    "        \"Salary\": [],\n",
    "        \"Location\": [],\n",
    "        \"Company Name\": [],\n",
    "        \"Company Rating\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the search result webpage.\n",
    "\n",
    "#### Download webpage and create a BeautifulSoup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    '''\n",
    "    This function will download the webpage for the url supplied as argument\n",
    "    and return the BeautifulSoup object for the webpage which can be used to \n",
    "    grab required information for the webpage.\n",
    "    '''\n",
    "    response = requests.get(url, \"html.parser\", headers = header)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(url))\n",
    "        \n",
    "    soup = BeautifulSoup(response.text)\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for `get_soup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = get_soup(base_url.format(10))\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a transform function\n",
    "Now, we'll create a transform function to grab the list of jobs from the result webpage.\n",
    "\n",
    "To do this, we'll pick `td` tags with the class: `resultContent`\n",
    "\n",
    "![](https://i.imgur.com/8MXbGpC.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(soup):\n",
    "    # find all the job listings on the webpage.\n",
    "    jobs_tags = soup.find_all(\"td\", class_ = \"resultContent\")\n",
    "    \n",
    "    # for each job, call helper functions to grab information about the job\n",
    "    # and save that to jobs dictionary.\n",
    "    for job in jobs_tags:\n",
    "        jobs[\"Job Title\"].append(get_job_title(job))\n",
    "        jobs[\"Salary\"].append(get_job_salary(job))\n",
    "        jobs[\"Location\"].append(get_company_location(job))\n",
    "        jobs[\"Company Name\"].append(get_company_name(job))\n",
    "        jobs[\"Company Rating\"].append(get_company_rating(job))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example for finding the job tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "job_tags = soup.find_all(\"td\", class_ = \"resultContent\")\n",
    "print(len(job_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newData Scientist IIMasterCard4.1Gurgaon, Haryana\n"
     ]
    }
   ],
   "source": [
    "print(job_tags[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create helper functions\n",
    "Create helper functions to grab job information for each job and store that in the jobs dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First grab Job Title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Scientist II'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_job_title(job):\n",
    "    '''\n",
    "    Function to grab the job title.\n",
    "    Because some job titles have a prefix new in their job titles,\n",
    "    this function will automatically detect this prefix and return\n",
    "    the title sans 'new' in the job title.\n",
    "    '''\n",
    "    title = job.find(class_ = \"jobTitle\").text\n",
    "    if title[:3] == \"new\":\n",
    "        return title[3:]\n",
    "    else:\n",
    "        return title\n",
    "    \n",
    "get_job_title(job_tags[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll grab the job salary, if the listing has one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_job_salary(job):\n",
    "    \n",
    "    salary = job.find(\"div\", class_ = \"salary-snippet\")\n",
    "    if salary:\n",
    "        return salary.text\n",
    "    else:\n",
    "        return nan\n",
    "    \n",
    "get_job_salary(job_tags[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we'll grab the company name, location, and its rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MasterCard\n",
      "Gurgaon, Haryana\n",
      "4.1\n"
     ]
    }
   ],
   "source": [
    "def get_company_name(job):\n",
    "    '''\n",
    "    Returns the company name for the supp'''\n",
    "    return job.find(class_ = \"companyName\").text\n",
    "\n",
    "    \n",
    "def get_company_location(job):\n",
    "    '''\n",
    "    Returns the company location for the supplied job tag\n",
    "    '''\n",
    "    return job.find(class_ = \"companyLocation\").text\n",
    "\n",
    "\n",
    "def get_company_rating(job):\n",
    "    '''\n",
    "    Returns the company rating for the supplied job tag\n",
    "    '''\n",
    "    rating = job.find(class_ = \"ratingNumber\")\n",
    "    if rating:\n",
    "        return float(rating.text)\n",
    "    else:\n",
    "        return nan\n",
    "\n",
    "\n",
    "print(get_company_name(job_tags[0]), \n",
    "      get_company_location(job_tags[0]), \n",
    "      get_company_rating(job_tags[0]),\n",
    "     sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "We'll use a `for loop` to loop through 10 search result pages. Within this loop, we can apply the `get_soup` function to download these pages and the `transform` function to parse through all job listings from these pages and save the information in the `jobs` dictionary.\n",
    "We'll then use this dictionary to create a pandas DataFrame, which can then be saved to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 0...\n",
      "Scraping page 10...\n",
      "Scraping page 20...\n",
      "Scraping page 30...\n",
      "Scraping page 40...\n",
      "Scraping page 50...\n",
      "Scraping page 60...\n",
      "Scraping page 70...\n",
      "Scraping page 80...\n",
      "Scraping page 90...\n",
      "Scraping page 100...\n"
     ]
    }
   ],
   "source": [
    "for page in range(0, 110, 10):\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    soup = get_soup(base_url.format(page))\n",
    "    transform(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df = pd.DataFrame(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Company Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analyst-Data Science</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bengaluru, Karnataka+1 location</td>\n",
       "      <td>Amex</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>₹12,00,000 - ₹18,00,000 a year</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "      <td>Onward Group</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist - ML</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "      <td>JPMorgan Chase Bank, N.A.</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pune, Maharashtra+1 location</td>\n",
       "      <td>Maersk</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gurgaon, Haryana+3 locations</td>\n",
       "      <td>NatWest Group</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Job Title                          Salary  \\\n",
       "0  Analyst-Data Science                             NaN   \n",
       "1          Data Science  ₹12,00,000 - ₹18,00,000 a year   \n",
       "2   Data Scientist - ML                             NaN   \n",
       "3        Data Scientist                             NaN   \n",
       "4         Data Engineer                             NaN   \n",
       "\n",
       "                          Location               Company Name  Company Rating  \n",
       "0  Bengaluru, Karnataka+1 location                       Amex             4.1  \n",
       "1             Bengaluru, Karnataka               Onward Group             NaN  \n",
       "2             Bengaluru, Karnataka  JPMorgan Chase Bank, N.A.             3.9  \n",
       "3     Pune, Maharashtra+1 location                     Maersk             4.1  \n",
       "4     Gurgaon, Haryana+3 locations              NatWest Group             3.3  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df.to_csv(\"Data_Science_jobs_from_indeed.com.csv\", index = None, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
