{
  
    
        "post0": {
            "title": "EDA and Data Visualization of Zomato Bangalore Restuarants Dataset",
            "content": ". Introduction . Zomato is an Indian multinational restaurant aggregator and food delivery company. In this project, we&#39;re going to study and analyze the Zomato Dataset shared by Himanshu Podder on Kaggle. This dataset contains information on restaurants in the city of Bengaluru, India. We can use this dataset to get an idea of different factors affecting the restaurants in different parts of the city and also answer questions like which type of food is most popular in the city, how does the location of the restuarant affects its rating on the Zomato platform, and what relation does the rating of the restaurant and the number of cuisines it offers has? . We will use the approach of Exploratory Data Analysis (EDA) for studying this data, which is used to analyse datasets to summarize their main characteristics, often using statistical graphics and other data visualization methods. EDA can help in seeing what the data can tell us beyond the formal modeling or hypothesis testing task.3 . The dataset can be used to answer a lot of questions but for this project, we are going to limit ourselves to 5-10 questions and then try to answer these questions. . Setup . Import the required libraries and get the file path. . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # Data Visualization import seaborn as sns # Data Visualization import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . /kaggle/input/zomato-bangalore-restaurants/zomato.csv . file_path = &quot;/kaggle/input/zomato-bangalore-restaurants/zomato.csv&quot; !ls -lh {file_path} . -rw-r--r-- 1 nobody nogroup 548M Feb 1 14:14 /kaggle/input/zomato-bangalore-restaurants/zomato.csv . The file size of the dataset is 548MB and it is safe to import the whole dataset at once. . # Read the csv file into a pandas DataFrame df = pd.read_csv(file_path, thousands = &#39;,&#39;) . Data Preparation and Cleaning . Premilinary Analysis . Evaluate the structure of the dataset . df.head() . url address name online_order book_table rate votes phone location rest_type dish_liked cuisines approx_cost(for two people) reviews_list menu_item listed_in(type) listed_in(city) . 0 https://www.zomato.com/bangalore/jalsa-banasha... | 942, 21st Main Road, 2nd Stage, Banashankari, ... | Jalsa | Yes | Yes | 4.1/5 | 775 | 080 42297555 r n+91 9743772233 | Banashankari | Casual Dining | Pasta, Lunch Buffet, Masala Papad, Paneer Laja... | North Indian, Mughlai, Chinese | 800.0 | [(&#39;Rated 4.0&#39;, &#39;RATED n A beautiful place to ... | [] | Buffet | Banashankari | . 1 https://www.zomato.com/bangalore/spice-elephan... | 2nd Floor, 80 Feet Road, Near Big Bazaar, 6th ... | Spice Elephant | Yes | No | 4.1/5 | 787 | 080 41714161 | Banashankari | Casual Dining | Momos, Lunch Buffet, Chocolate Nirvana, Thai G... | Chinese, North Indian, Thai | 800.0 | [(&#39;Rated 4.0&#39;, &#39;RATED n Had been here for din... | [] | Buffet | Banashankari | . 2 https://www.zomato.com/SanchurroBangalore?cont... | 1112, Next to KIMS Medical College, 17th Cross... | San Churro Cafe | Yes | No | 3.8/5 | 918 | +91 9663487993 | Banashankari | Cafe, Casual Dining | Churros, Cannelloni, Minestrone Soup, Hot Choc... | Cafe, Mexican, Italian | 800.0 | [(&#39;Rated 3.0&#39;, &quot;RATED n Ambience is not that ... | [] | Buffet | Banashankari | . 3 https://www.zomato.com/bangalore/addhuri-udupi... | 1st Floor, Annakuteera, 3rd Stage, Banashankar... | Addhuri Udupi Bhojana | No | No | 3.7/5 | 88 | +91 9620009302 | Banashankari | Quick Bites | Masala Dosa | South Indian, North Indian | 300.0 | [(&#39;Rated 4.0&#39;, &quot;RATED n Great food and proper... | [] | Buffet | Banashankari | . 4 https://www.zomato.com/bangalore/grand-village... | 10, 3rd Floor, Lakshmi Associates, Gandhi Baza... | Grand Village | No | No | 3.8/5 | 166 | +91 8026612447 r n+91 9901210005 | Basavanagudi | Casual Dining | Panipuri, Gol Gappe | North Indian, Rajasthani | 600.0 | [(&#39;Rated 4.0&#39;, &#39;RATED n Very good restaurant ... | [] | Buffet | Banashankari | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 51717 entries, 0 to 51716 Data columns (total 17 columns): # Column Non-Null Count Dtype -- -- 0 url 51717 non-null object 1 address 51717 non-null object 2 name 51717 non-null object 3 online_order 51717 non-null object 4 book_table 51717 non-null object 5 rate 43942 non-null object 6 votes 51717 non-null int64 7 phone 50509 non-null object 8 location 51696 non-null object 9 rest_type 51490 non-null object 10 dish_liked 23639 non-null object 11 cuisines 51672 non-null object 12 approx_cost(for two people) 51371 non-null float64 13 reviews_list 51717 non-null object 14 menu_item 51717 non-null object 15 listed_in(type) 51717 non-null object 16 listed_in(city) 51717 non-null object dtypes: float64(1), int64(1), object(15) memory usage: 6.7+ MB . Change the structure . We&#39;ll drop the columns which don&#39;t inform us much about the restuarants. . df.drop([&quot;url&quot;, &quot;name&quot;, &quot;phone&quot;, &quot;reviews_list&quot;, &quot;address&quot;, &quot;menu_item&quot;], axis = 1, inplace = True) df.columns . Index([&#39;online_order&#39;, &#39;book_table&#39;, &#39;rate&#39;, &#39;votes&#39;, &#39;location&#39;, &#39;rest_type&#39;, &#39;dish_liked&#39;, &#39;cuisines&#39;, &#39;approx_cost(for two people)&#39;, &#39;listed_in(type)&#39;, &#39;listed_in(city)&#39;], dtype=&#39;object&#39;) . We&#39;ll also rename some of the columns. . df.rename(mapper = {&quot;listed_in(type)&quot;: &quot;type&quot;, &quot;approx_cost(for two people)&quot;: &quot;cost_for_two_people&quot;, &quot;rate&quot;: &quot;rating&quot;}, axis = 1, inplace = True) df.columns . Index([&#39;online_order&#39;, &#39;book_table&#39;, &#39;rating&#39;, &#39;votes&#39;, &#39;location&#39;, &#39;rest_type&#39;, &#39;dish_liked&#39;, &#39;cuisines&#39;, &#39;cost_for_two_people&#39;, &#39;type&#39;, &#39;listed_in(city)&#39;], dtype=&#39;object&#39;) . Correcting datatypes . Now we&#39;ll look at the dtypes of the DataFrame and see if that needs any change. . df.dtypes . online_order object book_table object rating object votes int64 location object rest_type object dish_liked object cuisines object cost_for_two_people float64 type object listed_in(city) object dtype: object . We need to change the rating column to numeric dtype. . # All distinct values in the `rating` column df.rating.unique() . array([&#39;4.1/5&#39;, &#39;3.8/5&#39;, &#39;3.7/5&#39;, &#39;3.6/5&#39;, &#39;4.6/5&#39;, &#39;4.0/5&#39;, &#39;4.2/5&#39;, &#39;3.9/5&#39;, &#39;3.1/5&#39;, &#39;3.0/5&#39;, &#39;3.2/5&#39;, &#39;3.3/5&#39;, &#39;2.8/5&#39;, &#39;4.4/5&#39;, &#39;4.3/5&#39;, &#39;NEW&#39;, &#39;2.9/5&#39;, &#39;3.5/5&#39;, nan, &#39;2.6/5&#39;, &#39;3.8 /5&#39;, &#39;3.4/5&#39;, &#39;4.5/5&#39;, &#39;2.5/5&#39;, &#39;2.7/5&#39;, &#39;4.7/5&#39;, &#39;2.4/5&#39;, &#39;2.2/5&#39;, &#39;2.3/5&#39;, &#39;3.4 /5&#39;, &#39;-&#39;, &#39;3.6 /5&#39;, &#39;4.8/5&#39;, &#39;3.9 /5&#39;, &#39;4.2 /5&#39;, &#39;4.0 /5&#39;, &#39;4.1 /5&#39;, &#39;3.7 /5&#39;, &#39;3.1 /5&#39;, &#39;2.9 /5&#39;, &#39;3.3 /5&#39;, &#39;2.8 /5&#39;, &#39;3.5 /5&#39;, &#39;2.7 /5&#39;, &#39;2.5 /5&#39;, &#39;3.2 /5&#39;, &#39;2.6 /5&#39;, &#39;4.5 /5&#39;, &#39;4.3 /5&#39;, &#39;4.4 /5&#39;, &#39;4.9/5&#39;, &#39;2.1/5&#39;, &#39;2.0/5&#39;, &#39;1.8/5&#39;, &#39;4.6 /5&#39;, &#39;4.9 /5&#39;, &#39;3.0 /5&#39;, &#39;4.8 /5&#39;, &#39;2.3 /5&#39;, &#39;4.7 /5&#39;, &#39;2.4 /5&#39;, &#39;2.1 /5&#39;, &#39;2.2 /5&#39;, &#39;2.0 /5&#39;, &#39;1.8 /5&#39;], dtype=object) . # Remove the non-desired values from the rating column df = df.loc[df.rating != &quot;NEW&quot;] df = df.loc[df.rating != &quot;-&quot;] # Select the first 3 characters and convert the column to numeric df.rating = pd.to_numeric(df.rating.str[:3]) . df.rating.head() . 0 4.1 1 4.1 2 3.8 3 3.7 4 3.8 Name: rating, dtype: float64 . Deal with null values . # Number of null values df.isnull().sum().sort_values(ascending = False) . dish_liked 25948 rating 7775 cost_for_two_people 341 rest_type 225 cuisines 45 location 21 online_order 0 book_table 0 votes 0 type 0 listed_in(city) 0 dtype: int64 . It appears that in all the columns with null values, absence of values neither indicates the value of zero nor informs us on something useful. Thus, it&#39;s better to drop the rows with null values. In the dish_liked column, because the null values account for about half of the data, it&#39;s better to drop the whole column. . df.dropna(subset = [&quot;location&quot;, &quot;rating&quot;, &quot;rest_type&quot;, &quot;cuisines&quot;, &quot;cost_for_two_people&quot;], inplace = True) df.drop([&quot;dish_liked&quot;], axis = 1, inplace = True) df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 41263 entries, 0 to 51716 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 online_order 41263 non-null object 1 book_table 41263 non-null object 2 rating 41263 non-null float64 3 votes 41263 non-null int64 4 location 41263 non-null object 5 rest_type 41263 non-null object 6 cuisines 41263 non-null object 7 cost_for_two_people 41263 non-null float64 8 type 41263 non-null object 9 listed_in(city) 41263 non-null object dtypes: float64(2), int64(1), object(7) memory usage: 3.5+ MB . df.head() . online_order book_table rating votes location rest_type cuisines cost_for_two_people type listed_in(city) . 0 Yes | Yes | 4.1 | 775 | Banashankari | Casual Dining | North Indian, Mughlai, Chinese | 800.0 | Buffet | Banashankari | . 1 Yes | No | 4.1 | 787 | Banashankari | Casual Dining | Chinese, North Indian, Thai | 800.0 | Buffet | Banashankari | . 2 Yes | No | 3.8 | 918 | Banashankari | Cafe, Casual Dining | Cafe, Mexican, Italian | 800.0 | Buffet | Banashankari | . 3 No | No | 3.7 | 88 | Banashankari | Quick Bites | South Indian, North Indian | 300.0 | Buffet | Banashankari | . 4 No | No | 3.8 | 166 | Basavanagudi | Casual Dining | North Indian, Rajasthani | 600.0 | Buffet | Banashankari | . Extras . We&#39;ll also convert the values in rest_type and cuisines columns to lists. . df.cuisines = df.cuisines.str.split(&quot;,&quot;) df.rest_type = df.rest_type.str.split(&quot;,&quot;) . df.head() . online_order book_table rating votes location rest_type cuisines cost_for_two_people type listed_in(city) . 0 Yes | Yes | 4.1 | 775 | Banashankari | [Casual Dining] | [North Indian, Mughlai, Chinese] | 800.0 | Buffet | Banashankari | . 1 Yes | No | 4.1 | 787 | Banashankari | [Casual Dining] | [Chinese, North Indian, Thai] | 800.0 | Buffet | Banashankari | . 2 Yes | No | 3.8 | 918 | Banashankari | [Cafe, Casual Dining] | [Cafe, Mexican, Italian] | 800.0 | Buffet | Banashankari | . 3 No | No | 3.7 | 88 | Banashankari | [Quick Bites] | [South Indian, North Indian] | 300.0 | Buffet | Banashankari | . 4 No | No | 3.8 | 166 | Basavanagudi | [Casual Dining] | [North Indian, Rajasthani] | 600.0 | Buffet | Banashankari | . Exploratory Analysis and Visualization . With these columns in mind, we will try to answer the following questions from the dataset: . What locations are most popular for restaurants in Benagluru? | Which locations have the best rated restaurants? | What relation does the rating and number of votes that a restaurant receives have? What about table booking and online order facility? | Is a restaurant which offers online order facility rated better? | Are restaurants offering expensive food rated better? Does a table booking facility make a difference? | Does the number of cuisines that a restaurant provides have a relation to the rating it recieves? | How do Casual Dining and Fine Dining restaurants differ in their rating? | How do Casual Dining and Fine Dining restaurants differ in their cost_for_two_people? | What are the number of different types of restaurants? | Q1. What locations are most popular for restaurants in Benagluru? . popular_locations = df.location.value_counts().head(15) popular_locations . BTM 3879 Koramangala 5th Block 2297 HSR 1993 Indiranagar 1800 JP Nagar 1710 Jayanagar 1634 Whitefield 1568 Marathahalli 1410 Bannerghatta Road 1226 Koramangala 7th Block 1055 Koramangala 6th Block 1054 Brigade Road 1052 Bellandur 997 Sarjapur Road 854 Koramangala 1st Block 852 Name: location, dtype: int64 . plt.figure(figsize = (10, 8)) sns.barplot(x = popular_locations, y = popular_locations.index) . &lt;AxesSubplot:xlabel=&#39;location&#39;&gt; . The 5 most popular locations for restaurants are BTM, Koramangala 5th Block, HSR, Indiranagar, and JP Nagar, with BTM boasting of nearly 4000 eateries. . Q2. Which locations have the best rated restaurants? . (We&#39;ll select only those locations which have a minimum of 50 restaurants.) . # groupby location and get the count of each location along with the average rating of restaurants in that location. location_rating = df.groupby(by = [&quot;location&quot;])[&quot;rating&quot;].agg([&quot;count&quot;, &quot;mean&quot;]) location_rating.head() . count mean . location . BTM 3879 | 3.571410 | . Banashankari 744 | 3.649866 | . Banaswadi 468 | 3.496368 | . Bannerghatta Road 1226 | 3.509869 | . Basavanagudi 595 | 3.671092 | . # select the locations with 50 minimumn eateries and then sort them by their rating. rated_locations = location_rating.loc[location_rating[&quot;count&quot;] &gt;= 50].sort_values(by = &quot;mean&quot;, ascending = False) # select the top 20 locations. top20_rated_locations = rated_locations[:20] top20_rated_locations.head() . count mean . location . Lavelle Road 481 | 4.141788 | . Koramangala 3rd Block 191 | 4.020419 | . St. Marks Road 343 | 4.017201 | . Koramangala 5th Block 2297 | 4.006661 | . Church Street 546 | 3.992125 | . # plot the observations plt.figure(figsize = (15, 5)) sns.barplot(x = top20_rated_locations.index, y = top20_rated_locations[&quot;mean&quot;]) plt.xticks(rotation = 45) plt.ylim(3.5, 4.3) plt.show() . These are the top 20 locations in Bengaluru based on the eatries&#39; ratings. The average rating in these locations range from 4.14 and 3.8. . Q3. What relation does the rating and number of votes that a restaurant receives have? What about table booking and online order facility? . # Relationship between `rating`, `votes` and `book_table` plt.figure(figsize = (8, 5)) sns.scatterplot(x = &quot;rating&quot;, y = &quot;votes&quot;, hue = &quot;book_table&quot;, data = df, s = 40) . &lt;AxesSubplot:xlabel=&#39;rating&#39;, ylabel=&#39;votes&#39;&gt; . The number of votes look to be directly correlated with the rating of the restaurant, and they look to increase exponentially with the rating after a critical point. This is to be expected because better restaurants would atrract more customers and thus more votes. . Also, the restaurants which don&#39;t provide booking facility are clustered in low ratings and less number of votes. In other words, more popular restaurants with high ratings are more likely to offer table booking facility, which can also be seen the following graph. . sns.boxplot(x = &quot;book_table&quot;, y = &quot;rating&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;book_table&#39;, ylabel=&#39;rating&#39;&gt; . Now, we can look at the scatterplot for rating, votes and online_order . # Relationship between `rating`, `votes` and `online_order` plt.figure(figsize = (8, 5)) sns.scatterplot(x = &quot;rating&quot;, y = &quot;votes&quot;, hue = &quot;online_order&quot;, data = df, s = 40) . &lt;AxesSubplot:xlabel=&#39;rating&#39;, ylabel=&#39;votes&#39;&gt; . The data points for online order facility are scattered in the graph, and thus the data doesn&#39;t reveal any relationship between these variables. But, it may be worth exploring its relationship with the votes and the rating individually, which we&#39;ll do in the following sections. . Q4. Is a restaurant which offers online order facility rated better? . sns.violinplot(x = &quot;online_order&quot;, y = &quot;rating&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;online_order&#39;, ylabel=&#39;rating&#39;&gt; . As discussed in the previous question, the rating doesn&#39;t seem to be any different between the restaurants which offer online order facility and which don&#39;t. . Q5. Are restaurants offering expensive food rated better? Does a table booking facility make a difference? . plt.figure(figsize = (8, 5)) sns.scatterplot(x = &quot;rating&quot;, y = &quot;cost_for_two_people&quot;, data = df, hue = &#39;book_table&#39;) . &lt;AxesSubplot:xlabel=&#39;rating&#39;, ylabel=&#39;cost_for_two_people&#39;&gt; . Although there are hints of an exponential relationship between the cost for two people and the rating of a restaurant, most of the data is clustered in low cost, and thus because of lack of data points for expensive restaurants, the data is inconclusive for this relations. But we can study book table facility individuallly with the cost for two people, where restaurants which offer book_table facility seem to be more expensive than restaurants which don&#39;t. . sns.boxplot(x = &quot;book_table&quot;, y = &quot;cost_for_two_people&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;book_table&#39;, ylabel=&#39;cost_for_two_people&#39;&gt; . This graph also supports the idea that table booking is correlated with the cost for two people. . Q6. Does the number of cuisines that a restaurant provides have a relation to the rating it recieves? . The cuisines column shows the all the cuisines that a restaurant offers. We can add column to the DataFrame to store the number of cuisines that a restaurant offers. . df[&quot;no_of_cuisines&quot;] = df.cuisines.str.len() df[&quot;no_of_cuisines&quot;].head() . 0 3 1 3 2 3 3 2 4 2 Name: no_of_cuisines, dtype: int64 . plt.figure(figsize = (8, 5)) sns.stripplot(x = &quot;no_of_cuisines&quot;, y = &quot;rating&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;no_of_cuisines&#39;, ylabel=&#39;rating&#39;&gt; . The rating seems to become more concentrated towards mean as the no. of cuisines that a restaurant offers goes up. But, this could also be a artifact of low no. of restaurants offering higher no. of cuisines. In general, the mean of rating also seems to go up with the increase in no. of cuisines, but the graph is inconclusive. We&#39;ll explore this more in the boxplot. . sns.boxplot(x = &quot;no_of_cuisines&quot;, y = &quot;rating&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;no_of_cuisines&#39;, ylabel=&#39;rating&#39;&gt; . This graph reflects the relationship better and does support the idea that restaurants offering more no of cuisines are usually rated better. . Q7. How do Casual Dining and Fine Dining restaurants differ in their cost_for_two_people? . #Look at the dataframe df.head() . online_order book_table rating votes location rest_type cuisines cost_for_two_people type listed_in(city) no_of_cuisines . 0 Yes | Yes | 4.1 | 775 | Banashankari | [Casual Dining] | [North Indian, Mughlai, Chinese] | 800.0 | Buffet | Banashankari | 3 | . 1 Yes | No | 4.1 | 787 | Banashankari | [Casual Dining] | [Chinese, North Indian, Thai] | 800.0 | Buffet | Banashankari | 3 | . 2 Yes | No | 3.8 | 918 | Banashankari | [Cafe, Casual Dining] | [Cafe, Mexican, Italian] | 800.0 | Buffet | Banashankari | 3 | . 3 No | No | 3.7 | 88 | Banashankari | [Quick Bites] | [South Indian, North Indian] | 300.0 | Buffet | Banashankari | 2 | . 4 No | No | 3.8 | 166 | Basavanagudi | [Casual Dining] | [North Indian, Rajasthani] | 600.0 | Buffet | Banashankari | 2 | . We&#39;ll need to explode the rest_type column to extract information. . # extract information from `rest_type` column rest_type_exploded = df.explode(column = &quot;rest_type&quot;) rest_type_exploded[&quot;rest_type&quot;] = rest_type_exploded[&quot;rest_type&quot;].str.strip() rest_type_exploded.head() . online_order book_table rating votes location rest_type cuisines cost_for_two_people type listed_in(city) no_of_cuisines . 0 Yes | Yes | 4.1 | 775 | Banashankari | Casual Dining | [North Indian, Mughlai, Chinese] | 800.0 | Buffet | Banashankari | 3 | . 1 Yes | No | 4.1 | 787 | Banashankari | Casual Dining | [Chinese, North Indian, Thai] | 800.0 | Buffet | Banashankari | 3 | . 2 Yes | No | 3.8 | 918 | Banashankari | Cafe | [Cafe, Mexican, Italian] | 800.0 | Buffet | Banashankari | 3 | . 2 Yes | No | 3.8 | 918 | Banashankari | Casual Dining | [Cafe, Mexican, Italian] | 800.0 | Buffet | Banashankari | 3 | . 3 No | No | 3.7 | 88 | Banashankari | Quick Bites | [South Indian, North Indian] | 300.0 | Buffet | Banashankari | 2 | . # separate data for casual dining restaurants and fine dining restaurants. fine_dining_rest = rest_type_exploded.loc[rest_type_exploded.rest_type == &quot;Fine Dining&quot;] casual_dining_rest = rest_type_exploded.loc[rest_type_exploded.rest_type == &quot;Casual Dining&quot;] . # plot the data sns.kdeplot(fine_dining_rest.cost_for_two_people, fill = True) sns.kdeplot(casual_dining_rest.cost_for_two_people, fill = True) plt.legend([&quot;Fine Dining&quot;, &quot;Casual Dining&quot;]) plt.show() . As expected, fine dining restaurants are much more expensive than casual dining restaurants. . Q8. How do Casual Dining and Fine Dining restaurants differ in their rating? . sns.kdeplot(fine_dining_rest.rating, fill = True) sns.kdeplot(casual_dining_rest.rating, fill = True) plt.legend([&quot;Fine Dining&quot;, &quot;Casual Dining&quot;]) plt.show() . Fine dining restaurants are usually rated better and their ratings show much less variance than the ratings of casual dining restaurants. . Q9. What are the five most common types of restaurants? . # Five most common types of restaurants most_common_types_of_restaurants = rest_type_exploded.rest_type.value_counts() most_common_types_of_restaurants.head() . Quick Bites 15144 Casual Dining 12188 Cafe 4604 Delivery 2946 Dessert Parlor 2695 Name: rest_type, dtype: int64 . sns.barplot(x = most_common_types_of_restaurants.head(), y = most_common_types_of_restaurants.index[:5]) plt.show() . Q10. What are the top 5 rated restaurants in type and no_of_cuisines combined? . In other words, which type and no_of_cuisines combinations gather the highest ratings? . # group and extract data for different `types` and `no of cuisines` rating_data = df.groupby(by = [&quot;type&quot;, &quot;no_of_cuisines&quot;])[&quot;rating&quot;].agg(&quot;mean&quot;) rating_data.head() . type no_of_cuisines Buffet 1 3.995402 2 3.903509 3 3.911111 4 4.063212 5 4.217284 Name: rating, dtype: float64 . Now, we&#39;ll make a 2D datarame out of this multiindexed pandas Series. . rating_data_df = rating_data.unstack() rating_data_df.head() . no_of_cuisines 1 2 3 4 5 6 7 8 . type . Buffet 3.995402 | 3.903509 | 3.911111 | 4.063212 | 4.217284 | 3.926667 | 3.500000 | 4.062500 | . Cafes 3.643697 | 3.760366 | 3.941109 | 3.938434 | 4.015942 | 4.090698 | 4.156757 | 4.084615 | . Delivery 3.593535 | 3.629772 | 3.654311 | 3.739603 | 3.817625 | 3.901439 | 3.999333 | 3.801149 | . Desserts 3.698520 | 3.772140 | 3.755556 | 3.934932 | 4.125610 | 4.084211 | 3.920000 | 4.060000 | . Dine-out 3.613442 | 3.603628 | 3.687592 | 3.825194 | 3.903406 | 4.006091 | 4.035433 | 3.982051 | . # plot the data plt.figure(figsize = (9, 7)) fig = sns.heatmap(data = rating_data_df, annot = True, cmap = &quot;rocket_r&quot;) fig.set(xlabel = &quot;No. of cuisines&quot;, ylabel = &quot;Type&quot;) plt.show() . From the plot, Pubs and bars which offer more than 3 cuisines are all rated high. Similarly, Drinks &amp; nightlife restaurants with multiple cuisines are also rated really high. . We can get the top 5 combinations from the rating_data Series. . # top 5 combinations for `type` and `no_of_cuisines` rating_data.sort_values(ascending = False).head() . type no_of_cuisines Drinks &amp; nightlife 7 4.464706 Pubs and bars 7 4.455556 8 4.400000 Drinks &amp; nightlife 6 4.264865 8 4.250000 Name: rating, dtype: float64 . Q11. What is the relationship between the type and cost_for_two_people? . plt.figure(figsize = (8, 6)) sns.boxplot(x = &quot;type&quot;, y = &quot;cost_for_two_people&quot;, data = df) plt.xticks(rotation = 45) plt.show() . First thing to note is that there are quite a few outliers in the data, almost all of them offering much more expensive food from the rest of the distribution. Also buffet, drinks &amp; nighlife, and pubs are much more expensive than eateries of the type desserts and delivery. . Summary and Conclusion . Many questions could be asked and explored from the Zomato Dataset, but here we tried to answer 11 of them. We studied all the restaurants in Bengaluru, who have registered on Zomato, and tried to explore multiple variables&#39; relationship with the restaurants&#39; ratings. We also studied what factors go along with the food cost for two people in these restaurants. . There are two important things to note here before making any conclusions. First, all the analysis we did might apply only to restaurants registered on Zomato and other similar online platforms, and might differ significantly if we explore the food industry offline. Second really important thing is all the relationships that we studied are correlational in nature. This project thus does not establish causal relationships, although it might suggest some and can be taken as an inspiration to conduct actual experimental studies to explore the variables discussed here. Keeping in mind this, we can look at what we actually did establish in this EDA of Zomato Dataset. . The most popular places for restaurants in Benagluru are BTM, Koramangala 5th Block, HSR, Indiranagar, and JP Nagar, with BTM boasting of nearly 4000 eateries. | The top 5 locations according to avg rating of restaurants are Lavelle Road, Koramangala 3rd Block, St. Marks Road, Koramangala 5th Block and Church Street. | Restaurants with higher ratings have generally received more votes than the restaurants with lower rating and they are more likely to offer table booking facility. | Also, restaurants offering table booking facility are also generally more expensive. | Restaurants offering more no. of cuisines are also on average rated better. | Fine dining restaurants are much more expensive than casual dining restaurants and they are also usually rated better with much less variance in the ratings. | Quick Bites and Casual Dining restaurants but are the most popular types of restaurant in Bengaluru on Zomato. | Pubs and bars which offer more than 3 cuisines are all rated high. Similarly, Drinks &amp; nightlife restaurants with multiple cuisines are also rated really highly. | Buffet, drinks &amp; nighlife, and pubs are much more expensive than eateries of the type desserts and delivery. | For the other questions we asked, the data was more or less inconclusive. We may need more extensive data to answer those questions. | . Apart from these inferences, many more interesting relationships can be studied and should be explored from this data. . Thanks for reading. .",
            "url": "https://ncitshubham.github.io/blogs/2021/11/23/eda-and-data-vis-of-zomato-dataset.html",
            "relUrl": "/2021/11/23/eda-and-data-vis-of-zomato-dataset.html",
            "date": " • Nov 23, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Scraping Data Science Jobs on in.indeed.com",
            "content": "Introduction . Indeed.com is a worldwide employment website for job listings. Globally, it hosts millions of job listings on thousands of jobs. In this project, we are interested in the &#39;Data Science&#39; related job listings on https://in.indeed.com/ . Thus, we&#39;ll scrape the website for this information and save that to a csv file for future use. In order to do this, we&#39;ll use the following tools: . Python as the programming language | Requests library for downloading the webpage contents | BeautifulSoup library for finding and accessing the relevant information from the downloaded webpage. | Numpy library for handling missing values. | Pandas library for saving the accessed information to a csv file. | . Here are the steps we&#39;ll follow: . We&#39;ll scrape first 30 pages from https://in.indeed.com/jobs?q=data%20science | We&#39;ll get a list of all 15 jobs on each page. | For each job, we&#39;ll grab the Job Title, Salary, Location, Company Name, and Company Rating. | We&#39;ll save all of the information in a csv file in the following format: . Title,Salary,Location,Company Name,Company Rating Data science,&quot;₹35,000 - ₹45,000 a month&quot;,&quot;Mumbai, Maharashtra&quot;,V Digitech Sevices Data Science ( 2 - 8 Yrs) - Remote,&quot;₹8,00,000 - ₹20,00,000 a year&quot;,Remote,ProGrad Data Science Internship,&quot;₹10,000 a month&quot;,&quot;Gurgaon, Haryana&quot;,Zigram . | . Initial Setup . Import the required libraries . import requests from bs4 import BeautifulSoup import pandas as pd from numpy import nan . Set up base URL and the user-agent. . # The base_url is grabbed after searching &#39;Data Science&#39; on in.indeed.com # The start value in the base_url will increment by 10 to access each following page. base_url = &quot;https://in.indeed.com/jobs?q=data%20science&amp;start={}&quot; header = {&quot;User-Agent&quot;: &quot;Mozilla/5.0&quot;} . Create a dictionary to save all information. . jobs = {&quot;Job Title&quot;: [], &quot;Salary&quot;: [], &quot;Location&quot;: [], &quot;Company Name&quot;: [], &quot;Company Rating&quot;: []} . Scrape the search result webpage. . Download webpage and create a BeautifulSoup object . def get_soup(url): &#39;&#39;&#39; This function will download the webpage for the url supplied as argument and return the BeautifulSoup object for the webpage which can be used to grab required information for the webpage. &#39;&#39;&#39; response = requests.get(url, &quot;html.parser&quot;, headers = header) if response.status_code != 200: raise Exception(&#39;Failed to load page {}&#39;.format(url)) soup = BeautifulSoup(response.text) return soup . Example for get_soup . soup = get_soup(base_url.format(10)) type(soup) . bs4.BeautifulSoup . Create a transform function . Now, we&#39;ll create a transform function to grab the list of jobs from the result webpage. . To do this, we&#39;ll pick td tags with the class: resultContent . . def transform(soup): # find all the job listings on the webpage. jobs_tags = soup.find_all(&quot;td&quot;, class_ = &quot;resultContent&quot;) # for each job, call helper functions to grab information about the job # and save that to jobs dictionary. for job in jobs_tags: jobs[&quot;Job Title&quot;].append(get_job_title(job)) jobs[&quot;Salary&quot;].append(get_job_salary(job)) jobs[&quot;Location&quot;].append(get_company_location(job)) jobs[&quot;Company Name&quot;].append(get_company_name(job)) jobs[&quot;Company Rating&quot;].append(get_company_rating(job)) . Example for finding the job tags . job_tags = soup.find_all(&quot;td&quot;, class_ = &quot;resultContent&quot;) print(len(job_tags)) . 15 . print(job_tags[0].text) . newData Analyst/ScientistSopra Steria3.5Noida, Uttar Pradesh . Create helper functions . Create helper functions to grab job information for each job and store that in the jobs dictionary. . First grab Job Title. . def get_job_title(job): &#39;&#39;&#39; Function to grab the job title. Because some job titles have a prefix new in their job titles, this function will automatically detect this prefix and return the title sans &#39;new&#39; in the job title. &#39;&#39;&#39; title = job.find(class_ = &quot;jobTitle&quot;).text if title[:3] == &quot;new&quot;: return title[3:] else: return title get_job_title(job_tags[0]) . &#39;Data Analyst/Scientist&#39; . Now, we&#39;ll grab the job salary, if the listing has one. . def get_job_salary(job): salary = job.find(&quot;div&quot;, class_ = &quot;salary-snippet&quot;) if salary: return salary.text else: return nan get_job_salary(job_tags[1]) . &#39;₹500 an hour&#39; . Similarly, we&#39;ll grab the company name, location, and its rating. . def get_company_name(job): &#39;&#39;&#39; Returns the company name for the supp&#39;&#39;&#39; return job.find(class_ = &quot;companyName&quot;).text def get_company_location(job): &#39;&#39;&#39; Returns the company location for the supplied job tag &#39;&#39;&#39; return job.find(class_ = &quot;companyLocation&quot;).text def get_company_rating(job): &#39;&#39;&#39; Returns the company rating for the supplied job tag &#39;&#39;&#39; rating = job.find(class_ = &quot;ratingNumber&quot;) if rating: return float(rating.text) else: return nan # Example print(get_company_name(job_tags[0]), get_company_location(job_tags[0]), get_company_rating(job_tags[0]), sep = &quot; n&quot;) . Sopra Steria Noida, Uttar Pradesh 3.5 . Putting it all together . We&#39;ll use a for loop to loop through 30 search result pages. Within this loop, we can apply the get_soup function to download these pages and the transform function to parse through all job listings from these pages and save the information in the jobs dictionary. We&#39;ll then use this dictionary to create a pandas DataFrame, which can then be saved to a csv file. . for page in range(0, 310, 10): print(f&quot;Scraping page {page}...&quot;) soup = get_soup(base_url.format(page)) transform(soup) . Scraping page 0... Scraping page 10... Scraping page 20... Scraping page 30... Scraping page 40... Scraping page 50... Scraping page 60... Scraping page 70... Scraping page 80... Scraping page 90... Scraping page 100... Scraping page 110... Scraping page 120... Scraping page 130... Scraping page 140... Scraping page 150... Scraping page 160... Scraping page 170... Scraping page 180... Scraping page 190... Scraping page 200... Scraping page 210... Scraping page 220... Scraping page 230... Scraping page 240... Scraping page 250... Scraping page 260... Scraping page 270... Scraping page 280... Scraping page 290... Scraping page 300... . # create a pandas DataFrame of the scraped data jobs_df = pd.DataFrame(jobs) jobs_df.head() . Job Title Salary Location Company Name Company Rating . 0 Technology Analyst: Data Science | Machine Lea... | NaN | Bengaluru, Karnataka | Infosys Limited | 3.9 | . 1 Analyst-Data Science | NaN | Gurgaon, Haryana+2 locations | Amex | NaN | . 2 Junior Data Scientist Data Science Chennai, India | NaN | Tamil Nadu | Applied Data Finance | NaN | . 3 Data Engineer – EPH | NaN | India | Kyndryl | 3.4 | . 4 Data Science Analyst | NaN | India | Helius Technologies | NaN | . # save data to a csv file jobs_df.to_csv(&quot;Data_Science_jobs_from_indeed.com.csv&quot;, index = None, encoding = &quot;utf-8&quot;) . Summary . This was a short project, where we looked into how job listings can be scraped from Indeed.com. We craped 30 pages of job listings with tags Data Science. This gave us a total of 450 job listings with the details like the job title, salary, company, location, etc. We then saved this scraped data into a csv file for future use. .",
            "url": "https://ncitshubham.github.io/blogs/2021/10/19/scraping-indeed.com-for-data-science-jobs.html",
            "relUrl": "/2021/10/19/scraping-indeed.com-for-data-science-jobs.html",
            "date": " • Oct 19, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Predicting Real Estate Sale Price on Kaggle",
            "content": "Introduction . In this notebook, we&#39;ll work on the Ames Housing Dataset available on Kaggle as an educational competition. With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, the competition challenges us to predict the final price of each home. The competition dataset was compiled by Dean De Cock and it is an incredible alternative as a modernized and expanded version of the often cited Boston Housing dataset. . We&#39;ll try to make predictions on the SalePrice of the test dataset available as part of this competition, and then make submissions. Our submissions will be evaluated by rmsle, and we&#39;ll try to improve on this metric with each of our submission. . Setup . First, we&#39;ll import the required libraries and get the file paths . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # data visualization import seaborn as sns # data visualization from sklearn.preprocessing import OneHotEncoder, Normalizer, RobustScaler # data preparation from sklearn.impute import SimpleImputer # missing value handling from sklearn.model_selection import KFold, cross_val_score # model selection from sklearn.metrics import mean_squared_error # metrics from scipy.stats import norm, skew # statistics import psutil # get cpu core count from bayes_opt import BayesianOptimization # hyperparameter tuning # pipelines from sklearn.compose import make_column_transformer from sklearn.pipeline import make_pipeline # ML models from sklearn.dummy import DummyRegressor from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV from sklearn.ensemble import RandomForestRegressor import lightgbm as lgb import xgboost as xgb # ignore warnings import warnings warnings.filterwarnings(&quot;ignore&quot;) # get file paths import os data_dir = os.getcwd() for dirname, _, filenames in os.walk(data_dir): for filename in filenames: if filename[-4:] == &#39;.csv&#39;: print(os.path.join(dirname, filename)) . C: Users saini Downloads Ames_Housing_Competition sample_submission.csv C: Users saini Downloads Ames_Housing_Competition test.csv C: Users saini Downloads Ames_Housing_Competition train.csv . All data files are under 1MB and safe to import wholly. . train = pd.read_csv(data_dir + &quot;/train.csv&quot;, index_col = [&quot;Id&quot;]) test = pd.read_csv(data_dir + &quot;/test.csv&quot;, index_col = [&quot;Id&quot;]) submission_df = pd.read_csv(data_dir + &quot;/sample_submission.csv&quot;) . train.head() . MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice . Id . 1 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | Inside | ... | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | 208500 | . 2 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | FR2 | ... | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | 181500 | . 3 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | Inside | ... | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | 223500 | . 4 70 | RL | 60.0 | 9550 | Pave | NaN | IR1 | Lvl | AllPub | Corner | ... | 0 | NaN | NaN | NaN | 0 | 2 | 2006 | WD | Abnorml | 140000 | . 5 60 | RL | 84.0 | 14260 | Pave | NaN | IR1 | Lvl | AllPub | FR2 | ... | 0 | NaN | NaN | NaN | 0 | 12 | 2008 | WD | Normal | 250000 | . 5 rows × 80 columns . Observations . data contains both numeric and categorical features. | SalePrice is the target column | . seed = 42 . X_train = train.copy() y_train = X_train.pop(&quot;SalePrice&quot;) . EDA and Data Preparation . Now, we&#39;ll perform some basic data analysis and we&#39;ll use the insights we&#39;ll get to prepare the data for ML training. . Preliminary Analysis . pd.set_option(&quot;display.max_columns&quot;, None) X_train.head(15) . MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle OverallQual OverallCond YearBuilt YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF Heating HeatingQC CentralAir Electrical 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr KitchenQual TotRmsAbvGrd Functional Fireplaces FireplaceQu GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond PavedDrive WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition . Id . 1 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2003 | 2003 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 196.0 | Gd | TA | PConc | Gd | TA | No | GLQ | 706 | Unf | 0 | 150 | 856 | GasA | Ex | Y | SBrkr | 856 | 854 | 0 | 1710 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 8 | Typ | 0 | NaN | Attchd | 2003.0 | RFn | 2 | 548 | TA | TA | Y | 0 | 61 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | . 2 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | FR2 | Gtl | Veenker | Feedr | Norm | 1Fam | 1Story | 6 | 8 | 1976 | 1976 | Gable | CompShg | MetalSd | MetalSd | None | 0.0 | TA | TA | CBlock | Gd | TA | Gd | ALQ | 978 | Unf | 0 | 284 | 1262 | GasA | Ex | Y | SBrkr | 1262 | 0 | 0 | 1262 | 0 | 1 | 2 | 0 | 3 | 1 | TA | 6 | Typ | 1 | TA | Attchd | 1976.0 | RFn | 2 | 460 | TA | TA | Y | 298 | 0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | . 3 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2001 | 2002 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 162.0 | Gd | TA | PConc | Gd | TA | Mn | GLQ | 486 | Unf | 0 | 434 | 920 | GasA | Ex | Y | SBrkr | 920 | 866 | 0 | 1786 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 6 | Typ | 1 | TA | Attchd | 2001.0 | RFn | 2 | 608 | TA | TA | Y | 0 | 42 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | . 4 70 | RL | 60.0 | 9550 | Pave | NaN | IR1 | Lvl | AllPub | Corner | Gtl | Crawfor | Norm | Norm | 1Fam | 2Story | 7 | 5 | 1915 | 1970 | Gable | CompShg | Wd Sdng | Wd Shng | None | 0.0 | TA | TA | BrkTil | TA | Gd | No | ALQ | 216 | Unf | 0 | 540 | 756 | GasA | Gd | Y | SBrkr | 961 | 756 | 0 | 1717 | 1 | 0 | 1 | 0 | 3 | 1 | Gd | 7 | Typ | 1 | Gd | Detchd | 1998.0 | Unf | 3 | 642 | TA | TA | Y | 0 | 35 | 272 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2006 | WD | Abnorml | . 5 60 | RL | 84.0 | 14260 | Pave | NaN | IR1 | Lvl | AllPub | FR2 | Gtl | NoRidge | Norm | Norm | 1Fam | 2Story | 8 | 5 | 2000 | 2000 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 350.0 | Gd | TA | PConc | Gd | TA | Av | GLQ | 655 | Unf | 0 | 490 | 1145 | GasA | Ex | Y | SBrkr | 1145 | 1053 | 0 | 2198 | 1 | 0 | 2 | 1 | 4 | 1 | Gd | 9 | Typ | 1 | TA | Attchd | 2000.0 | RFn | 3 | 836 | TA | TA | Y | 192 | 84 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 12 | 2008 | WD | Normal | . 6 50 | RL | 85.0 | 14115 | Pave | NaN | IR1 | Lvl | AllPub | Inside | Gtl | Mitchel | Norm | Norm | 1Fam | 1.5Fin | 5 | 5 | 1993 | 1995 | Gable | CompShg | VinylSd | VinylSd | None | 0.0 | TA | TA | Wood | Gd | TA | No | GLQ | 732 | Unf | 0 | 64 | 796 | GasA | Ex | Y | SBrkr | 796 | 566 | 0 | 1362 | 1 | 0 | 1 | 1 | 1 | 1 | TA | 5 | Typ | 0 | NaN | Attchd | 1993.0 | Unf | 2 | 480 | TA | TA | Y | 40 | 30 | 0 | 320 | 0 | 0 | NaN | MnPrv | Shed | 700 | 10 | 2009 | WD | Normal | . 7 20 | RL | 75.0 | 10084 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | Somerst | Norm | Norm | 1Fam | 1Story | 8 | 5 | 2004 | 2005 | Gable | CompShg | VinylSd | VinylSd | Stone | 186.0 | Gd | TA | PConc | Ex | TA | Av | GLQ | 1369 | Unf | 0 | 317 | 1686 | GasA | Ex | Y | SBrkr | 1694 | 0 | 0 | 1694 | 1 | 0 | 2 | 0 | 3 | 1 | Gd | 7 | Typ | 1 | Gd | Attchd | 2004.0 | RFn | 2 | 636 | TA | TA | Y | 255 | 57 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 8 | 2007 | WD | Normal | . 8 60 | RL | NaN | 10382 | Pave | NaN | IR1 | Lvl | AllPub | Corner | Gtl | NWAmes | PosN | Norm | 1Fam | 2Story | 7 | 6 | 1973 | 1973 | Gable | CompShg | HdBoard | HdBoard | Stone | 240.0 | TA | TA | CBlock | Gd | TA | Mn | ALQ | 859 | BLQ | 32 | 216 | 1107 | GasA | Ex | Y | SBrkr | 1107 | 983 | 0 | 2090 | 1 | 0 | 2 | 1 | 3 | 1 | TA | 7 | Typ | 2 | TA | Attchd | 1973.0 | RFn | 2 | 484 | TA | TA | Y | 235 | 204 | 228 | 0 | 0 | 0 | NaN | NaN | Shed | 350 | 11 | 2009 | WD | Normal | . 9 50 | RM | 51.0 | 6120 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | OldTown | Artery | Norm | 1Fam | 1.5Fin | 7 | 5 | 1931 | 1950 | Gable | CompShg | BrkFace | Wd Shng | None | 0.0 | TA | TA | BrkTil | TA | TA | No | Unf | 0 | Unf | 0 | 952 | 952 | GasA | Gd | Y | FuseF | 1022 | 752 | 0 | 1774 | 0 | 0 | 2 | 0 | 2 | 2 | TA | 8 | Min1 | 2 | TA | Detchd | 1931.0 | Unf | 2 | 468 | Fa | TA | Y | 90 | 0 | 205 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 4 | 2008 | WD | Abnorml | . 10 190 | RL | 50.0 | 7420 | Pave | NaN | Reg | Lvl | AllPub | Corner | Gtl | BrkSide | Artery | Artery | 2fmCon | 1.5Unf | 5 | 6 | 1939 | 1950 | Gable | CompShg | MetalSd | MetalSd | None | 0.0 | TA | TA | BrkTil | TA | TA | No | GLQ | 851 | Unf | 0 | 140 | 991 | GasA | Ex | Y | SBrkr | 1077 | 0 | 0 | 1077 | 1 | 0 | 1 | 0 | 2 | 2 | TA | 5 | Typ | 2 | TA | Attchd | 1939.0 | RFn | 1 | 205 | Gd | TA | Y | 0 | 4 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 1 | 2008 | WD | Normal | . 11 20 | RL | 70.0 | 11200 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | Sawyer | Norm | Norm | 1Fam | 1Story | 5 | 5 | 1965 | 1965 | Hip | CompShg | HdBoard | HdBoard | None | 0.0 | TA | TA | CBlock | TA | TA | No | Rec | 906 | Unf | 0 | 134 | 1040 | GasA | Ex | Y | SBrkr | 1040 | 0 | 0 | 1040 | 1 | 0 | 1 | 0 | 3 | 1 | TA | 5 | Typ | 0 | NaN | Detchd | 1965.0 | Unf | 1 | 384 | TA | TA | Y | 0 | 0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | . 12 60 | RL | 85.0 | 11924 | Pave | NaN | IR1 | Lvl | AllPub | Inside | Gtl | NridgHt | Norm | Norm | 1Fam | 2Story | 9 | 5 | 2005 | 2006 | Hip | CompShg | WdShing | Wd Shng | Stone | 286.0 | Ex | TA | PConc | Ex | TA | No | GLQ | 998 | Unf | 0 | 177 | 1175 | GasA | Ex | Y | SBrkr | 1182 | 1142 | 0 | 2324 | 1 | 0 | 3 | 0 | 4 | 1 | Ex | 11 | Typ | 2 | Gd | BuiltIn | 2005.0 | Fin | 3 | 736 | TA | TA | Y | 147 | 21 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 7 | 2006 | New | Partial | . 13 20 | RL | NaN | 12968 | Pave | NaN | IR2 | Lvl | AllPub | Inside | Gtl | Sawyer | Norm | Norm | 1Fam | 1Story | 5 | 6 | 1962 | 1962 | Hip | CompShg | HdBoard | Plywood | None | 0.0 | TA | TA | CBlock | TA | TA | No | ALQ | 737 | Unf | 0 | 175 | 912 | GasA | TA | Y | SBrkr | 912 | 0 | 0 | 912 | 1 | 0 | 1 | 0 | 2 | 1 | TA | 4 | Typ | 0 | NaN | Detchd | 1962.0 | Unf | 1 | 352 | TA | TA | Y | 140 | 0 | 0 | 0 | 176 | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | . 14 20 | RL | 91.0 | 10652 | Pave | NaN | IR1 | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 1Story | 7 | 5 | 2006 | 2007 | Gable | CompShg | VinylSd | VinylSd | Stone | 306.0 | Gd | TA | PConc | Gd | TA | Av | Unf | 0 | Unf | 0 | 1494 | 1494 | GasA | Ex | Y | SBrkr | 1494 | 0 | 0 | 1494 | 0 | 0 | 2 | 0 | 3 | 1 | Gd | 7 | Typ | 1 | Gd | Attchd | 2006.0 | RFn | 3 | 840 | TA | TA | Y | 160 | 33 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 8 | 2007 | New | Partial | . 15 20 | RL | NaN | 10920 | Pave | NaN | IR1 | Lvl | AllPub | Corner | Gtl | NAmes | Norm | Norm | 1Fam | 1Story | 6 | 5 | 1960 | 1960 | Hip | CompShg | MetalSd | MetalSd | BrkFace | 212.0 | TA | TA | CBlock | TA | TA | No | BLQ | 733 | Unf | 0 | 520 | 1253 | GasA | TA | Y | SBrkr | 1253 | 0 | 0 | 1253 | 1 | 0 | 1 | 1 | 2 | 1 | TA | 5 | Typ | 1 | Fa | Attchd | 1960.0 | RFn | 1 | 352 | TA | TA | Y | 0 | 213 | 176 | 0 | 0 | 0 | NaN | GdWo | NaN | 0 | 5 | 2008 | WD | Normal | . X_train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1460 entries, 1 to 1460 Data columns (total 79 columns): # Column Non-Null Count Dtype -- -- 0 MSSubClass 1460 non-null object 1 MSZoning 1460 non-null object 2 LotFrontage 1201 non-null float64 3 LotArea 1460 non-null int64 4 Street 1460 non-null object 5 Alley 91 non-null object 6 LotShape 1460 non-null object 7 LandContour 1460 non-null object 8 Utilities 1460 non-null object 9 LotConfig 1460 non-null object 10 LandSlope 1460 non-null object 11 Neighborhood 1460 non-null object 12 Condition1 1460 non-null object 13 Condition2 1460 non-null object 14 BldgType 1460 non-null object 15 HouseStyle 1460 non-null object 16 OverallQual 1460 non-null int64 17 OverallCond 1460 non-null int64 18 YearBuilt 1460 non-null int64 19 YearRemodAdd 1460 non-null int64 20 RoofStyle 1460 non-null object 21 RoofMatl 1460 non-null object 22 Exterior1st 1460 non-null object 23 Exterior2nd 1460 non-null object 24 MasVnrType 1452 non-null object 25 MasVnrArea 1452 non-null float64 26 ExterQual 1460 non-null object 27 ExterCond 1460 non-null object 28 Foundation 1460 non-null object 29 BsmtQual 1423 non-null object 30 BsmtCond 1423 non-null object 31 BsmtExposure 1422 non-null object 32 BsmtFinType1 1423 non-null object 33 BsmtFinSF1 1460 non-null int64 34 BsmtFinType2 1422 non-null object 35 BsmtFinSF2 1460 non-null int64 36 BsmtUnfSF 1460 non-null int64 37 TotalBsmtSF 1460 non-null int64 38 Heating 1460 non-null object 39 HeatingQC 1460 non-null object 40 CentralAir 1460 non-null object 41 Electrical 1459 non-null object 42 1stFlrSF 1460 non-null int64 43 2ndFlrSF 1460 non-null int64 44 LowQualFinSF 1460 non-null int64 45 GrLivArea 1460 non-null int64 46 BsmtFullBath 1460 non-null int64 47 BsmtHalfBath 1460 non-null int64 48 FullBath 1460 non-null int64 49 HalfBath 1460 non-null int64 50 BedroomAbvGr 1460 non-null int64 51 KitchenAbvGr 1460 non-null int64 52 KitchenQual 1460 non-null object 53 TotRmsAbvGrd 1460 non-null int64 54 Functional 1460 non-null object 55 Fireplaces 1460 non-null int64 56 FireplaceQu 770 non-null object 57 GarageType 1379 non-null object 58 GarageYrBlt 1379 non-null float64 59 GarageFinish 1379 non-null object 60 GarageCars 1460 non-null int64 61 GarageArea 1460 non-null int64 62 GarageQual 1379 non-null object 63 GarageCond 1379 non-null object 64 PavedDrive 1460 non-null object 65 WoodDeckSF 1460 non-null int64 66 OpenPorchSF 1460 non-null int64 67 EnclosedPorch 1460 non-null int64 68 3SsnPorch 1460 non-null int64 69 ScreenPorch 1460 non-null int64 70 PoolArea 1460 non-null int64 71 PoolQC 7 non-null object 72 Fence 281 non-null object 73 MiscFeature 54 non-null object 74 MiscVal 1460 non-null int64 75 MoSold 1460 non-null int64 76 YrSold 1460 non-null int64 77 SaleType 1460 non-null object 78 SaleCondition 1460 non-null object dtypes: float64(3), int64(32), object(44) memory usage: 944.8+ KB . Observations . There are 1460 entries with 79 features. | Some features have null values, and thus will need further inspection. | Some features can be accumulated to give new features. For eg. totalling different types of rooms to give a feature total_rooms. | . Null Values . plt.figure(figsize = (25, 10)) sns.heatmap(X_train.isnull(), yticklabels = &quot;&quot;) plt.title(&quot;Distribution of null values&quot;) plt.show() . null_count = X_train.isnull().sum() null_count = null_count.to_frame(name = &quot;null_values&quot;)[null_count &gt; 0] null_count[&quot;null_percentage&quot;] = null_count[&quot;null_values&quot;]/len(X_train)*100 null_vals = null_count.sort_values(by = [&quot;null_values&quot;], ascending = False) null_vals . null_values null_percentage . PoolQC 1453 | 99.520548 | . MiscFeature 1406 | 96.301370 | . Alley 1369 | 93.767123 | . Fence 1179 | 80.753425 | . FireplaceQu 690 | 47.260274 | . LotFrontage 259 | 17.739726 | . GarageType 81 | 5.547945 | . GarageYrBlt 81 | 5.547945 | . GarageFinish 81 | 5.547945 | . GarageQual 81 | 5.547945 | . GarageCond 81 | 5.547945 | . BsmtExposure 38 | 2.602740 | . BsmtFinType2 38 | 2.602740 | . BsmtFinType1 37 | 2.534247 | . BsmtCond 37 | 2.534247 | . BsmtQual 37 | 2.534247 | . MasVnrArea 8 | 0.547945 | . MasVnrType 8 | 0.547945 | . Electrical 1 | 0.068493 | . Observations . PoolQC, MiscFeature, Alley and Fence have a lot of null values. | Fortunately, the data description tells us that in all of the categorical feature columns bar one, null values indicate absence of those feature condition in the entries. Therefore, we&#39;ll impute null values in these feaure columns with &#39;None&#39;. | Electrical is the only feature column where null values don&#39;t indicate absence of any condition. But because there is only 1 null entry, we can drop this entry. | With regards to numerical columns, MasVnrArea and LotFrontage can be filled with 0 because here too, null values indicate absence of the conditions. GarageYrBlt can be filled with the median value, because although it is linked with the GarageCond column, absence of garage cannot have year 0 in GarageYrBlt. | For newer null value columns in the test dataset, we&#39;ll choose to impute them with either most_frequent(Categorical) or median(Numeric). | . X_train.dtypes.value_counts() . object 44 int64 32 float64 3 dtype: int64 . Handle Null values . null_cols = null_vals.index null_cols = null_cols.drop(&quot;Electrical&quot;) # drop null from `Electrical` def drop_electrical_null(X, y): drop_idx = X[X[&quot;Electrical&quot;].isnull()].index X = X.drop(drop_idx) y = y.drop(drop_idx) return X, y # categorical columns X_train[&#39;MSSubClass&#39;] = X_train[&#39;MSSubClass&#39;].apply(str) cat_cols = X_train.select_dtypes(include = [&quot;object&quot;]).columns cat_null_cols = null_cols.intersection(cat_cols) cat_null_cols_imp = SimpleImputer(strategy = &quot;constant&quot;, fill_value = &quot;None&quot;) cat_not_null_cols = cat_cols.difference(cat_null_cols) cat_not_null_coll_imp = SimpleImputer(strategy = &quot;most_frequent&quot;) cat_null_ct = make_column_transformer((cat_null_cols_imp, cat_null_cols), (cat_not_null_coll_imp, cat_not_null_cols)) # numeric columns num_cols = X_train.select_dtypes(exclude = [&quot;object&quot;]).columns num_null0_cols = pd.Index([&quot;MasVnrArea&quot;, &quot;LotFrontage&quot;]) num_null0_cols_imp = SimpleImputer(strategy = &quot;constant&quot;, fill_value = 0) num_not_null_cols = num_cols.difference(num_null0_cols) num_not_null_cols_imp = SimpleImputer(strategy = &quot;median&quot;) num_null_ct = make_column_transformer((num_null0_cols_imp, num_null0_cols), (num_not_null_cols_imp, num_not_null_cols)) # combine both into a common transformer null_ct = make_column_transformer((cat_null_ct, cat_cols), (num_null_ct, num_cols)) null_ct_features_in = cat_null_cols.append(cat_not_null_cols).append(num_null0_cols).append(num_not_null_cols) . Distribution and Outliers . sns.distplot(y_train, kde = True) plt.title(&quot;Distribution of Sale Price&quot;) plt.show() . The target looks skewed. We&#39;ll normalise it. . y_train = np.log1p(y_train) sns.distplot(y_train, fit = norm, kde = True) plt.title(&quot;Distribution of Sale Price&quot;) plt.show() . The target is normalized. Now, we can look at the numerical features. . plt.figure(figsize = (12,10)) fig = sns.boxplot(data = X_train[num_cols], orient = &#39;h&#39;) fig.set_xscale(&quot;log&quot;) . There are quite a few outliers in the numeric columns. We&#39;ll need to scale numeric data using RobustScaler in the preprocessor building section. Also, many of the numeric features are skewed and will need to be normalized so that we can train ML models which assume normality in the features. . imputed_features = pd.DataFrame(num_null_ct.fit_transform(X_train[num_cols])) num_features_in = num_null0_cols.append(num_not_null_cols) imputed_features.columns = num_features_in skew_features = imputed_features.apply(lambda x: skew(x)).sort_values(ascending = False) high_skew = skew_features.loc[skew_features &gt; 0.5] skewed_index = high_skew.index high_skew . MiscVal 24.451640 PoolArea 14.813135 LotArea 12.195142 3SsnPorch 10.293752 LowQualFinSF 9.002080 KitchenAbvGr 4.483784 BsmtFinSF2 4.250888 ScreenPorch 4.117977 BsmtHalfBath 4.099186 EnclosedPorch 3.086696 MasVnrArea 2.674865 OpenPorchSF 2.361912 BsmtFinSF1 1.683771 WoodDeckSF 1.539792 TotalBsmtSF 1.522688 1stFlrSF 1.375342 GrLivArea 1.365156 BsmtUnfSF 0.919323 2ndFlrSF 0.812194 OverallCond 0.692355 TotRmsAbvGrd 0.675646 HalfBath 0.675203 Fireplaces 0.648898 BsmtFullBath 0.595454 dtype: float64 . norma = Normalizer() normalized_cols = pd.DataFrame(norma.fit_transform(imputed_features[skewed_index])) normalized_cols.columns = skewed_index normalized_cols.head() . MiscVal PoolArea LotArea 3SsnPorch LowQualFinSF KitchenAbvGr BsmtFinSF2 ScreenPorch BsmtHalfBath EnclosedPorch MasVnrArea OpenPorchSF BsmtFinSF1 WoodDeckSF TotalBsmtSF 1stFlrSF GrLivArea BsmtUnfSF 2ndFlrSF OverallCond TotRmsAbvGrd HalfBath Fireplaces BsmtFullBath . 0 0.0 | 0.0 | 0.962439 | 0.0 | 0.0 | 0.000114 | 0.0 | 0.0 | 0.000000 | 0.00000 | 0.022324 | 0.006948 | 0.080412 | 0.000000 | 0.097497 | 0.097497 | 0.194766 | 0.017085 | 0.097269 | 0.000569 | 0.000911 | 0.000114 | 0.000000 | 0.000114 | . 1 0.0 | 0.0 | 0.969430 | 0.0 | 0.0 | 0.000101 | 0.0 | 0.0 | 0.000101 | 0.00000 | 0.000000 | 0.000000 | 0.098761 | 0.030093 | 0.127440 | 0.127440 | 0.127440 | 0.028679 | 0.000000 | 0.000808 | 0.000606 | 0.000000 | 0.000101 | 0.000000 | . 2 0.0 | 0.0 | 0.976793 | 0.0 | 0.0 | 0.000087 | 0.0 | 0.0 | 0.000000 | 0.00000 | 0.014066 | 0.003647 | 0.042197 | 0.000000 | 0.079880 | 0.079880 | 0.155071 | 0.037683 | 0.075191 | 0.000434 | 0.000521 | 0.000087 | 0.000087 | 0.000087 | . 3 0.0 | 0.0 | 0.971507 | 0.0 | 0.0 | 0.000102 | 0.0 | 0.0 | 0.000000 | 0.02767 | 0.000000 | 0.003560 | 0.021973 | 0.000000 | 0.076907 | 0.097761 | 0.174668 | 0.054933 | 0.076907 | 0.000509 | 0.000712 | 0.000000 | 0.000102 | 0.000102 | . 4 0.0 | 0.0 | 0.977664 | 0.0 | 0.0 | 0.000069 | 0.0 | 0.0 | 0.000000 | 0.00000 | 0.023996 | 0.005759 | 0.044907 | 0.013163 | 0.078501 | 0.078501 | 0.150695 | 0.033594 | 0.072194 | 0.000343 | 0.000617 | 0.000069 | 0.000069 | 0.000069 | . plt.figure(figsize = (12, 10)) fig = sns.boxplot(data = normalized_cols, orient = &quot;h&quot;) fig.set_xscale(&quot;log&quot;) . Some features like MasVnrArea, OpenPorchSF, BsmtFinSF1, WoodDeckDF, 2ndFlrSF couldn&#39;t be normalized. These columns will thus need to be dropped after extracting important information from them, which we&#39;ll do in the feature engineering section. . Feature Engineering . Area related features are important in predicting sale price of houses, and thus we can engineer some features related to area. We can also add some binary features indicating presence of swimming pool, garage or fireplace, which also are an important determiners of real estate value. . def add_cols(df): # Add area related features df[&#39;TotalSF&#39;] = df[&#39;TotalBsmtSF&#39;] + df[&#39;1stFlrSF&#39;] + df[&#39;2ndFlrSF&#39;] df[&#39;Total_Bathrooms&#39;] = (df[&#39;FullBath&#39;] + (0.5 * df[&#39;HalfBath&#39;]) + df[&#39;BsmtFullBath&#39;] + (0.5 * df[&#39;BsmtHalfBath&#39;])) df[&#39;Total_porch_sf&#39;] = (df[&#39;OpenPorchSF&#39;] + df[&#39;3SsnPorch&#39;] + df[&#39;EnclosedPorch&#39;] + df[&#39;ScreenPorch&#39;] + df[&#39;WoodDeckSF&#39;]) # Add simplified categorical features df[&#39;haspool&#39;] = df[&#39;PoolArea&#39;].apply(lambda x: 1 if x &gt; 0 else 0) df[&#39;hasgarage&#39;] = df[&#39;GarageArea&#39;].apply(lambda x: 1 if x &gt; 0 else 0) df[&#39;hasbsmt&#39;] = df[&#39;TotalBsmtSF&#39;].apply(lambda x: 1 if x &gt; 0 else 0) df[&#39;hasfireplace&#39;] = df[&#39;Fireplaces&#39;].apply(lambda x: 1 if x &gt; 0 else 0) df[[&quot;haspool&quot;, &quot;hasgarage&quot;, &quot;hasbsmt&quot;, &quot;hasfireplace&quot;]] = df[[&quot;haspool&quot;, &quot;hasgarage&quot;, &quot;hasbsmt&quot;, &quot;hasfireplace&quot;]].astype(&quot;object&quot;) return df . Prepare the data . Now, we will combine all we have learnt and done in the previous sections to process the data on which an ML algorithm can be trained. We&#39;ll . normalize target | impute null values | add features | drop features | onehotencode categorical features | normalize skewed numeric features | scale numeric features | . def get_prepared_data(transform_numeric = True): X_trn = train.copy() y_trn = X_trn.pop(&quot;SalePrice&quot;) X_tst = test.copy() X_trn[&#39;MSSubClass&#39;] = X_trn[&#39;MSSubClass&#39;].astype(&quot;object&quot;) X_tst[&#39;MSSubClass&#39;] = X_tst[&#39;MSSubClass&#39;].astype(&quot;object&quot;) # normalize target y_trn = np.log1p(y_trn) # handle null values X_trn, y_trn = drop_electrical_null(X_trn, y_trn) X_trn = pd.DataFrame(null_ct.fit_transform(X_trn)) X_tst = pd.DataFrame(null_ct.transform(X_tst)) X_trn = X_trn.infer_objects() X_tst = X_tst.infer_objects() # re add column names X_trn.columns = null_ct_features_in X_tst.columns = null_ct_features_in X_trn[&#39;MSSubClass&#39;] = X_trn[&#39;MSSubClass&#39;].astype(&quot;object&quot;) X_tst[&#39;MSSubClass&#39;] = X_tst[&#39;MSSubClass&#39;].astype(&quot;object&quot;) # add features X_trn = add_cols(X_trn) X_tst = add_cols(X_tst) # drop features X_trn.drop(columns = [&quot;MasVnrArea&quot;, &quot;OpenPorchSF&quot;, &quot;BsmtFinSF1&quot;, &quot;WoodDeckSF&quot;, &quot;2ndFlrSF&quot;], inplace = True) X_tst.drop(columns = [&quot;MasVnrArea&quot;, &quot;OpenPorchSF&quot;, &quot;BsmtFinSF1&quot;, &quot;WoodDeckSF&quot;, &quot;2ndFlrSF&quot;], inplace = True) # categorical features cat_cols = X_trn.select_dtypes(include = [&quot;object&quot;]).columns cat_ohe = OneHotEncoder(drop = &#39;first&#39;, handle_unknown = &#39;ignore&#39;, sparse = False, dtype = &#39;uint8&#39;) # normalize numeric features num_cols = X_trn.select_dtypes(exclude = [&quot;object&quot;]).columns num_pipe = make_pipeline(Normalizer(), RobustScaler()) # column transformer if transform_numeric: ct = make_column_transformer((cat_ohe, cat_cols), (num_pipe, num_cols)) else: ct = make_column_transformer((cat_ohe, cat_cols), remainder = &quot;passthrough&quot;) X_trn = pd.DataFrame(ct.fit_transform(X_trn)) X_tst = pd.DataFrame(ct.transform(X_tst)) return X_trn, y_trn, X_tst . X_trn, y_trn, X_tst = get_prepared_data(True) . Now, we can move on to ML model training. . Train a Baseline Model and Evaluate Performance . We&#39;ll train a dummy classifier to establish a baseline score. This will give us a score value, which our future models should at least beat. It helps to identify errors in training. . base_model = DummyRegressor() . def evaluate_model(model, X_trn = X_trn): cvs = cross_val_score(model, X_trn, y_trn, scoring = &quot;neg_mean_squared_error&quot;) rmsle = np.sqrt(-cvs.mean()) print(f&quot;Model RMSLE: {rmsle:.5f}&quot;) evaluate_model(base_model) . Model RMSLE: 0.39968 . Make submission . base_model.fit(X_trn, y_trn) # make predictions base_preds = base_model.predict(X_tst) . submission_df[&quot;SalePrice&quot;] = base_preds submission_df.to_csv(&quot;baseline_model.csv&quot;, index = None) . The submission gives use a score of 0.42578. The future ML models should at least beat the cv rmsle score of 0.39968 and submission rmsle score of 0.42578. . Linear Models . First we&#39;ll train some linear models and compare their performance and make decision on training a final one. . k_folds = KFold(5, shuffle = True, random_state = seed) # parameters for cv e_alphas = np.arange(0.0001, 0.0007, 0.0001) e_l1ratio = np.arange(0.8, 1, 0.05) alphas_ridge = np.arange(10, 16, 0.5) alphas_lasso = np.arange(0.0001, 0.0008, 0.0001) . ridge = RidgeCV(alphas = alphas_ridge, scoring = &quot;neg_mean_squared_error&quot;, cv = k_folds) lasso = LassoCV(alphas = alphas_lasso, max_iter = 1e6, cv = k_folds, n_jobs = -1, random_state = seed) elastic_net = ElasticNetCV(l1_ratio = e_l1ratio, alphas = e_alphas, max_iter = 1e6, cv = k_folds, n_jobs = -1, random_state = seed) models = {&quot;Ridge&quot;: ridge, &quot;Lasso&quot;: lasso, &quot;ElasticNet&quot;: elastic_net} . scores = {} for model_name, model in models.items(): print(f&quot;{model_name}:&quot;) score = np.sqrt(-cross_val_score(model, X_trn, y_trn, scoring = &quot;neg_mean_squared_error&quot;, cv = k_folds)) print(score) print(f&quot;RMSLE mean: {score.mean():.5f} nRMSLE std: {score.std():.5f}&quot;) print(&quot;-&quot; * 50) scores[model_name] = (score.mean(), score.std()) . Ridge: [0.13595902 0.15096778 0.13826833 0.13605681 0.12490943] RMSLE mean: 0.13723 RMSLE std: 0.00830 -- Lasso: [0.13737072 0.15284838 0.13888969 0.13586582 0.12556263] RMSLE mean: 0.13811 RMSLE std: 0.00873 -- ElasticNet: [0.13721347 0.15238829 0.13744699 0.13594653 0.12550764] RMSLE mean: 0.13770 RMSLE std: 0.00858 -- . All these scores are improvements over baseline score and achieve rmsle scores in the range of 0.12-0.15. We can get a better score by blending all these linear models. Blending makes theses models complement each other and reduce their individual overfits. . Blended Model . %%time print(&quot;training started...&quot;) # train all models ridge.fit(X_trn, y_trn) lasso.fit(X_trn, y_trn) elastic_net.fit(X_trn, y_trn) print(&quot;training complete&quot;) . training started... training complete CPU times: total: 8.7 s Wall time: 2.62 s . Make Submission . blended_preds = (ridge.predict(X_tst) + lasso.predict(X_tst) + elastic_net.predict(X_tst))/3 blended_preds = np.expm1(blended_preds) . submission_df[&quot;SalePrice&quot;] = blended_preds submission_df.to_csv(&quot;blended_linear.csv&quot;, index = None) . This submission from blended linear models give us a rmsle score of 0.13819 which is similar to the performance of a single lasso model. But we can improve more on this score by using gradient boosting trees, which we&#39;ll do in the next section. . Gradient Boosting Models . In this section, first we&#39;ll train a lightgbm model and then we&#39;ll train an xgboost model. Normalizing and scaling that we applied on numeric features earlier deteriorates the performance of gradient boosting trees, which can actually use the information lost through transformation. Therefore, To train these models, we&#39;ll reload the processed data, this time without transforming the numeric features. . X_trn, y_trn, X_tst = get_prepared_data(False) . LightGBM - Train and Evaluate . core_count = psutil.cpu_count(logical = False) core_count . 4 . param = {&quot;bagging_fraction&quot;: 0.8, &quot;bagging_freq&quot;: 2, &quot;learning_rate&quot;: 0.01, &quot;num_leaves&quot;: 10, &quot;max_depth&quot;: 5, &quot;min_data_in_leaf&quot;: 10, &quot;metric&quot;: &quot;rmse&quot;, &quot;num_threads&quot;: core_count, &quot;verbosity&quot;: -1} . # train and evaluate lightgbm val_scores = [] i = 1 for trn_idx, val_idx in k_folds.split(X_trn, y_trn): print(f&quot;Split {i}:&quot;) trn = lgb.Dataset(X_trn.iloc[trn_idx], y_trn.iloc[trn_idx]) val = lgb.Dataset(X_trn.iloc[val_idx], y_trn.iloc[val_idx]) bst = lgb.train(param, trn, num_boost_round = 3000, valid_sets = [trn, val], early_stopping_rounds = 10, verbose_eval = 50) score = bst.best_score[&quot;valid_1&quot;][&quot;rmse&quot;] val_scores.append(score) print(f&quot;RMSLE: {score:.5f}&quot;) print(&quot;-&quot; * 65) i += 1 . Split 1: Training until validation scores don&#39;t improve for 10 rounds [50] training&#39;s rmse: 0.2779 valid_1&#39;s rmse: 0.294501 [100] training&#39;s rmse: 0.208419 valid_1&#39;s rmse: 0.22513 [150] training&#39;s rmse: 0.168045 valid_1&#39;s rmse: 0.186805 [200] training&#39;s rmse: 0.143163 valid_1&#39;s rmse: 0.164704 [250] training&#39;s rmse: 0.126932 valid_1&#39;s rmse: 0.151274 [300] training&#39;s rmse: 0.115682 valid_1&#39;s rmse: 0.14419 [350] training&#39;s rmse: 0.10737 valid_1&#39;s rmse: 0.139166 [400] training&#39;s rmse: 0.101333 valid_1&#39;s rmse: 0.135577 [450] training&#39;s rmse: 0.0965005 valid_1&#39;s rmse: 0.133299 [500] training&#39;s rmse: 0.0926217 valid_1&#39;s rmse: 0.131744 [550] training&#39;s rmse: 0.0891204 valid_1&#39;s rmse: 0.130774 [600] training&#39;s rmse: 0.0863924 valid_1&#39;s rmse: 0.12962 [650] training&#39;s rmse: 0.0838749 valid_1&#39;s rmse: 0.129178 Early stopping, best iteration is: [644] training&#39;s rmse: 0.0840888 valid_1&#39;s rmse: 0.129117 RMSLE: 0.12912 -- Split 2: Training until validation scores don&#39;t improve for 10 rounds [50] training&#39;s rmse: 0.273782 valid_1&#39;s rmse: 0.313671 [100] training&#39;s rmse: 0.205466 valid_1&#39;s rmse: 0.244458 [150] training&#39;s rmse: 0.165209 valid_1&#39;s rmse: 0.204575 [200] training&#39;s rmse: 0.140476 valid_1&#39;s rmse: 0.181242 [250] training&#39;s rmse: 0.124644 valid_1&#39;s rmse: 0.166237 [300] training&#39;s rmse: 0.113833 valid_1&#39;s rmse: 0.157775 [350] training&#39;s rmse: 0.105639 valid_1&#39;s rmse: 0.151577 [400] training&#39;s rmse: 0.0995413 valid_1&#39;s rmse: 0.148238 [450] training&#39;s rmse: 0.0946096 valid_1&#39;s rmse: 0.145141 Early stopping, best iteration is: [484] training&#39;s rmse: 0.0917289 valid_1&#39;s rmse: 0.144245 RMSLE: 0.14425 -- Split 3: Training until validation scores don&#39;t improve for 10 rounds [50] training&#39;s rmse: 0.27963 valid_1&#39;s rmse: 0.281509 [100] training&#39;s rmse: 0.208148 valid_1&#39;s rmse: 0.220384 [150] training&#39;s rmse: 0.16596 valid_1&#39;s rmse: 0.188637 [200] training&#39;s rmse: 0.139878 valid_1&#39;s rmse: 0.171665 [250] training&#39;s rmse: 0.123095 valid_1&#39;s rmse: 0.162633 [300] training&#39;s rmse: 0.11184 valid_1&#39;s rmse: 0.158664 [350] training&#39;s rmse: 0.103994 valid_1&#39;s rmse: 0.156169 [400] training&#39;s rmse: 0.0980702 valid_1&#39;s rmse: 0.154727 [450] training&#39;s rmse: 0.0935271 valid_1&#39;s rmse: 0.153839 Early stopping, best iteration is: [466] training&#39;s rmse: 0.0922914 valid_1&#39;s rmse: 0.15353 RMSLE: 0.15353 -- Split 4: Training until validation scores don&#39;t improve for 10 rounds [50] training&#39;s rmse: 0.279398 valid_1&#39;s rmse: 0.284018 [100] training&#39;s rmse: 0.208896 valid_1&#39;s rmse: 0.220487 [150] training&#39;s rmse: 0.16791 valid_1&#39;s rmse: 0.185005 [200] training&#39;s rmse: 0.14287 valid_1&#39;s rmse: 0.165001 [250] training&#39;s rmse: 0.126724 valid_1&#39;s rmse: 0.153032 [300] training&#39;s rmse: 0.115538 valid_1&#39;s rmse: 0.145386 [350] training&#39;s rmse: 0.107436 valid_1&#39;s rmse: 0.140329 [400] training&#39;s rmse: 0.10126 valid_1&#39;s rmse: 0.136382 [450] training&#39;s rmse: 0.096186 valid_1&#39;s rmse: 0.133607 [500] training&#39;s rmse: 0.0921356 valid_1&#39;s rmse: 0.131694 [550] training&#39;s rmse: 0.0886764 valid_1&#39;s rmse: 0.12993 [600] training&#39;s rmse: 0.0858351 valid_1&#39;s rmse: 0.129052 Early stopping, best iteration is: [593] training&#39;s rmse: 0.0862509 valid_1&#39;s rmse: 0.129002 RMSLE: 0.12900 -- Split 5: Training until validation scores don&#39;t improve for 10 rounds [50] training&#39;s rmse: 0.286016 valid_1&#39;s rmse: 0.2487 [100] training&#39;s rmse: 0.21402 valid_1&#39;s rmse: 0.188862 [150] training&#39;s rmse: 0.171467 valid_1&#39;s rmse: 0.156549 [200] training&#39;s rmse: 0.145601 valid_1&#39;s rmse: 0.139452 [250] training&#39;s rmse: 0.129239 valid_1&#39;s rmse: 0.130076 [300] training&#39;s rmse: 0.117925 valid_1&#39;s rmse: 0.124245 [350] training&#39;s rmse: 0.109824 valid_1&#39;s rmse: 0.120353 [400] training&#39;s rmse: 0.103684 valid_1&#39;s rmse: 0.118195 [450] training&#39;s rmse: 0.0986858 valid_1&#39;s rmse: 0.116684 [500] training&#39;s rmse: 0.0947134 valid_1&#39;s rmse: 0.115513 [550] training&#39;s rmse: 0.0912204 valid_1&#39;s rmse: 0.114181 [600] training&#39;s rmse: 0.0884721 valid_1&#39;s rmse: 0.113515 Early stopping, best iteration is: [632] training&#39;s rmse: 0.0869814 valid_1&#39;s rmse: 0.113224 RMSLE: 0.11322 -- . . np.mean(val_scores) . 0.13382349293051415 . Even without hyperparameter tuning, the validation scores are better than those of linear models. Now, we&#39;ll train on the whole dataset and make a submission. . trn = lgb.Dataset(X_trn, y_trn) lgb_cv = lgb.cv(param, trn, num_boost_round = 3000, folds = k_folds, early_stopping_rounds = 10) lgb_cv[&quot;rmse-mean&quot;][-1] . 0.13197370164429242 . bst = lgb.train(param, trn, num_boost_round = len(lgb_cv[&quot;rmse-mean&quot;])) # make predictions lgb_preds = np.expm1(bst.predict(X_tst)) . Make submission . submission_df[&quot;SalePrice&quot;] = lgb_preds submission_df.to_csv(&quot;lgb.csv&quot;, index = None) . This submission gives us a score of 0.12901, which is an improvement over the last submission. Now, we can further optimize it with hyperparameter tuning. . LightGBM - Hyperparameter tuning . We&#39;ll use Bayesian Optimization to tune the hyperparameters in this section and then we&#39;ll make a submssion. . def LGB_bayesian(bagging_fraction, bagging_freq, lambda_l1, lambda_l2, learning_rate, max_depth, min_data_in_leaf, min_gain_to_split, min_sum_hessian_in_leaf, num_leaves, feature_fraction): # LightGBM expects these parameters to be integer. So we make them integer bagging_freq = int(bagging_freq) num_leaves = int(num_leaves) min_data_in_leaf = int(min_data_in_leaf) max_depth = int(max_depth) # parameters param = {&#39;bagging_fraction&#39;: bagging_fraction, &#39;bagging_freq&#39;: bagging_freq, &#39;lambda_l1&#39;: lambda_l1, &#39;lambda_l2&#39;: lambda_l2, &#39;learning_rate&#39;: learning_rate, &#39;max_depth&#39;: max_depth, &#39;min_data_in_leaf&#39;: min_data_in_leaf, &#39;min_gain_to_split&#39;: min_gain_to_split, &#39;min_sum_hessian_in_leaf&#39;: min_sum_hessian_in_leaf, &#39;num_leaves&#39;: num_leaves, &#39;feature_fraction&#39;: feature_fraction, &#39;seed&#39;: seed, &#39;feature_fraction_seed&#39;: seed, &#39;bagging_seed&#39;: seed, &#39;drop_seed&#39;: seed, &#39;boosting_type&#39;: &#39;gbdt&#39;, &#39;metric&#39;: &#39;rmse&#39;, &#39;verbosity&#39;: -1, &#39;num_threads&#39;: core_count} trn = lgb.Dataset(X_trn, y_trn) lgb_cv = lgb.cv(param, trn, num_boost_round = 1500, folds = k_folds, stratified = False, early_stopping_rounds = 10, seed = seed) score = lgb_cv[&quot;rmse-mean&quot;][-1] return 1/score . bounds_LGB = { &#39;bagging_fraction&#39;: (0.5, 1), &#39;bagging_freq&#39;: (1, 4), &#39;lambda_l1&#39;: (0, 3.0), &#39;lambda_l2&#39;: (0, 3.0), &#39;learning_rate&#39;: (0.005, 0.3), &#39;max_depth&#39;:(3,8), &#39;min_data_in_leaf&#39;: (5, 20), &#39;min_gain_to_split&#39;: (0, 1), &#39;min_sum_hessian_in_leaf&#39;: (0.01, 20), &#39;num_leaves&#39;: (5, 20), &#39;feature_fraction&#39;: (0.05, 1) } . LG_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state = seed) . # find the best hyperparameters LG_BO.maximize(init_points = 10, n_iter = 200) . | iter | target | baggin... | baggin... | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | min_da... | min_ga... | min_su... | num_le... | - | 1 | 6.338 | 0.6873 | 3.852 | 0.7454 | 1.796 | 0.4681 | 0.05102 | 3.29 | 17.99 | 0.6011 | 14.16 | 5.309 | | 2 | 6.913 | 0.985 | 3.497 | 0.2517 | 0.5455 | 0.5502 | 0.09475 | 5.624 | 11.48 | 0.2912 | 12.24 | 7.092 | | 3 | 6.262 | 0.6461 | 2.099 | 0.4833 | 2.356 | 0.599 | 0.1567 | 5.962 | 5.697 | 0.6075 | 3.419 | 5.976 | | 4 | 6.481 | 0.9744 | 3.897 | 0.818 | 0.9138 | 0.293 | 0.2068 | 5.201 | 6.831 | 0.4952 | 0.6974 | 18.64 | | 5 | 6.091 | 0.6294 | 2.988 | 0.3461 | 1.56 | 1.64 | 0.05953 | 7.848 | 16.63 | 0.9395 | 17.9 | 13.97 | | 6 | 7.011 | 0.9609 | 1.265 | 0.2362 | 0.1357 | 0.976 | 0.1197 | 4.357 | 17.43 | 0.3568 | 5.626 | 13.14 | | 7 | 5.948 | 0.5705 | 3.407 | 0.1208 | 2.961 | 2.317 | 0.06362 | 3.028 | 17.23 | 0.7069 | 14.58 | 16.57 | | 8 | 6.32 | 0.537 | 2.075 | 0.1601 | 2.589 | 1.87 | 0.1026 | 3.318 | 9.665 | 0.3252 | 14.59 | 14.56 | | 9 | 6.456 | 0.9436 | 2.417 | 0.1636 | 2.14 | 2.282 | 0.1706 | 6.855 | 12.41 | 0.5227 | 8.557 | 5.381 | | 10 | 6.361 | 0.5539 | 1.094 | 0.6546 | 0.9431 | 1.526 | 0.2727 | 4.246 | 11.16 | 0.7556 | 4.584 | 6.155 | | 11 | 7.721 | 1.0 | 2.125 | 0.1412 | 0.0 | 0.0 | 0.06735 | 5.043 | 15.68 | 0.0 | 7.024 | 11.51 | | 12 | 7.152 | 1.0 | 3.976 | 0.05 | 0.0 | 0.0 | 0.005 | 6.074 | 15.12 | 0.0 | 7.333 | 11.65 | | 13 | 7.117 | 1.0 | 1.0 | 0.05 | 0.0 | 0.0 | 0.005 | 4.35 | 15.28 | 0.0 | 8.484 | 10.99 | | 14 | 7.192 | 1.0 | 1.735 | 0.3443 | 0.0 | 0.0 | 0.3 | 5.697 | 15.98 | 0.0 | 5.747 | 10.45 | | 15 | 7.484 | 1.0 | 2.551 | 0.1004 | 0.0 | 0.0 | 0.005 | 3.799 | 14.85 | 0.0 | 6.269 | 12.16 | | 16 | 7.229 | 1.0 | 1.369 | 0.05 | 0.0 | 0.0 | 0.005 | 5.841 | 14.82 | 0.0 | 6.828 | 13.1 | | 17 | 7.598 | 0.5 | 3.165 | 1.0 | 0.0 | 0.0 | 0.005 | 3.953 | 16.9 | 0.0 | 7.403 | 11.63 | | 18 | 6.571 | 1.0 | 2.755 | 0.05 | 2.317 | 0.0 | 0.005 | 4.365 | 16.39 | 0.0 | 7.109 | 11.7 | | 19 | 7.101 | 0.5 | 2.554 | 1.0 | 0.0 | 1.657 | 0.3 | 4.573 | 15.56 | 0.0 | 7.148 | 11.65 | | 20 | 7.5 | 1.0 | 2.199 | 1.0 | 0.0 | 0.0 | 0.005 | 5.806 | 17.47 | 0.0 | 7.955 | 12.02 | | 21 | 7.663 | 0.5 | 3.658 | 1.0 | 0.0 | 0.0 | 0.005 | 4.533 | 19.86 | 0.0 | 7.692 | 10.71 | | 22 | 6.163 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.511 | 20.0 | 1.0 | 8.371 | 12.92 | | 23 | 7.701 | 0.5 | 3.072 | 1.0 | 0.0 | 0.0 | 0.005 | 5.102 | 18.32 | 0.0 | 7.531 | 9.631 | | 24 | 7.628 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 6.035 | 20.0 | 0.0 | 6.098 | 9.744 | | 25 | 7.427 | 1.0 | 3.651 | 1.0 | 0.0 | 0.0 | 0.005 | 3.433 | 20.0 | 0.0 | 6.478 | 8.486 | | 26 | 7.494 | 1.0 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 6.458 | 20.0 | 0.0 | 8.892 | 8.96 | | 27 | 7.722 | 0.5 | 1.094 | 1.0 | 0.0 | 0.0 | 0.005 | 5.619 | 20.0 | 0.0 | 7.365 | 9.731 | | 28 | 7.016 | 0.5 | 2.615 | 0.05 | 0.0 | 2.514 | 0.005 | 5.673 | 20.0 | 0.0 | 7.401 | 9.38 | | 29 | 7.696 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 6.844 | 6.929 | | 30 | 7.641 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 3.315 | 5.912 | | 31 | 6.112 | 0.5 | 3.916 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 1.0 | 5.31 | 5.0 | | 32 | 7.713 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 4.486 | 9.099 | | 33 | 7.736 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 0.4313 | 8.136 | | 34 | 7.696 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 4.89 | 20.0 | 0.0 | 2.015 | 7.902 | | 35 | 6.966 | 0.5 | 1.0 | 1.0 | 3.0 | 0.0 | 0.005 | 7.259 | 20.0 | 0.0 | 1.833 | 7.665 | | 36 | 7.658 | 0.5 | 1.0 | 1.0 | 0.0 | 2.869 | 0.005 | 6.494 | 20.0 | 0.0 | 0.01 | 6.427 | | 37 | 7.51 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 5.559 | 20.0 | 0.0 | 0.01 | 5.0 | | 38 | 7.705 | 0.5 | 1.0 | 1.0 | 0.0 | 2.56 | 0.005 | 6.076 | 20.0 | 0.0 | 0.01 | 10.63 | | 39 | 7.714 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 0.01 | 12.97 | | 40 | 7.618 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 5.622 | 20.0 | 0.0 | 0.01 | 11.08 | | 41 | 7.543 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 20.0 | 0.0 | 0.01 | 13.77 | | 42 | 7.713 | 0.5 | 1.0 | 1.0 | 0.0 | 0.647 | 0.005 | 8.0 | 20.0 | 0.0 | 0.01 | 17.94 | | 43 | 6.926 | 0.5 | 3.143 | 1.0 | 3.0 | 3.0 | 0.005 | 8.0 | 20.0 | 0.0 | 0.01 | 19.14 | | 44 | 7.717 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 16.55 | 0.0 | 0.01 | 15.91 | | 45 | 7.703 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 4.411 | 19.77 | 0.0 | 0.01 | 15.51 | | 46 | 6.15 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 6.572 | 18.47 | 1.0 | 0.01 | 16.1 | | 47 | 7.585 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 3.0 | 20.0 | 0.0 | 0.01 | 13.47 | | 48 | 7.685 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 20.0 | 0.0 | 0.01 | 15.03 | | 49 | 7.52 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 15.19 | 0.0 | 0.01 | 18.18 | | 50 | 7.044 | 0.5 | 1.0 | 0.05 | 0.0 | 0.0 | 0.005 | 3.0 | 20.0 | 0.0 | 0.01 | 11.03 | | 51 | 7.751 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 16.68 | 0.0 | 0.01 | 12.46 | | 52 | 7.68 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 20.0 | 0.0 | 0.01 | 9.493 | | 53 | 7.652 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 20.0 | 0.0 | 2.526 | 12.02 | | 54 | 7.379 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 4.384 | 15.97 | 0.0 | 0.01 | 15.01 | | 55 | 6.92 | 0.5 | 1.0 | 1.0 | 3.0 | 3.0 | 0.005 | 8.0 | 19.11 | 0.0 | 0.01 | 12.79 | | 56 | 7.122 | 0.5 | 1.0 | 0.05 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 3.681 | 20.0 | | 57 | 6.481 | 0.9844 | 3.004 | 0.461 | 0.1131 | 0.5129 | 0.09865 | 7.624 | 17.4 | 0.9014 | 0.1111 | 9.671 | | 58 | 7.112 | 1.0 | 1.0 | 1.0 | 3.0 | 0.0 | 0.3 | 8.0 | 5.0 | 0.0 | 20.0 | 5.0 | | 59 | 7.555 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 3.979 | 20.0 | 0.0 | 0.01 | 8.798 | | 60 | 7.585 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 3.0 | 20.0 | 0.0 | 0.01 | 18.13 | | 61 | 6.396 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 5.0 | 1.0 | 10.39 | 20.0 | | 62 | 7.206 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.3 | 3.0 | 20.0 | 0.0 | 2.043 | 5.0 | | 63 | 7.122 | 0.5 | 1.0 | 0.05 | 0.0 | 3.0 | 0.3 | 3.0 | 15.63 | 0.0 | 0.01 | 20.0 | | 64 | 7.759 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 13.28 | 0.0 | 0.01 | 14.36 | | 65 | 6.188 | 0.5 | 1.0 | 0.05 | 0.0 | 0.0 | 0.3 | 8.0 | 12.68 | 0.0 | 0.01 | 16.17 | | 66 | 7.683 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 16.53 | 0.0 | 1.6 | 15.2 | | 67 | 6.994 | 1.0 | 1.0 | 1.0 | 3.0 | 0.0 | 0.3 | 3.0 | 20.0 | 0.0 | 0.01 | 18.09 | | 68 | 7.713 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 9.356 | 9.508 | | 69 | 6.969 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.3 | 5.246 | 20.0 | 0.0 | 1.954 | 15.77 | | 70 | 6.964 | 0.5 | 1.0 | 1.0 | 3.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 8.347 | 8.476 | | 71 | 7.497 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 12.7 | 0.0 | 0.01 | 11.39 | | 72 | 7.27 | 0.7003 | 2.9 | 0.9973 | 0.1094 | 2.975 | 0.09622 | 3.103 | 5.03 | 0.1283 | 19.92 | 5.92 | | 73 | 6.659 | 0.5 | 4.0 | 0.05 | 0.0 | 0.0 | 0.3 | 3.0 | 5.0 | 0.0 | 14.83 | 5.0 | | 74 | 7.147 | 0.5 | 1.0 | 1.0 | 2.124 | 3.0 | 0.005 | 8.0 | 14.89 | 0.0 | 0.01 | 14.45 | | 75 | 7.766 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 14.1 | 0.0 | 2.427 | 13.15 | | 76 | 7.597 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 5.19 | 20.0 | 0.0 | 0.01 | 11.81 | | 77 | 7.691 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 20.0 | 0.0 | 2.412 | 8.24 | | 78 | 5.994 | 1.0 | 4.0 | 1.0 | 3.0 | 3.0 | 0.3 | 8.0 | 5.0 | 1.0 | 20.0 | 20.0 | | 79 | 6.312 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.3 | 8.0 | 11.0 | 1.0 | 20.0 | 5.0 | | 80 | 7.384 | 1.0 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 3.0 | 5.0 | 0.0 | 20.0 | 11.43 | | 81 | 6.07 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 5.0 | 1.0 | 20.0 | 10.16 | | 82 | 7.696 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 11.45 | 6.089 | | 83 | 6.986 | 1.0 | 1.0 | 1.0 | 3.0 | 0.0 | 0.005 | 3.0 | 5.0 | 0.0 | 20.0 | 9.878 | | 84 | 6.952 | 0.5 | 1.0 | 0.05 | 3.0 | 0.0 | 0.3 | 8.0 | 20.0 | 0.0 | 20.0 | 5.0 | | 85 | 6.961 | 1.0 | 1.0 | 0.05 | 0.0 | 3.0 | 0.005 | 8.0 | 20.0 | 0.0 | 13.09 | 5.0 | | 86 | 7.093 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.3 | 8.0 | 20.0 | 0.0 | 13.16 | 9.104 | | 87 | 7.46 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 3.0 | 5.0 | 0.0 | 0.01 | 11.73 | | 88 | 7.469 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 5.0 | 0.0 | 0.01 | 12.26 | | 89 | 6.317 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 3.0 | 5.0 | 1.0 | 0.01 | 13.96 | | 90 | 6.38 | 0.5 | 4.0 | 0.05542 | 2.958 | 3.0 | 0.005 | 8.0 | 5.0 | 0.0 | 0.01 | 11.45 | | 91 | 7.706 | 0.5 | 3.738 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 13.74 | 0.0 | 0.9225 | 13.42 | | 92 | 7.185 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.3 | 5.567 | 20.0 | 0.0 | 9.453 | 6.733 | | 93 | 7.481 | 1.0 | 2.642 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 10.85 | 0.0 | 2.701 | 13.31 | | 94 | 7.577 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 3.931 | 11.69 | 0.0 | 0.01 | 12.14 | | 95 | 6.869 | 0.5 | 4.0 | 0.05 | 0.0 | 3.0 | 0.005 | 3.0 | 5.0 | 0.0 | 6.951 | 12.17 | | 96 | 6.988 | 1.0 | 4.0 | 1.0 | 3.0 | 3.0 | 0.3 | 3.0 | 10.07 | 0.0 | 0.01 | 10.4 | | 97 | 7.409 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 5.0 | 0.0 | 0.01 | 9.278 | | 98 | 6.397 | 0.5 | 1.611 | 1.0 | 0.0 | 3.0 | 0.005 | 5.695 | 12.98 | 1.0 | 1.15 | 13.16 | | 99 | 7.153 | 0.5 | 2.703 | 1.0 | 0.0 | 1.932 | 0.3 | 6.105 | 20.0 | 0.0 | 2.283 | 9.836 | | 100 | 7.564 | 1.0 | 1.0 | 1.0 | 0.0 | 1.399 | 0.005 | 8.0 | 18.0 | 0.0 | 0.01 | 14.38 | | 101 | 7.541 | 1.0 | 1.0 | 1.0 | 0.0 | 2.267 | 0.005 | 8.0 | 17.94 | 0.0 | 0.01 | 17.56 | | 102 | 7.714 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 3.316 | 12.91 | | 103 | 7.532 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 14.19 | 0.0 | 3.933 | 16.08 | | 104 | 7.661 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 10.81 | 0.0 | 0.01 | 11.59 | | 105 | 7.555 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 6.718 | 11.51 | | 106 | 7.6 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 20.0 | 0.0 | 0.01 | 6.529 | | 107 | 6.931 | 0.5 | 4.0 | 1.0 | 3.0 | 0.0 | 0.005 | 3.0 | 20.0 | 0.0 | 0.01 | 5.0 | | 108 | 7.155 | 1.0 | 1.0 | 0.05 | 0.0 | 3.0 | 0.005 | 8.0 | 20.0 | 0.0 | 0.01 | 9.308 | | 109 | 7.771 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 15.34 | 0.0 | 3.898 | 20.0 | | 110 | 7.761 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 14.25 | 0.0 | 6.871 | 20.0 | | 111 | 6.885 | 1.0 | 1.0 | 1.0 | 2.867 | 3.0 | 0.005 | 8.0 | 14.4 | 0.0 | 5.394 | 20.0 | | 112 | 6.402 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 16.99 | 1.0 | 6.552 | 20.0 | | 113 | 7.744 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 12.91 | 0.0 | 4.779 | 19.72 | | 114 | 7.683 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 11.92 | 0.0 | 7.652 | 20.0 | | 115 | 7.698 | 0.5 | 3.461 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 13.06 | 0.0 | 6.217 | 20.0 | | 116 | 6.947 | 0.5 | 1.56 | 1.0 | 0.0 | 0.5025 | 0.3 | 8.0 | 13.04 | 0.0 | 6.418 | 20.0 | | 117 | 7.743 | 0.5 | 1.839 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 12.97 | 0.0 | 6.914 | 17.71 | | 118 | 7.721 | 0.5 | 1.572 | 1.0 | 0.0 | 3.0 | 0.005 | 5.583 | 13.05 | 0.0 | 6.423 | 19.37 | | 119 | 7.707 | 0.5 | 3.264 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 14.21 | 0.0 | 2.684 | 20.0 | | 120 | 7.446 | 1.0 | 3.063 | 1.0 | 0.0 | 3.0 | 0.005 | 6.612 | 13.0 | 0.0 | 9.332 | 19.46 | | 121 | 7.679 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 12.69 | 0.0 | 4.214 | 17.12 | | 122 | 7.637 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 4.938 | 13.18 | 0.0 | 4.092 | 20.0 | | 123 | 6.861 | 0.5 | 4.0 | 0.05 | 0.0 | 3.0 | 0.005 | 3.0 | 11.14 | 0.0 | 6.848 | 20.0 | | 124 | 7.72 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 5.764 | 14.13 | 0.0 | 3.12 | 20.0 | | 125 | 7.478 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 10.03 | 0.0 | 6.52 | 15.74 | | 126 | 6.526 | 0.5 | 4.0 | 0.05 | 0.0 | 3.0 | 0.3 | 8.0 | 10.66 | 0.0 | 7.016 | 17.2 | | 127 | 7.543 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 9.337 | 0.0 | 0.01 | 11.31 | | 128 | 7.5 | 1.0 | 2.932 | 1.0 | 0.0 | 3.0 | 0.005 | 6.703 | 14.46 | 0.0 | 4.53 | 18.53 | | 129 | 7.743 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 13.53 | 0.0 | 9.766 | 17.79 | | 130 | 7.124 | 0.5 | 1.0 | 0.05 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 1.39 | 15.51 | | 131 | 7.599 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 20.0 | 0.0 | 2.035 | 5.0 | | 132 | 7.701 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 17.46 | 0.0 | 0.01 | 20.0 | | 133 | 7.751 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 13.23 | 0.0 | 12.34 | 20.0 | | 134 | 6.433 | 0.8563 | 1.084 | 0.8246 | 1.864 | 2.859 | 0.2361 | 7.893 | 11.11 | 0.5258 | 13.22 | 19.96 | | 135 | 7.771 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 15.79 | 0.0 | 11.99 | 20.0 | | 136 | 7.737 | 0.5 | 1.0 | 1.0 | 0.0 | 0.1906 | 0.005 | 8.0 | 14.74 | 0.0 | 12.11 | 20.0 | | 137 | 7.155 | 1.0 | 1.0 | 1.0 | 0.0 | 1.618 | 0.3 | 8.0 | 15.76 | 0.0 | 14.82 | 20.0 | | 138 | 7.69 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 14.51 | 0.0 | 3.827 | 14.25 | | 139 | 7.645 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 20.0 | 0.0 | 20.0 | 20.0 | | 140 | 7.72 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 5.964 | 14.37 | 0.0 | 10.68 | 20.0 | | 141 | 6.144 | 0.703 | 3.009 | 0.9848 | 2.826 | 1.277 | 0.07818 | 4.065 | 19.83 | 0.578 | 19.77 | 19.91 | | 142 | 7.613 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 11.17 | 0.0 | 1.945 | 20.0 | | 143 | 7.005 | 0.5 | 1.0 | 0.05 | 0.0 | 3.0 | 0.005 | 8.0 | 14.04 | 0.0 | 10.12 | 20.0 | | 144 | 7.735 | 0.5 | 1.0 | 1.0 | 0.0 | 2.229 | 0.005 | 6.85 | 14.55 | 0.0 | 12.14 | 18.01 | | 145 | 7.53 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 13.48 | 0.0 | 8.942 | 15.1 | | 146 | 7.728 | 0.5 | 1.0 | 1.0 | 0.0 | 0.3714 | 0.005 | 4.877 | 14.62 | 0.0 | 11.85 | 20.0 | | 147 | 7.726 | 0.5 | 1.0 | 1.0 | 0.0 | 0.7395 | 0.005 | 6.036 | 17.27 | 0.0 | 11.66 | 20.0 | | 148 | 7.703 | 0.5 | 3.218 | 1.0 | 0.0 | 1.823 | 0.005 | 6.511 | 15.24 | 0.0 | 12.17 | 20.0 | | 149 | 7.691 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 5.666 | 20.0 | 0.0 | 0.01 | 20.0 | | 150 | 7.074 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.3 | 4.744 | 15.87 | 0.0 | 12.67 | 20.0 | | 151 | 7.727 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 5.209 | 13.45 | 0.0 | 9.067 | 17.37 | | 152 | 7.664 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.536 | 15.33 | 0.0 | 9.038 | 20.0 | | 153 | 7.688 | 0.5 | 3.665 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 17.47 | 0.0 | 11.57 | 20.0 | | 154 | 6.824 | 0.8679 | 1.189 | 0.6865 | 1.241 | 0.2776 | 0.2391 | 7.836 | 17.99 | 0.3011 | 11.28 | 18.28 | | 155 | 7.765 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 5.745 | 13.59 | 0.0 | 10.19 | 18.0 | | 156 | 7.637 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 12.1 | 0.0 | 10.06 | 20.0 | | 157 | 5.829 | 0.5 | 1.0 | 1.0 | 3.0 | 0.0 | 0.005 | 3.0 | 13.65 | 1.0 | 9.414 | 20.0 | | 158 | 7.722 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 14.14 | 0.0 | 11.83 | 18.6 | | 159 | 7.696 | 0.5 | 3.769 | 1.0 | 0.0 | 0.0 | 0.005 | 5.468 | 15.68 | 0.0 | 10.23 | 20.0 | | 160 | 7.735 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 12.44 | 0.0 | 11.57 | 16.4 | | 161 | 7.678 | 0.5 | 3.038 | 1.0 | 0.0 | 0.0 | 0.005 | 5.759 | 12.45 | 0.0 | 11.87 | 20.0 | | 162 | 7.651 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 19.93 | 0.0 | 10.31 | 20.0 | | 163 | 7.512 | 1.0 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 13.06 | 0.0 | 1.242 | 16.52 | | 164 | 6.866 | 0.5 | 1.0 | 0.05 | 0.0 | 0.0 | 0.005 | 3.0 | 11.23 | 0.0 | 20.0 | 20.0 | | 165 | 7.59 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 4.937 | 20.0 | 0.0 | 11.45 | 20.0 | | 166 | 7.716 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 9.52 | 0.0 | 10.1 | 13.67 | | 167 | 7.762 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 7.836 | 0.0 | 9.655 | 11.97 | | 168 | 7.466 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 5.822 | 0.0 | 10.12 | 11.86 | | 169 | 6.291 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 10.17 | 1.0 | 11.29 | 13.24 | | 170 | 7.75 | 0.5 | 1.0 | 1.0 | 0.0 | 1.089 | 0.005 | 8.0 | 7.938 | 0.0 | 7.208 | 12.35 | | 171 | 7.233 | 0.5 | 1.0 | 1.0 | 1.813 | 3.0 | 0.005 | 8.0 | 5.894 | 0.0 | 8.166 | 11.77 | | 172 | 7.7 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 5.798 | 8.182 | 0.0 | 8.413 | 14.1 | | 173 | 7.713 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 10.43 | 0.0 | 7.339 | 13.99 | | 174 | 7.71 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 6.127 | 10.56 | 0.0 | 9.516 | 16.69 | | 175 | 7.686 | 0.5 | 3.691 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 8.28 | 0.0 | 8.72 | 12.18 | | 176 | 7.045 | 0.5 | 1.0 | 1.0 | 2.637 | 0.0 | 0.005 | 8.0 | 8.554 | 0.0 | 8.412 | 13.73 | | 177 | 7.633 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 8.996 | 0.0 | 4.923 | 11.66 | | 178 | 7.69 | 0.5 | 3.174 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 8.15 | 0.0 | 7.805 | 10.37 | | 179 | 7.589 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 18.62 | 0.0 | 8.369 | 20.0 | | 180 | 7.134 | 0.5 | 2.777 | 1.0 | 0.0 | 0.0 | 0.3 | 3.0 | 17.39 | 0.0 | 11.15 | 20.0 | | 181 | 7.701 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 8.916 | 0.0 | 8.803 | 10.23 | | 182 | 7.734 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 6.595 | 0.0 | 10.12 | 8.767 | | 183 | 5.831 | 0.5883 | 3.886 | 0.9875 | 2.478 | 2.413 | 0.1524 | 7.782 | 5.016 | 0.955 | 11.68 | 6.71 | | 184 | 6.93 | 0.5 | 4.0 | 0.05 | 0.0 | 3.0 | 0.005 | 3.0 | 20.0 | 0.0 | 8.267 | 20.0 | | 185 | 7.645 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 20.0 | 0.0 | 6.37 | 20.0 | | 186 | 7.707 | 0.5 | 1.0 | 1.0 | 0.0 | 2.342 | 0.005 | 5.92 | 7.563 | 0.0 | 8.868 | 10.45 | | 187 | 7.467 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 9.747 | 0.0 | 6.339 | 11.01 | | 188 | 7.577 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 11.48 | 0.0 | 0.01 | 5.0 | | 189 | 7.543 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 3.0 | 14.29 | 0.0 | 0.01 | 5.0 | | 190 | 7.665 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 16.57 | 0.0 | 4.447 | 20.0 | | 191 | 7.431 | 1.0 | 2.88 | 1.0 | 0.0 | 0.0 | 0.005 | 5.406 | 9.268 | 0.0 | 7.44 | 11.86 | | 192 | 7.558 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 9.919 | 0.0 | 0.01 | 8.668 | | 193 | 7.376 | 1.0 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 10.94 | 0.0 | 0.01 | 5.0 | | 194 | 7.677 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 10.99 | 0.0 | 2.338 | 9.088 | | 195 | 7.14 | 0.5 | 1.0 | 0.05 | 0.0 | 2.317 | 0.005 | 8.0 | 7.301 | 0.0 | 8.58 | 10.18 | | 196 | 7.723 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 5.91 | 5.767 | 0.0 | 11.35 | 10.89 | | 197 | 6.866 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.3 | 4.858 | 7.589 | 0.0 | 10.84 | 11.05 | | 198 | 7.574 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 3.0 | 12.83 | 0.0 | 0.01 | 8.644 | | 199 | 7.548 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 16.21 | 0.0 | 5.393 | 20.0 | | 200 | 7.725 | 0.5 | 1.398 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 9.514 | 0.0 | 0.01 | 8.145 | | 201 | 7.081 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.3 | 8.0 | 20.0 | 0.0 | 0.01 | 5.348 | | 202 | 7.579 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 8.526 | 0.0 | 0.01 | 7.379 | | 203 | 7.704 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 10.12 | 0.0 | 7.341 | 9.8 | | 204 | 7.414 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 5.0 | 0.0 | 6.01 | 14.66 | | 205 | 7.69 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 5.959 | 6.401 | 0.0 | 8.928 | 13.36 | | 206 | 7.672 | 0.5 | 3.405 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 5.613 | 0.0 | 10.28 | 12.21 | | 207 | 7.677 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 7.738 | 0.0 | 0.01 | 10.18 | | 208 | 5.809 | 0.5 | 1.0 | 1.0 | 3.0 | 3.0 | 0.005 | 3.0 | 20.0 | 1.0 | 20.0 | 8.587 | | 209 | 7.649 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 12.52 | 0.0 | 0.01 | 8.392 | | 210 | 7.427 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 20.0 | 0.0 | 2.523 | 20.0 | ============================================================================================================================================================= . . tuned_lgbm_score = 1/LG_BO.max[&#39;target&#39;] print(f&quot;RMSLE of tuned lightgbm: {tuned_lgbm_score:.5f}&quot;) . RMSLE of tuned lightgbm: 0.12868 . params = LG_BO.max[&quot;params&quot;] int_params = [&quot;bagging_freq&quot;, &quot;max_depth&quot;, &quot;min_data_in_leaf&quot;, &quot;num_leaves&quot;] for parameter in int_params: params[parameter] = int(params[parameter]) other_lgbm_params = {&#39;seed&#39;: seed, &#39;feature_fraction_seed&#39;: seed, &#39;bagging_seed&#39;: seed, &#39;drop_seed&#39;: seed, &#39;boosting_type&#39;: &#39;gbdt&#39;, &#39;metric&#39;: &#39;rmse&#39;, &#39;verbosity&#39;: -1, &#39;num_threads&#39;: core_count} params.update(other_lgbm_params) params . {&#39;bagging_fraction&#39;: 0.5, &#39;bagging_freq&#39;: 1, &#39;feature_fraction&#39;: 1.0, &#39;lambda_l1&#39;: 0.0, &#39;lambda_l2&#39;: 3.0, &#39;learning_rate&#39;: 0.005, &#39;max_depth&#39;: 8, &#39;min_data_in_leaf&#39;: 15, &#39;min_gain_to_split&#39;: 0.0, &#39;min_sum_hessian_in_leaf&#39;: 3.8982726762658557, &#39;num_leaves&#39;: 20, &#39;seed&#39;: 42, &#39;feature_fraction_seed&#39;: 42, &#39;bagging_seed&#39;: 42, &#39;drop_seed&#39;: 42, &#39;boosting_type&#39;: &#39;gbdt&#39;, &#39;metric&#39;: &#39;rmse&#39;, &#39;verbosity&#39;: -1, &#39;num_threads&#39;: 4} . Train and Make Submission . trn = lgb.Dataset(X_trn, y_trn) lgb_cv = lgb.cv(params, trn, num_boost_round = 3000, folds = k_folds, early_stopping_rounds = 10) num_boost_round = len(lgb_cv[&quot;rmse-mean&quot;]) - 10 num_boost_round . 1479 . bst = lgb.train(params, trn, num_boost_round = num_boost_round) # make predictions lgb_preds = np.expm1(bst.predict(X_tst)) . submission_df[&quot;SalePrice&quot;] = lgb_preds submission_df.to_csv(&quot;lgb_tuned.csv&quot;, index = None) . This submission gives us a score of 0.12715. The hyperparameter tuning helped to improve the performance of the lightgb model by a little bit. Now, we&#39;ll see how xgboost performs. . XGBoost - Train and Evaluate . train_d = xgb.DMatrix(X_trn, y_trn) test_d = xgb.DMatrix(X_tst) . xgb_params = {&quot;eta&quot;: 0.1, &quot;subsample&quot;: 0.7, &quot;tree_method&quot;: &quot;hist&quot;, &quot;random_state&quot;: seed} . xgb_cv = xgb.cv(xgb_params, train_d, num_boost_round = 1500, nfold = 5, early_stopping_rounds = 10) xgb_cv.tail() . train-rmse-mean train-rmse-std test-rmse-mean test-rmse-std . 126 0.033865 | 0.001209 | 0.127286 | 0.018559 | . 127 0.033507 | 0.001256 | 0.127247 | 0.018577 | . 128 0.033214 | 0.001191 | 0.127263 | 0.018581 | . 129 0.032872 | 0.001175 | 0.127245 | 0.018490 | . 130 0.032607 | 0.001167 | 0.127240 | 0.018507 | . Even without hyperparameter tuning, xgboost is giving us a rmsle validation score of 0.127240. We can improve this score by hyperparameter tuning. First, we&#39;ll make predictions and a submission. . Make Submission . xgb_bst = xgb.train(xgb_params, train_d, num_boost_round = len(xgb_cv)) # make predictions xgb_preds = np.expm1(xgb_bst.predict(test_d)) . submission_df[&quot;SalePrice&quot;] = xgb_preds submission_df.to_csv(&quot;xgb_non_tuned.csv&quot;, index = None) . This submission gives a score of 0.13190, which looks like the model overfit a little bit. We can tune the hyperparameters and improve the performance. . XGBoost - Hyperparameter tuning . def xgb_bayesian(eta, gamma, subsample, colsample_bytree, colsample_bynode, colsample_bylevel, max_depth): # this parameter has to an integer max_depth = int(max_depth) # xgboost parameters params = {&quot;eta&quot;: eta, &quot;gamma&quot;: gamma, &quot;subsample&quot;: subsample, &quot;colsample_bytree&quot;: colsample_bytree, &quot;colsample_bynode&quot;: colsample_bynode, &quot;colsample_bylevel&quot;: colsample_bylevel, &quot;max_depth&quot;: max_depth, &quot;tree_method&quot;: &quot;hist&quot;} # train and score xgb_cv = xgb.cv(params, train_d, num_boost_round = 1500, nfold = 5, early_stopping_rounds = 10, seed = seed) score = xgb_cv.iloc[-10][&quot;test-rmse-mean&quot;] return 1/score . xgb_bounds = {&quot;eta&quot;: (0.01, 0.05), &quot;gamma&quot;: (0, 20), &quot;subsample&quot;: (0.4, 1), &quot;colsample_bytree&quot;: (0.5, 1), &quot;colsample_bynode&quot;: (0.5, 1), &quot;colsample_bylevel&quot;: (0.5, 1), &quot;max_depth&quot;: (2, 7)} . xgb_bo = BayesianOptimization(xgb_bayesian, xgb_bounds, random_state = seed) . # find the best hyperparameters xgb_bo.maximize(init_points = 3, n_iter = 60) . | iter | target | colsam... | colsam... | colsam... | eta | gamma | max_depth | subsample | - | 1 | 5.275 | 0.6873 | 0.9754 | 0.866 | 0.03395 | 3.12 | 2.78 | 0.4349 | | 2 | 3.383 | 0.9331 | 0.8006 | 0.854 | 0.01082 | 19.4 | 6.162 | 0.5274 | | 3 | 4.52 | 0.5909 | 0.5917 | 0.6521 | 0.03099 | 8.639 | 3.456 | 0.7671 | | 4 | 7.739 | 0.5 | 1.0 | 0.5 | 0.05 | 0.0 | 7.0 | 1.0 | | 5 | 5.792 | 1.0 | 0.5 | 1.0 | 0.01 | 2.192 | 7.0 | 1.0 | | 6 | 7.648 | 0.5 | 1.0 | 0.5 | 0.05 | 0.0 | 5.578 | 0.4 | | 7 | 7.691 | 0.5 | 1.0 | 0.5 | 0.05 | 0.0 | 2.0 | 1.0 | | 8 | 7.76 | 1.0 | 0.5 | 0.5 | 0.01 | 0.0 | 3.967 | 1.0 | | 9 | 7.818 | 0.5 | 0.5 | 1.0 | 0.05 | 0.0 | 5.085 | 1.0 | | 10 | 7.422 | 0.7925 | 0.8704 | 0.985 | 0.04765 | 0.008645 | 2.932 | 0.7795 | | 11 | 7.732 | 0.5 | 0.5 | 0.5 | 0.01 | 0.0 | 3.967 | 1.0 | | 12 | 7.775 | 1.0 | 1.0 | 0.5 | 0.01 | 0.0 | 5.477 | 1.0 | | 13 | 7.671 | 1.0 | 0.5 | 0.5 | 0.05 | 0.0 | 7.0 | 0.4 | | 14 | 7.869 | 0.5 | 0.5 | 0.5 | 0.05 | 0.0 | 6.08 | 1.0 | | 15 | 7.797 | 0.5413 | 0.7305 | 0.9571 | 0.0127 | 0.01644 | 6.651 | 0.9833 | | 16 | 7.615 | 1.0 | 0.5 | 0.5 | 0.01 | 0.0 | 2.0 | 1.0 | | 17 | 7.672 | 1.0 | 0.5 | 0.5 | 0.05 | 0.0 | 5.607 | 1.0 | | 18 | 4.074 | 0.5 | 0.5 | 1.0 | 0.05 | 14.46 | 2.0 | 1.0 | | 19 | 4.196 | 1.0 | 1.0 | 0.5 | 0.01 | 11.69 | 7.0 | 1.0 | | 20 | 7.66 | 0.8068 | 0.9216 | 0.5596 | 0.01745 | 0.04023 | 4.768 | 0.995 | | 21 | 3.822 | 0.5 | 0.5 | 0.5 | 0.05 | 20.0 | 2.0 | 1.0 | | 22 | 7.696 | 0.5 | 1.0 | 1.0 | 0.01 | 0.0 | 5.865 | 1.0 | | 23 | 7.522 | 0.7704 | 0.6646 | 0.5423 | 0.02316 | 0.07314 | 2.729 | 0.4044 | | 24 | 7.616 | 0.5 | 0.5 | 1.0 | 0.01 | 0.0 | 2.0 | 1.0 | | 25 | 7.566 | 0.5 | 0.5 | 0.5 | 0.01 | 0.0 | 2.0 | 0.4 | | 26 | 7.887 | 0.5 | 0.5 | 0.5 | 0.01 | 0.0 | 7.0 | 0.4 | | 27 | 6.545 | 0.5 | 0.5 | 0.5 | 0.01 | 0.8323 | 2.0 | 1.0 | | 28 | 4.306 | 0.5 | 1.0 | 0.5 | 0.01 | 6.463 | 7.0 | 0.4 | | 29 | 7.789 | 0.5 | 1.0 | 1.0 | 0.01 | 0.0 | 7.0 | 0.4 | | 30 | 7.773 | 0.5007 | 0.5555 | 0.7216 | 0.03146 | 0.08095 | 6.635 | 0.5173 | | 31 | 7.842 | 1.0 | 0.5 | 0.5 | 0.01 | 0.0 | 4.195 | 0.4 | | 32 | 7.491 | 0.9942 | 0.5431 | 0.9432 | 0.04836 | 0.03342 | 3.756 | 0.4115 | | 33 | 7.886 | 0.5 | 0.5 | 0.5 | 0.01 | 0.0 | 5.175 | 1.0 | | 34 | 7.679 | 1.0 | 1.0 | 0.5 | 0.01 | 0.0 | 3.824 | 0.4 | | 35 | 7.808 | 0.5 | 1.0 | 0.5 | 0.01 | 0.0 | 7.0 | 0.4 | | 36 | 7.789 | 0.5 | 0.5 | 0.5 | 0.01 | 0.0 | 4.712 | 0.4 | | 37 | 7.835 | 0.5 | 0.5 | 0.5 | 0.01 | 0.0 | 7.0 | 1.0 | | 38 | 7.833 | 0.5 | 0.5 | 1.0 | 0.01 | 0.0 | 6.063 | 1.0 | | 39 | 7.886 | 0.5 | 0.5 | 0.5 | 0.01 | 0.0 | 5.654 | 1.0 | | 40 | 7.762 | 0.5803 | 0.505 | 0.5571 | 0.02706 | 0.004659 | 6.536 | 0.8425 | | 41 | 7.706 | 0.5 | 0.5 | 1.0 | 0.05 | 0.0 | 7.0 | 0.4 | | 42 | 7.044 | 0.5109 | 0.5865 | 0.7321 | 0.03324 | 0.4898 | 6.953 | 0.8459 | | 43 | 4.434 | 1.0 | 0.5 | 1.0 | 0.01 | 6.029 | 2.0 | 0.4 | | 44 | 7.363 | 1.0 | 1.0 | 1.0 | 0.01 | 0.0 | 2.0 | 0.4 | | 45 | 7.829 | 1.0 | 0.5 | 0.5 | 0.01 | 0.0 | 5.106 | 0.4 | | 46 | 7.809 | 0.5 | 0.5 | 1.0 | 0.01 | 0.0 | 5.964 | 0.4 | | 47 | 7.689 | 1.0 | 0.5 | 1.0 | 0.01 | 0.0 | 4.909 | 0.4 | | 48 | 7.76 | 1.0 | 0.5 | 0.5 | 0.01 | 0.0 | 3.017 | 1.0 | | 49 | 7.843 | 1.0 | 0.5 | 0.5 | 0.01 | 0.0 | 4.755 | 1.0 | | 50 | 7.734 | 0.5 | 0.5 | 0.5 | 0.01 | 0.0 | 3.453 | 0.4 | | 51 | 7.522 | 1.0 | 1.0 | 1.0 | 0.01 | 0.0 | 7.0 | 1.0 | | 52 | 7.81 | 0.5 | 0.5 | 0.5 | 0.01 | 0.0 | 6.282 | 0.4 | | 53 | 7.699 | 1.0 | 0.5 | 0.5 | 0.05 | 0.0 | 4.654 | 0.4 | | 54 | 3.392 | 0.5034 | 0.7218 | 0.5388 | 0.04978 | 15.4 | 6.853 | 0.4056 | | 55 | 6.381 | 1.0 | 0.5 | 0.5 | 0.01 | 1.022 | 4.746 | 1.0 | | 56 | 7.643 | 1.0 | 1.0 | 1.0 | 0.01 | 0.0 | 6.238 | 0.4 | | 57 | 4.014 | 0.6623 | 0.9548 | 0.6727 | 0.0302 | 11.18 | 2.002 | 0.5538 | | 58 | 7.491 | 0.5 | 1.0 | 0.5 | 0.01 | 0.0 | 2.789 | 1.0 | | 59 | 7.747 | 0.9694 | 0.8278 | 0.8129 | 0.02669 | 0.0109 | 5.277 | 0.6284 | | 60 | 7.651 | 1.0 | 0.5 | 0.5 | 0.01 | 0.0 | 3.44 | 0.4 | | 61 | 7.858 | 0.5 | 0.8667 | 0.8305 | 0.01 | 0.0 | 6.538 | 0.4 | | 62 | 7.788 | 1.0 | 1.0 | 0.5 | 0.01 | 0.0 | 5.181 | 0.4 | | 63 | 7.683 | 0.5313 | 0.5206 | 0.7006 | 0.03925 | 0.000322 | 5.92 | 0.7606 | ============================================================================================================= . tuned_xgb_score = 1/xgb_bo.max[&#39;target&#39;] print(f&quot;RMSLE of tuned xgboost: {tuned_xgb_score:.5f}&quot;) . RMSLE of tuned xgboost: 0.12679 . xgb_bo.max . {&#39;target&#39;: 7.787494840784668, &#39;params&#39;: {&#39;colsample_bylevel&#39;: 0.5, &#39;colsample_bynode&#39;: 0.5, &#39;colsample_bytree&#39;: 0.5, &#39;eta&#39;: 0.01, &#39;gamma&#39;: 0.0, &#39;max_depth&#39;: 4.676653597241575, &#39;subsample&#39;: 1.0}} . xgb_tuned_params = {&quot;eta&quot;: 0.01, &quot;gamma&quot;: 0, &quot;subsample&quot;: 1.0, &quot;colsample_bytree&quot;: 0.5, &quot;colsample_bynode&quot;: 0.5, &quot;colsample_bylevel&quot;: 0.5, &quot;max_depth&quot;: 4, &quot;tree_method&quot;: &quot;hist&quot;} . Train and Make Submission . xgb_cv = xgb.cv(xgb_tuned_params, train_d, num_boost_round = 1500, nfold = 5, early_stopping_rounds = 10) num_boost_round = len(xgb_cv) - 10 xgb_cv.tail() . train-rmse-mean train-rmse-std test-rmse-mean test-rmse-std . 1495 0.063411 | 0.002336 | 0.121000 | 0.020686 | . 1496 0.063391 | 0.002330 | 0.120999 | 0.020687 | . 1497 0.063374 | 0.002329 | 0.120994 | 0.020690 | . 1498 0.063356 | 0.002330 | 0.120985 | 0.020681 | . 1499 0.063333 | 0.002331 | 0.120981 | 0.020679 | . bst = xgb.train(xgb_tuned_params, train_d, num_boost_round = num_boost_round) # make predictions xgb_preds = np.expm1(bst.predict(test_d)) . submission_df[&quot;SalePrice&quot;] = xgb_preds submission_df.to_csv(&quot;xgb_tuned.csv&quot;, index = None) . This submission gives us the best score yet of rmsle 0.12488. This is our final submission. . Summary and Conclusion . In this project, we worked on the Ames housing data provided as part of a competition on Kaggle. We tried to predict the Sale Price of houses in Ames from this dataset. Before we could train ML models, we evaluated the data and prepared it for ML algorithms. We also trained a dummy model to spot errors in training. In the ML model training part, we first trained some linear models. Then we combined these linear models to form blended predictions. This improved the overall performance. Then we trained two gradient boosting models. First we trained a lightgbm model, which improved on the performance by blended linear models. Hyperparameter tuning also helped to further increase the submission score. Then we trained an XGBoost model, for which we also tuned its hyperparameters. This gave us the best rmsle score of 0.12488. .",
            "url": "https://ncitshubham.github.io/blogs/2021/10/16/ames-housing-competition.html",
            "relUrl": "/2021/10/16/ames-housing-competition.html",
            "date": " • Oct 16, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "EDA of Stroke Dataset and Prediction of Strokes using Selected ML Algorithms",
            "content": "Introduction . According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths. . In this project, we&#39;ll try to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. To do this, we&#39;ll use the Stroke Prediction Dataset provided by fedesoriano on Kaggle. . Each row in the dataset provides relavant information about the patient like age, smoking status, gender, heart disease, bmi, work type and in the end whether the patient suffered a stroke. This last parameter will be our target, which we&#39;ll try to predict using information from the other columns. . The steps that we&#39;ll take: . Setup and import the dataset | Perform basic EDA and prepare the dataset for training | Train and evaluate a baseline model | Train multiple ML models and make predictions. | Evaluate and compare their performance. | . Setup . import numpy as np # Linear algebra import pandas as pd # Data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # Data Visualizatin import seaborn as sns # Data Visualization from imblearn.over_sampling import SMOTE # Oversampling imbalanced classes from sklearn.impute import SimpleImputer, MissingIndicator # Handle missing values from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate, GridSearchCV, StratifiedKFold # Model validation from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, RocCurveDisplay, PrecisionRecallDisplay, confusion_matrix, ConfusionMatrixDisplay # Metrics from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, FunctionTransformer # Data preparation from imblearn.pipeline import make_pipeline # Build pipeline from sklearn.compose import make_column_transformer # Combine data preparation steps # Models for prediciton from sklearn.dummy import DummyClassifier from sklearn.linear_model import LogisticRegression, RidgeClassifier from sklearn.svm import SVC from sklearn.naive_bayes import GaussianNB from sklearn.neighbors import KNeighborsClassifier from sklearn.ensemble import RandomForestClassifier from xgboost import XGBClassifier # ignore warnings import warnings warnings.filterwarnings(&quot;ignore&quot;) # get the file path import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . /kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv . file_path = &quot;/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv&quot; . # file size of the dataset !ls -lh {file_path} . -rw-r--r-- 1 nobody nogroup 310K Jan 29 2021 /kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv . # look at the first few rows of the dataseet !head {file_path} . id,gender,age,hypertension,heart_disease,ever_married,work_type,Residence_type,avg_glucose_level,bmi,smoking_status,stroke 9046,Male,67,0,1,Yes,Private,Urban,228.69,36.6,formerly smoked,1 51676,Female,61,0,0,Yes,Self-employed,Rural,202.21,N/A,never smoked,1 31112,Male,80,0,1,Yes,Private,Rural,105.92,32.5,never smoked,1 60182,Female,49,0,0,Yes,Private,Urban,171.23,34.4,smokes,1 1665,Female,79,1,0,Yes,Self-employed,Rural,174.12,24,never smoked,1 56669,Male,81,0,0,Yes,Private,Urban,186.21,29,formerly smoked,1 53882,Male,74,1,1,Yes,Private,Rural,70.09,27.4,never smoked,1 10434,Female,69,0,0,No,Private,Urban,94.39,22.8,never smoked,1 27419,Female,59,0,0,Yes,Private,Rural,76.15,N/A,Unknown,1 . **Observations** . File size is 310 KB, thus it is safe to import the whole dataset. | The delimiters are , | id is the index column | stroke is the prediction class in the last column | . # Load the dataset into pandas DataFrame df = pd.read_csv(file_path, index_col = [&quot;id&quot;]) df.head() . gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status stroke . id . 9046 Male | 67.0 | 0 | 1 | Yes | Private | Urban | 228.69 | 36.6 | formerly smoked | 1 | . 51676 Female | 61.0 | 0 | 0 | Yes | Self-employed | Rural | 202.21 | NaN | never smoked | 1 | . 31112 Male | 80.0 | 0 | 1 | Yes | Private | Rural | 105.92 | 32.5 | never smoked | 1 | . 60182 Female | 49.0 | 0 | 0 | Yes | Private | Urban | 171.23 | 34.4 | smokes | 1 | . 1665 Female | 79.0 | 1 | 0 | Yes | Self-employed | Rural | 174.12 | 24.0 | never smoked | 1 | . Basic EDA and Data Preparation . First, it is really important to separate the test data from the train data at this point, so that the transformers and models cannot learn from the test data itself. Before making a split, it is worth looking at the distribution of prediction class, to see if there is an imbalance and whether we will need to stratify the split. . # Distribution of prediction class stroke_val_count = df.stroke.value_counts() print(f&quot;Value Count in the prediction class - Stroke: n{stroke_val_count} n n&quot;) sns.barplot(x = stroke_val_count.index, y = stroke_val_count) plt.show() . Value Count in the prediction class - Stroke: 0 4861 1 249 Name: stroke, dtype: int64 . As we can see, the prediction class is highly imbalanced. Therefore, we&#39;ll need to stratify the split. Also, after making the split, it would be worth to generate artificial data in the training dataset to help the ML models distinguish better between the two categories of prediction class. . # Separate the prediction class from the training features X = df.copy() y = X.pop(&quot;stroke&quot;) # split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 42) . We&#39;ll perform EDA on the dataset and use the observations that we make for building a data preparation pipeline as the last step. . Basic Inspection . # Look at the dataset X_train.head() . gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status . id . 25283 Female | 48.0 | 0 | 0 | Yes | Private | Urban | 69.21 | 33.1 | never smoked | . 43734 Male | 15.0 | 0 | 0 | No | Private | Rural | 122.25 | 21.0 | never smoked | . 47113 Female | 67.0 | 0 | 0 | Yes | Self-employed | Rural | 110.42 | 24.9 | never smoked | . 56996 Male | 44.0 | 0 | 0 | Yes | Private | Urban | 65.41 | 24.8 | smokes | . 26325 Male | 14.0 | 0 | 0 | No | Govt_job | Urban | 82.34 | 31.6 | Unknown | . X_train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 4088 entries, 25283 to 31836 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 gender 4088 non-null object 1 age 4088 non-null float64 2 hypertension 4088 non-null int64 3 heart_disease 4088 non-null int64 4 ever_married 4088 non-null object 5 work_type 4088 non-null object 6 Residence_type 4088 non-null object 7 avg_glucose_level 4088 non-null float64 8 bmi 3918 non-null float64 9 smoking_status 4088 non-null object dtypes: float64(3), int64(2), object(5) memory usage: 351.3+ KB . **Observations** . There are 4088 entries in the train dataset. | There are total 10 features which we can use to predict the occurance of stroke. | There are some categorical features like gender, work_type, Residence_type of dtype object, which we&#39;ll need to be One Hot Encoded. | Numerical features will need to be scaled. | There are some missing values in the bmi column. | . Null values . # Null values X_train.isnull().sum() . gender 0 age 0 hypertension 0 heart_disease 0 ever_married 0 work_type 0 Residence_type 0 avg_glucose_level 0 bmi 170 smoking_status 0 dtype: int64 . # Distribution of null values in the dataset plt.figure(figsize = (10, 8)) sns.heatmap(X_train.isnull(), cmap = &quot;rocket&quot;, yticklabels = &quot;&quot;, cbar = None) . &lt;AxesSubplot:ylabel=&#39;id&#39;&gt; . null_bmi = y_train.loc[X_train.bmi.isna()] not_null_bmi = y_train.loc[X_train.bmi.notnull()] print(f&quot;&quot;&quot;Non null bmi values:- Stroke-no stroke ratio: {null_bmi.sum()/len(null_bmi)} Null bmi values:- Stroke-no stroke ratio: {not_null_bmi.sum()/len(not_null_bmi)} &quot;&quot;&quot;) . Non null bmi values:- Stroke-no stroke ratio: 0.21764705882352942 Null bmi values:- Stroke-no stroke ratio: 0.04134762633996937 . **Observations** . Although Null values look to be evenly distributed in the heatmap, the ratio for occurance of stroke is significatly different in the entries with null bmi values. Thus, instead of dropping the null values, it would be better to impute the null values with median bmi value, and also encode the presence of null values in a separate column. This may help in better prediction of stroke. . Categorical columns . First it is worth inspecting the categories and their distributions in all categorical columns. . # Distribution of Categorical Features cat_cols = [&#39;gender&#39;, &#39;hypertension&#39;, &#39;heart_disease&#39;, &#39;ever_married&#39;, &#39;work_type&#39;, &#39;Residence_type&#39;, &#39;smoking_status&#39;] print(&quot;Value Counts of all categorical columns... n&quot;) for i, col in enumerate(cat_cols): val_count = X_train[col].value_counts() print(f&quot;Values:- n{val_count} n n&quot;) plt.figure(i) sns.barplot(x = val_count.index, y = val_count) . Value Counts of all categorical columns... Values:- Female 2395 Male 1692 Other 1 Name: gender, dtype: int64 Values:- 0 3691 1 397 Name: hypertension, dtype: int64 Values:- 0 3867 1 221 Name: heart_disease, dtype: int64 Values:- Yes 2700 No 1388 Name: ever_married, dtype: int64 Values:- Private 2332 Self-employed 667 children 554 Govt_job 522 Never_worked 13 Name: work_type, dtype: int64 Values:- Urban 2069 Rural 2019 Name: Residence_type, dtype: int64 Values:- never smoked 1501 Unknown 1247 formerly smoked 714 smokes 626 Name: smoking_status, dtype: int64 . **Observations** . The categories in all the categorical features look ok, albeit most of the categories are unevenly distributed. Thus, we&#39;ll just one hot encode these columns. . Numerical Columns . # Look at the basic statistics num_cols = [&quot;age&quot;, &quot;avg_glucose_level&quot;, &quot;bmi&quot;] X_train[num_cols].describe() . age avg_glucose_level bmi . count 4088.000000 | 4088.000000 | 3918.000000 | . mean 43.353288 | 106.317167 | 28.922180 | . std 22.596816 | 45.259652 | 7.928378 | . min 0.080000 | 55.120000 | 10.300000 | . 25% 26.000000 | 77.312500 | 23.600000 | . 50% 45.000000 | 91.945000 | 28.000000 | . 75% 61.000000 | 114.197500 | 33.100000 | . max 82.000000 | 271.740000 | 97.600000 | . **Observations** . Age appears to be slightly negatively skewed | avg_glucose_level appears to be positively skewed | min age of 0.08 suggests that age is stored in fractions, which needs further inspection. | . # Distribution of Numerical (Continuous) Features for i, col in enumerate(num_cols): plt.figure(i) sns.violinplot(x = X_train[col]) plt.legend([f&quot;skewness: {X_train[col].skew():.2f} nkurtosis: {X_train[col].kurtosis():.2f}&quot;]) . **Observations** . age is very slightly negatively skewed with negative kurtosis. | avg_glucose_level and bmi are positively skewed with sharp peaks(positive kurtosis) | . Although some ML models assume normal distribution of data, they can work fine with data with small skewness and kurtosis values. Therefore, we&#39;ll just scale this data using MinMaxScaler. . # inspect`age` column print(sorted(X_train.age.unique())) . [0.08, 0.24, 0.32, 0.4, 0.48, 0.56, 0.64, 0.72, 0.8, 0.88, 1.0, 1.08, 1.16, 1.24, 1.32, 1.4, 1.48, 1.56, 1.64, 1.72, 1.8, 1.88, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0] . **Observations** . Only values smaller than 2 are stored as floats. Thus, we can change the whole column to int datatype to make this feature uniform. . Build Column Transformer . Now, we can use combine all the observations that we made to build a data preparation pipeline with a column transformer. We&#39;ll use the transformer to: . impute missing values | change age column dtype to int | add missing value indicator | onehotencode categorical columns | scale numerical features | . # Columns age_col = [&quot;age&quot;] num_cols_without_age = [&quot;avg_glucose_level&quot;, &quot;bmi&quot;] cat_cols = [&quot;gender&quot;, &quot;ever_married&quot;, &quot;work_type&quot;, &quot;Residence_type&quot;, &quot;smoking_status&quot;] # Scale the data scaler = MinMaxScaler() # handle missing values in `bmi` imputer = SimpleImputer(strategy = &quot;median&quot;) # change dtype of `age` def to_int(x): return pd.DataFrame(x).astype(int) int_tr = FunctionTransformer(to_int) . # build transformer for `age` separately age_transformer = make_pipeline(int_tr, scaler) # build transformer for numeric cols without age num_transformer = make_pipeline(imputer, scaler) # missing_indicator miss_ind = MissingIndicator() # build transformer for categorical cols cat_transformer = OneHotEncoder(drop = &quot;first&quot;, handle_unknown = &quot;ignore&quot;, sparse = False) # combine transformers to make a single column transformer ct = make_column_transformer((age_transformer, age_col), (num_transformer, num_cols_without_age), (miss_ind, num_cols_without_age), (cat_transformer, cat_cols), remainder = &quot;passthrough&quot;) . ct.fit_transform(X_train).shape . (4088, 17) . SMOTE . We&#39;ll use imblearn&#39;s SMOTE over-sampling to balance the data. We&#39;ll implement this within the final pipeline with the training model. . sm = SMOTE(random_state = 42) sm . SMOTE(random_state=42) . Train and Evaluate a Baseline Model . A baseline model which doesn&#39;t use any of the features to make predictions will give a baseline score, that the future ML models should at least beat. This score will help to identify errors in model training and evaluation if the models perform worse than the baseline score. . # Build imblearn pipeline with the DummyClassifier base_model = DummyClassifier(strategy = &#39;prior&#39;) base_pipe = make_pipeline(ct, sm, base_model) # specify KFold strategy cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42) . Baseline scores . # Scores on multiple metrics from the DummyClassifier cross_validate(base_pipe, X_train, y_train, cv = cv, n_jobs = -1, scoring = [&quot;accuracy&quot;, &quot;recall&quot;, &quot;roc_auc&quot;, &quot;f1&quot;]) . {&#39;fit_time&#39;: array([0.05653143, 0.05199432, 0.05150819, 0.05320215, 0.04633212]), &#39;score_time&#39;: array([0.04810262, 0.04459357, 0.04388809, 0.04212928, 0.03426027]), &#39;test_accuracy&#39;: array([0.95110024, 0.95110024, 0.95110024, 0.95226438, 0.95104039]), &#39;test_recall&#39;: array([0., 0., 0., 0., 0.]), &#39;test_roc_auc&#39;: array([0.5, 0.5, 0.5, 0.5, 0.5]), &#39;test_f1&#39;: array([0., 0., 0., 0., 0.])} . The model accuracy comes out to be really great at about 0.95. Although, it would generally be a good score to achieve. in this domain, a good accuracy score can be decieving. We can actually confirm this by looking at the test recall scores. All the test recall scores are 0, which means that the model failed to catch even a single True Positive. This is what we almost never want in the medical sphere. If the testing isn&#39;t prohibitively expensive or risky for the patient, which in this case it isn&#39;t, the test should aim for a high recall score and not a high accuracy score. That is why we&#39;ll judge our models using the roc_auc score primarily, which shows the relation between the True Positive Rate(TPR/recall) and False Positive Rate(FPR) of the model. For this baseline model, the roc_auc score comes out to be 0.5, which means the model isn&#39;t able to distinguish between the prediction classes at any of the thresholds, which we would expect from a dummy model. We can also look at the roc_auc_curve of the model. . # ROC curve of DummyClassifier base_preds_prob = cross_val_predict(base_pipe, X_train, y_train, cv = cv, n_jobs = -1, method = &quot;predict_proba&quot;)[:, 1] RocCurveDisplay.from_predictions(y_train, base_preds_prob) . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7fae7fadca90&gt; . Model Selection . In this section, we&#39;ll train multiple ML models typically used for binary classification and compare their performance using the scores from cross_validate . # Specify ML models models = {&quot;Logistic_Regression&quot;: LogisticRegression(random_state = 42), &quot;Ridge_Classification&quot;: RidgeClassifier(random_state = 42), &quot;SVC&quot;: SVC(random_state = 42), &quot;GaussianNB&quot;: GaussianNB(), &quot;KNClassifier&quot;: KNeighborsClassifier(n_neighbors = 5), &quot;RandomForestClassifier&quot;: RandomForestClassifier(max_depth = 10, n_jobs = -1, random_state = 42), &quot;XGBClassifier&quot;: XGBClassifier(n_estimators = 50, learning_rate = 0.03, n_jobs = -1, objective = &quot;binary:logistic&quot;, eval_metric = &quot;auc&quot;, tree_method = &quot;hist&quot;, random_state = 42)} . # Model comparison on multiple metrics scores = {} for model_name, model in models.items(): model_pipe = make_pipeline(ct, sm, model) cross_val = cross_validate(model_pipe, X_train, y_train, cv = cv, scoring = [&quot;accuracy&quot;, &quot;recall&quot;, &quot;precision&quot;, &quot;roc_auc&quot;, &quot;f1&quot;]) del cross_val[&quot;fit_time&quot;] del cross_val[&quot;score_time&quot;] print(model_name + &quot; : n&quot;) for score_name, score_vals in cross_val.items(): score_mean = score_vals.mean() score_std = score_vals.std() cross_val[score_name] = score_mean print(f&quot;{score_name}: Mean: {(score_mean * 100):.2f} % Std: {(score_std * 100):.2f} n&quot;) print(&quot; n&quot;, &quot;-&quot; * 50, &quot; n&quot;) scores[model_name] = cross_val . Logistic_Regression : test_accuracy: Mean: 75.42 % Std: 1.51 test_recall: Mean: 78.91 % Std: 2.90 test_precision: Mean: 14.06 % Std: 0.93 test_roc_auc: Mean: 84.92 % Std: 0.96 test_f1: Mean: 23.86 % Std: 1.40 -- Ridge_Classification : test_accuracy: Mean: 73.95 % Std: 1.57 test_recall: Mean: 79.41 % Std: 3.60 test_precision: Mean: 13.41 % Std: 0.94 test_roc_auc: Mean: 85.04 % Std: 1.38 test_f1: Mean: 22.93 % Std: 1.47 -- SVC : test_accuracy: Mean: 81.97 % Std: 1.27 test_recall: Mean: 45.26 % Std: 10.97 test_precision: Mean: 12.36 % Std: 1.46 test_roc_auc: Mean: 76.44 % Std: 3.34 test_f1: Mean: 19.36 % Std: 2.79 -- GaussianNB : test_accuracy: Mean: 25.37 % Std: 4.74 test_recall: Mean: 99.50 % Std: 1.00 test_precision: Mean: 6.12 % Std: 0.43 test_roc_auc: Mean: 82.68 % Std: 1.54 test_f1: Mean: 11.53 % Std: 0.76 -- KNClassifier : test_accuracy: Mean: 83.76 % Std: 1.21 test_recall: Mean: 36.19 % Std: 3.84 test_precision: Mean: 11.84 % Std: 0.47 test_roc_auc: Mean: 65.28 % Std: 0.76 test_f1: Mean: 17.80 % Std: 0.71 -- RandomForestClassifier : test_accuracy: Mean: 83.41 % Std: 1.46 test_recall: Mean: 40.69 % Std: 5.24 test_precision: Mean: 12.65 % Std: 0.85 test_roc_auc: Mean: 80.27 % Std: 0.91 test_f1: Mean: 19.25 % Std: 1.29 -- XGBClassifier : test_accuracy: Mean: 77.28 % Std: 1.95 test_recall: Mean: 63.27 % Std: 6.57 test_precision: Mean: 12.89 % Std: 1.56 test_roc_auc: Mean: 80.95 % Std: 2.15 test_f1: Mean: 21.39 % Std: 2.45 -- . scores_df = pd.DataFrame.from_dict(scores, orient = &#39;index&#39;) scores_df.sort_values(by = &quot;test_roc_auc&quot;, ascending = False) . test_accuracy test_recall test_precision test_roc_auc test_f1 . Ridge_Classification 0.739481 | 0.794103 | 0.134058 | 0.850447 | 0.229299 | . Logistic_Regression 0.754158 | 0.789103 | 0.140608 | 0.849226 | 0.238578 | . GaussianNB 0.253661 | 0.995000 | 0.061211 | 0.826782 | 0.115301 | . XGBClassifier 0.772752 | 0.632692 | 0.128876 | 0.809516 | 0.213944 | . RandomForestClassifier 0.834148 | 0.406923 | 0.126488 | 0.802683 | 0.192495 | . SVC 0.819717 | 0.452564 | 0.123650 | 0.764393 | 0.193636 | . KNClassifier 0.837574 | 0.361923 | 0.118366 | 0.652839 | 0.177978 | . From the scores we get, linear models although lacking in accuracy porformed way much better than the others in recall, auc_roc, and f1 metrics. Their roc_auc score of above 80s but low precision score suggests that they can do well with threshold tuning. The tree models like RandomForestClassifier and XGBClassifier also achieved satisfactory performance with high accuracy and moderate roc_auc scores. KNClassifier and SVC performed the worst and may require some hyperparameter tuning. But time would be better spent tuning the linear models to get even better performance than to tune the other ML models. . Model Tuning . We&#39;ll train LogisticRegression and RidgeClassifier ML models with GridSearchCV to find the best model and its parameters. We can then train this model on the full train dataset and then make predictions on the test dataset in the next section. . # Initialize models and specify param_grid for GridSearchCV models_and_params = {&quot;LogisticRegression&quot;: [LogisticRegression(random_state = 42), {&quot;logisticregression__class_weight&quot;: [{0:1, 1: 1}, {0:1, 1:3}]}], &quot;RidgeClassification&quot;: [RidgeClassifier(random_state = 42), {&quot;ridgeclassifier__alpha&quot;: [1, 2, 3], &quot;ridgeclassifier__class_weight&quot;: [{0:1, 1: 1}, {0:1, 1:3}]}]} . # Run GridSearchCV and store its results tuning_scores = [] for model_name in models_and_params: model, params = models_and_params[model_name] model_pipe = make_pipeline(ct, sm, model) grid_cv = GridSearchCV(model_pipe, params, scoring = [&quot;accuracy&quot;, &quot;recall&quot;, &quot;precision&quot;, &quot;roc_auc&quot;, &quot;f1&quot;], n_jobs = -1, cv = cv, refit = False) grid_cv.fit(X_train, y_train) tuning_scores.append(grid_cv) . # Results for LogisticRegression logistic_regression_grid_result = pd.DataFrame.from_dict(tuning_scores[0].cv_results_) logistic_regression_grid_result = logistic_regression_grid_result[[ &quot;param_logisticregression__class_weight&quot;, &quot;mean_test_accuracy&quot;, &quot;mean_test_precision&quot;, &quot;mean_test_recall&quot;, &quot;mean_test_roc_auc&quot;, &quot;mean_test_f1&quot;]] logistic_regression_grid_result.sort_values(by = &quot;mean_test_roc_auc&quot;, ascending = False) . param_logisticregression__class_weight mean_test_accuracy mean_test_precision mean_test_recall mean_test_roc_auc mean_test_f1 . 0 {0: 1, 1: 1} | 0.754158 | 0.140608 | 0.789103 | 0.849226 | 0.238578 | . 1 {0: 1, 1: 3} | 0.610319 | 0.104987 | 0.929744 | 0.847900 | 0.188645 | . # Results for RidgeClassifier ridge_classifier_grid_result = pd.DataFrame.from_dict(tuning_scores[1].cv_results_) ridge_classifier_grid_result = ridge_classifier_grid_result[[ &quot;param_ridgeclassifier__alpha&quot;, &quot;param_ridgeclassifier__class_weight&quot;, &quot;mean_test_accuracy&quot;, &quot;mean_test_precision&quot;, &quot;mean_test_recall&quot;, &quot;mean_test_roc_auc&quot;, &quot;mean_test_f1&quot;]] ridge_classifier_grid_result.sort_values(by = &quot;mean_test_roc_auc&quot;, ascending = False) . param_ridgeclassifier__alpha param_ridgeclassifier__class_weight mean_test_accuracy mean_test_precision mean_test_recall mean_test_roc_auc mean_test_f1 . 4 3 | {0: 1, 1: 1} | 0.738258 | 0.133504 | 0.794103 | 0.850666 | 0.228482 | . 2 2 | {0: 1, 1: 1} | 0.739237 | 0.133963 | 0.794103 | 0.850492 | 0.229157 | . 0 1 | {0: 1, 1: 1} | 0.739481 | 0.134058 | 0.794103 | 0.850447 | 0.229299 | . 5 3 | {0: 1, 1: 3} | 0.582188 | 0.101112 | 0.959872 | 0.844802 | 0.182931 | . 3 2 | {0: 1, 1: 3} | 0.582432 | 0.101167 | 0.959872 | 0.844769 | 0.183022 | . 1 1 | {0: 1, 1: 3} | 0.583411 | 0.100952 | 0.954872 | 0.844712 | 0.182581 | . From these grid search, we have found that both the logistic regression and the ridge regression models are really close in performance on their roc_auc score which is the primary metric, and that the increasing alpha values in ridge classifier does help in improving the scores. Changing class weights improves test recall a lot but because of tradeoff between test recall and test precision, test precision takes a sligh hit, along with a large hit on test accuracy. Now, we have to make a call on what parameter is more important for us between test precision and test recall. with the roc_auc, the primary metric still high, we choose to prefer better test precision and test accuracy. . From this, we have identified the ML model - RidgeClassifier with the default class weghts 1:1 and alpha value of 3. Now, we will train this final model and make predictions on the test dataset and then evaluate those predictions. . Final Model and Predictions . We can now train a RidgeClassifier with the default parameters with a imblearn Pipeline and then make predictions on the test dataset. . # Model Training ridge = RidgeClassifier(random_state = 42, alpha = 3) ridge_pipe = make_pipeline(ct, sm, ridge) ridge_pipe.fit(X_train, y_train) . Pipeline(steps=[(&#39;columntransformer&#39;, ColumnTransformer(remainder=&#39;passthrough&#39;, transformers=[(&#39;pipeline-1&#39;, Pipeline(steps=[(&#39;functiontransformer&#39;, FunctionTransformer(func=&lt;function to_int at 0x7fae7f9bcf80&gt;)), (&#39;minmaxscaler&#39;, MinMaxScaler())]), [&#39;age&#39;]), (&#39;pipeline-2&#39;, Pipeline(steps=[(&#39;simpleimputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;minmaxscaler&#39;, MinM... [&#39;avg_glucose_level&#39;, &#39;bmi&#39;]), (&#39;missingindicator&#39;, MissingIndicator(), [&#39;avg_glucose_level&#39;, &#39;bmi&#39;]), (&#39;onehotencoder&#39;, OneHotEncoder(drop=&#39;first&#39;, handle_unknown=&#39;ignore&#39;, sparse=False), [&#39;gender&#39;, &#39;ever_married&#39;, &#39;work_type&#39;, &#39;Residence_type&#39;, &#39;smoking_status&#39;])])), (&#39;smote&#39;, SMOTE(random_state=42)), (&#39;ridgeclassifier&#39;, RidgeClassifier(alpha=3, random_state=42))]) . # Predictions on test dataset preds = ridge_pipe.predict(X_test) ConfusionMatrixDisplay.from_predictions(y_test, preds) . &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fae7f031b90&gt; . # Metrics for test predictions RocCurveDisplay.from_estimator(ridge_pipe, X_test, y_test, name = &quot;RidgeClassifier&quot;) print(&quot;Accuracy Score:&quot;, accuracy_score(y_test, preds)) print(&quot;Precision Score:&quot;, precision_score(y_test, preds)) print(&quot;Recall Score:&quot;, recall_score(y_test, preds)) . Accuracy Score: 0.7465753424657534 Precision Score: 0.1384083044982699 Recall Score: 0.8 . The classifier has predicted 723 True Negatives and 40 True Positives. There are 259 misclassifications too out of which 249 are False Positives. Considering that the predictions class was highly imbalanced to begin with, the model has worked really well in making correct classifications. Its AUC_ROC score on the test dataset is 0.85, similar to what we found on the train dataset. With more data, we can improve this score further. . Summary and Conclusion . In this project, we used the Stroke Dataset available on Kaggle to predict whether a patient would suffer from a stroke. First, we prepared the data for training and test by splitting it using train_test_split. Then we explored the data and understood where it needed some cleaning and preparation. With this knowledge, we developed imblearn&#39;s Pipelines to clean the data. We then explored multiple ML models and studied their performance through multiple metrics, primarily focusing on roc_auc scores. This choice of metrics was made with the knowledge that in the medicinal domain, the correct knowledge of True Positives is much more valuable than the wrong knowledge on False Positives. We chose 2 linear models from this step for further model tuning and selection. RidgeClassifier turned out to be performing the best with its default parameters. We then trained this model and made predictions on the test data that we got from the split earlier. On the predictions, we achieved respectable scores on recall - 0.8 and on precision - 0.14. .",
            "url": "https://ncitshubham.github.io/blogs/2021/09/21/stroke-prediction-auc-roc-85-accuracy-75.html",
            "relUrl": "/2021/09/21/stroke-prediction-auc-roc-85-accuracy-75.html",
            "date": " • Sep 21, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Scraping Top Repositories for Topics on GitHub",
            "content": "GitHub is an increasingly popular programming resource used for code sharing. It&#39;s a social networking site for programmers that many companies and organizations use to facilitate project management and collaboration. According to statistics collected in August 2021, it was the most prominent source code host, with over 60 million new repositories created in 2020 and boasting over 67 million total developers. . All the projects on Github are stored as repositories. These repositories can get upvotes which are stored as stars. The stars that a repository gets can give us a guage of how popular the repository is. We can further filter all the repositores on GitHub by the topic they ascribe to. The list of topics is available here. . Thus, we&#39;ll scrape GitHub for the top repocistories on each topic and then save that to a csv file for future use. In order to do this, we&#39;ll use the following tools: . Python as the programming language | Requests library for downloading the webpage contents | BeautifulSoup library for finding and accessing the relevant information from the downloaded webpage. | Pandas library for saving the accessed information to a csv file. | . Introduction about GitHub and the problem statement | . Here are the steps we&#39;ll follow: . We&#39;re going to scrape https://github.com/topics | We&#39;ll get a list of topics. For each topic, we&#39;ll get topic title, topic page URL and topic description | For each topic, we&#39;ll get the top 30 repositories in the topic from the topic page | For each repository, we&#39;ll grab the repo name, username, stars and repo URL | For each topic we&#39;ll create a CSV file in the following format: | . Repo Name,Username,Stars,Repo URL three.js,mrdoob,69700,https://github.com/mrdoob/three.js libgdx,libgdx,18300,https://github.com/libgdx/libgdx . Setup . Import the required libraries . import os import requests from bs4 import BeautifulSoup import pandas as pd . Set up URLs and the user-agent. . topics_url = &quot;https://github.com/topics&quot; base_url = &#39;https://github.com&#39; header = {&quot;User-Agent&quot;: &quot;Mozilla/5.0&quot;} . Create variables to store scraped information. . topic_titles = [] topic_desc = [] topic_URL = [] . Scrape the list of topics. . Download the topics webpage and create a BeautifulSoup object . Let&#39;s write a function to download the page. . def get_soup(url): &#39;&#39;&#39; This function will download the webpage for the url supplied as argument and return the BeautifulSoup object for the webpage which can be used to grab required information for the webpage. &#39;&#39;&#39; response = requests.get(url, &quot;html.parser&quot;, headers = header) if response.status_code != 200: raise Exception(&#39;Failed to load page {}&#39;.format(url)) soup = BeautifulSoup(response.text) return soup . # Example soup = get_soup(topics_url) type(soup) . bs4.BeautifulSoup . Create a transform function . Let&#39;s create some helper functions to parse information from the page. . Get topic titles . To get topic titles, we can pick p tags with the class &quot;f3 lh-condensed mb-0 mt-1 Link--primary&quot; . ![My Image alt text](https://i.imgur.com/OnzIdyP.png) # finding all topic titles def get_topic_titles(soup): selection_class = &#39;f3 lh-condensed mb-0 mt-1 Link--primary&#39; topic_title_tags = soup.find_all(&#39;p&#39;, {&#39;class&#39;: selection_class}) topic_titles = [] for tag in topic_title_tags: topic_titles.append(tag.text) return topic_titles . # Example titles = get_topic_titles(soup) len(titles) . 30 . titles[:5] . [&#39;3D&#39;, &#39;Ajax&#39;, &#39;Algorithm&#39;, &#39;Amp&#39;, &#39;Android&#39;] . This is the list of topics on page number 1. We will today scrape information for topics only on this page. In the future, we can scrape information from other pages as well by changing the page number in the url. Now we&#39;ll find the topic descriptions similarly. . Get topic descriptions . # finding all topics descriptions def get_topic_descs(soup): desc_selector = &#39;f5 color-fg-muted mb-0 mt-1&#39; topic_desc_tags = soup.find_all(&#39;p&#39;, {&#39;class&#39;: desc_selector}) topic_descs = [] for tag in topic_desc_tags: topic_descs.append(tag.text.strip()) return topic_descs . # Example topics_descs = get_topic_descs(soup) len(topics_descs) . 30 . topics_descs[:5] . [&#39;3D modeling is the process of virtually developing the surface and structure of a 3D object.&#39;, &#39;Ajax is a technique for creating interactive web applications.&#39;, &#39;Algorithms are self-contained sequences that carry out a variety of tasks.&#39;, &#39;Amp is a non-blocking concurrency library for PHP.&#39;, &#39;Android is an operating system built by Google designed for mobile devices.&#39;] . Similary, we&#39;ll find the topic urls. . Get topic URLs . def get_topic_urls(soup): topic_link_tags = soup.find_all(&#39;a&#39;, {&#39;class&#39;: &#39;no-underline flex-1 d-flex flex-column&#39;}) topic_urls = [] for tag in topic_link_tags: topic_urls.append(base_url + tag[&#39;href&#39;]) return topic_urls . # Example topic_urls = get_topic_urls(soup) len(topic_urls) . 30 . topic_urls[:5] . [&#39;https://github.com/topics/3d&#39;, &#39;https://github.com/topics/ajax&#39;, &#39;https://github.com/topics/algorithm&#39;, &#39;https://github.com/topics/amphp&#39;, &#39;https://github.com/topics/android&#39;] . Save all information . We&#39;ll put together all this information into a single function and then save the scraped information into a pandas DataFrame. . def scrape_topics(): topics_url = &#39;https://github.com/topics&#39; soup = get_soup(topics_url) topics_dict = { &#39;Title&#39;: get_topic_titles(soup), &#39;Description&#39;: get_topic_descs(soup), &#39;URL&#39;: get_topic_urls(soup) } return pd.DataFrame(topics_dict) . topics_df = scrape_topics() topics_df.head() . Title Description URL . 0 3D | 3D modeling is the process of virtually develo... | https://github.com/topics/3d | . 1 Ajax | Ajax is a technique for creating interactive w... | https://github.com/topics/ajax | . 2 Algorithm | Algorithms are self-contained sequences that c... | https://github.com/topics/algorithm | . 3 Amp | Amp is a non-blocking concurrency library for ... | https://github.com/topics/amphp | . 4 Android | Android is an operating system built by Google... | https://github.com/topics/android | . Scraping for top 30 repos for each topic . Now that we have the topics with their titles, descriptions and url, we can access each topic url to grab information about the top 30 repositories from that topic individually and then save the scraped information for each topic as a separate csv file. . Each topic page looks like this . . From this page, we&#39;ll grab information about the top 30 repositories based on their popularity as measured by the number of stars. The repositories are already sorted by popularity by default, so we can grab 30 of them from the first page on each topic itself. We&#39;ll begin by writing a function to download each topic page and create its BeautifulSoup object. . Download each topic page and create a BeautifulSoup Object . def get_topic_page(topic_url): # Download the page response = requests.get(topic_url, &quot;html.parser&quot;, headers = header) # Check successful response if response.status_code != 200: raise Exception(&#39;Failed to load page {}&#39;.format(topic_url)) # Parse using Beautiful soup topic_soup = BeautifulSoup(response.text) return topic_soup . page = get_topic_page(&#39;https://github.com/topics/3d&#39;) . Transform the topic Beautiful Object . Get all the required information about a repository . All the information that we need about a repository is given under a div tag with class d-flex flex-justify-between my-3. So we will make a function which takes in the content of each repository from these tags as arguement. It will then grab and return the required information from the content. . def get_repo_info(repo): # returns all the required info about a repository info = repo.find(&#39;h3&#39;, {&#39;class&#39;: &#39;f3 color-fg-muted text-normal lh-condensed&#39;}).find_all(&#39;a&#39;) username = info[0].text.strip() repo_name = info[1].text.strip() repo_url = base_url + info[0][&#39;href&#39;].strip() stars = repo.find(&#39;span&#39;, {&#39;id&#39;: &#39;repo-stars-counter-star&#39;}).text.strip() return username, repo_name, stars, repo_url . # Example repo_contents = page.find_all(&#39;div&#39;, {&#39;class&#39;: &#39;d-flex flex-justify-between my-3&#39;}) get_repo_info(repo_contents[0]) . (&#39;mrdoob&#39;, &#39;three.js&#39;, &#39;80.6k&#39;, &#39;https://github.com/mrdoob&#39;) . Here we can see that the function returns the information about the first repository from the topic page. The top repository in this case is &#39;three.js&#39; with 80.6k stars. . Grab the information from top 30 repos under a topic. . Now, we&#39;ll write a function to grab information about repositories within a topic. It will take in a topic soup and return a pandas DataFrame on the top 30 repos in that topic. . def get_topic_repos(topic_soup): div_selection_class = &#39;d-flex flex-justify-between my-3&#39; repo_tags = topic_soup.find_all(&#39;div&#39;, {&#39;class&#39;: div_selection_class}) topic_repos_dict = { &#39;username&#39;: [], &#39;repo_name&#39;: [], &#39;stars&#39;: [],&#39;repo_url&#39;: []} # Get repo info for i in range(len(repo_tags)): username, repo_name, stars, repo_url = get_repo_info(repo_tags[i]) topic_repos_dict[&#39;username&#39;].append(username) topic_repos_dict[&#39;repo_name&#39;].append(repo_name) topic_repos_dict[&#39;stars&#39;].append(stars) topic_repos_dict[&#39;repo_url&#39;].append(repo_url) return pd.DataFrame(topic_repos_dict) . # Example get_topic_repos(page) . username repo_name stars repo_url . 0 mrdoob | three.js | 80.6k | https://github.com/mrdoob | . 1 libgdx | libgdx | 19.8k | https://github.com/libgdx | . 2 pmndrs | react-three-fiber | 17.4k | https://github.com/pmndrs | . 3 BabylonJS | Babylon.js | 16.2k | https://github.com/BabylonJS | . 4 aframevr | aframe | 14k | https://github.com/aframevr | . 5 ssloy | tinyrenderer | 13.3k | https://github.com/ssloy | . 6 lettier | 3d-game-shaders-for-beginners | 12.5k | https://github.com/lettier | . 7 FreeCAD | FreeCAD | 11k | https://github.com/FreeCAD | . 8 metafizzy | zdog | 9.1k | https://github.com/metafizzy | . 9 CesiumGS | cesium | 8.5k | https://github.com/CesiumGS | . 10 timzhang642 | 3D-Machine-Learning | 7.8k | https://github.com/timzhang642 | . 11 a1studmuffin | SpaceshipGenerator | 7.1k | https://github.com/a1studmuffin | . 12 isl-org | Open3D | 6.4k | https://github.com/isl-org | . 13 blender | blender | 5.2k | https://github.com/blender | . 14 domlysz | BlenderGIS | 5k | https://github.com/domlysz | . 15 spritejs | spritejs | 4.8k | https://github.com/spritejs | . 16 openscad | openscad | 4.7k | https://github.com/openscad | . 17 tensorspace-team | tensorspace | 4.6k | https://github.com/tensorspace-team | . 18 jagenjo | webglstudio.js | 4.6k | https://github.com/jagenjo | . 19 YadiraF | PRNet | 4.6k | https://github.com/YadiraF | . 20 AaronJackson | vrn | 4.4k | https://github.com/AaronJackson | . 21 google | model-viewer | 4.1k | https://github.com/google | . 22 ssloy | tinyraytracer | 4.1k | https://github.com/ssloy | . 23 mosra | magnum | 3.9k | https://github.com/mosra | . 24 FyroxEngine | Fyrox | 3.5k | https://github.com/FyroxEngine | . 25 gfxfundamentals | webgl-fundamentals | 3.5k | https://github.com/gfxfundamentals | . 26 tengbao | vanta | 3.3k | https://github.com/tengbao | . 27 cleardusk | 3DDFA | 3.2k | https://github.com/cleardusk | . 28 jasonlong | isometric-contributions | 3.1k | https://github.com/jasonlong | . 29 cnr-isti-vclab | meshlab | 2.9k | https://github.com/cnr-isti-vclab | . As we can see, the function has returned a pandas DataFrame of the top 30 repos from the topic &#39;3d&#39;. Now, we&#39;ll make function to save this DataFrame as a csv file if we haven&#39;t already created a file on that topic. . Save topic file . def scrape_topic(topic_url, path): if os.path.exists(path): print(&quot;The file {} already exists. Skipping...&quot;.format(path)) return topic_df = get_topic_repos(get_topic_page(topic_url)) topic_df.to_csv(path, index=None) . Putting it all together . We have a funciton to get the list of topics | We have a function to create a CSV file for scraped repos from a topics page | Let&#39;s create a function to put them together | . def scrape_topics_repos(): print(&#39;Scraping list of topics&#39;) topics_df = scrape_topics() os.makedirs(&#39;data&#39;, exist_ok=True) for index, row in topics_df.iterrows(): print(f&quot;Scraping top repositories for {row[&#39;Title&#39;]}&quot;) scrape_topic(row[&#39;URL&#39;], f&quot;data/{row[&#39;Title&#39;]}.csv&quot;) . Let&#39;s run it to scrape the top repos for the all the topics on the first page of https://github.com/topics . scrape_topics_repos() . Scraping list of topics Scraping top repositories for 3D Scraping top repositories for Ajax Scraping top repositories for Algorithm Scraping top repositories for Amp Scraping top repositories for Android Scraping top repositories for Angular Scraping top repositories for Ansible Scraping top repositories for API Scraping top repositories for Arduino Scraping top repositories for ASP.NET Scraping top repositories for Atom Scraping top repositories for Awesome Lists Scraping top repositories for Amazon Web Services Scraping top repositories for Azure Scraping top repositories for Babel Scraping top repositories for Bash Scraping top repositories for Bitcoin Scraping top repositories for Bootstrap Scraping top repositories for Bot Scraping top repositories for C Scraping top repositories for Chrome Scraping top repositories for Chrome extension Scraping top repositories for Command line interface Scraping top repositories for Clojure Scraping top repositories for Code quality Scraping top repositories for Code review Scraping top repositories for Compiler Scraping top repositories for Continuous integration Scraping top repositories for COVID-19 Scraping top repositories for C++ . Summary and Conclusion . As we can see, we have successfully scraped top 30 repositories for 30 topics. And we have saved the information on these top 30 repositories as a csv file for each topic separately. The information that we have scraped for each repository is its title, owner username, star count and its url. . We have scraped repositories for only 30 topics today. This was the number of topics available on the page 1 of https://github.com/topics. But it is easy top scrape more topics. What we just need to do is change the page number in the url https://github.com/topics?page={i} where &#39;i&#39; is the page number. This way, we can scrape info on top repos for all the topics of GitHub. .",
            "url": "https://ncitshubham.github.io/blogs/2021/08/29/Scrape_GitHub_topic_repos.html",
            "relUrl": "/2021/08/29/Scrape_GitHub_topic_repos.html",
            "date": " • Aug 29, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Predicting Mushroom Edibility",
            "content": "Introduction . Setup . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # data visualization import seaborn as sns # data visualization from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV from sklearn.preprocessing import OneHotEncoder, LabelEncoder from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay, precision_score, recall_score, accuracy_score, f1_score from sklearn.pipeline import make_pipeline from sklearn.dummy import DummyClassifier from sklearn.linear_model import LogisticRegression, RidgeClassifier from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier from xgboost import XGBClassifier from sklearn.naive_bayes import GaussianNB from sklearn.svm import SVC import warnings warnings.filterwarnings(&quot;ignore&quot;) import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . /kaggle/input/mushroom-classification/mushrooms.csv . file_dir = &quot;/kaggle/input/mushroom-classification/mushrooms.csv&quot; . !ls -lh {file_dir} . -rw-r--r-- 1 nobody nogroup 366K Nov 21 08:43 /kaggle/input/mushroom-classification/mushrooms.csv . !head {file_dir} . class,cap-shape,cap-surface,cap-color,bruises,odor,gill-attachment,gill-spacing,gill-size,gill-color,stalk-shape,stalk-root,stalk-surface-above-ring,stalk-surface-below-ring,stalk-color-above-ring,stalk-color-below-ring,veil-type,veil-color,ring-number,ring-type,spore-print-color,population,habitat p,x,s,n,t,p,f,c,n,k,e,e,s,s,w,w,p,w,o,p,k,s,u e,x,s,y,t,a,f,c,b,k,e,c,s,s,w,w,p,w,o,p,n,n,g e,b,s,w,t,l,f,c,b,n,e,c,s,s,w,w,p,w,o,p,n,n,m p,x,y,w,t,p,f,c,n,n,e,e,s,s,w,w,p,w,o,p,k,s,u e,x,s,g,f,n,f,w,b,k,t,e,s,s,w,w,p,w,o,e,n,a,g e,x,y,y,t,a,f,c,b,n,e,c,s,s,w,w,p,w,o,p,k,n,g e,b,s,w,t,a,f,c,b,g,e,c,s,s,w,w,p,w,o,p,k,n,m e,b,y,w,t,l,f,c,b,n,e,c,s,s,w,w,p,w,o,p,n,s,m p,x,y,w,t,p,f,c,n,p,e,e,s,s,w,w,p,w,o,p,k,v,g . Observations . The dataset file size is 366 KB. | It will be safe to import the whole dataset. | The prediction class is the first column. | There appears to be no index column | . df = pd.read_csv(file_dir) # view all columns pd.set_option(&quot;display.max_columns&quot;, None) df.head() . class cap-shape cap-surface cap-color bruises odor gill-attachment gill-spacing gill-size gill-color stalk-shape stalk-root stalk-surface-above-ring stalk-surface-below-ring stalk-color-above-ring stalk-color-below-ring veil-type veil-color ring-number ring-type spore-print-color population habitat . 0 p | x | s | n | t | p | f | c | n | k | e | e | s | s | w | w | p | w | o | p | k | s | u | . 1 e | x | s | y | t | a | f | c | b | k | e | c | s | s | w | w | p | w | o | p | n | n | g | . 2 e | b | s | w | t | l | f | c | b | n | e | c | s | s | w | w | p | w | o | p | n | n | m | . 3 p | x | y | w | t | p | f | c | n | n | e | e | s | s | w | w | p | w | o | p | k | s | u | . 4 e | x | s | g | f | n | f | w | b | k | t | e | s | s | w | w | p | w | o | e | n | a | g | . EDA and Data Preparation . X = df.copy() y = X.pop(&quot;class&quot;) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) . class_vc = df[&quot;class&quot;].value_counts() class_vc . e 4208 p 3916 Name: class, dtype: int64 . sns.barplot(x = class_vc.index, y = class_vc) . &lt;AxesSubplot:ylabel=&#39;class&#39;&gt; . The class counts are not much imbalanced. . Distribution of features . for i, cols in enumerate(df): feature_vc = df[cols].value_counts() print(feature_vc, &quot; n_________ n&quot;) plt.figure(i) sns.barplot(x = feature_vc.index, y = feature_vc) . e 4208 p 3916 Name: class, dtype: int64 _________ x 3656 f 3152 k 828 b 452 s 32 c 4 Name: cap-shape, dtype: int64 _________ y 3244 s 2556 f 2320 g 4 Name: cap-surface, dtype: int64 _________ n 2284 g 1840 e 1500 y 1072 w 1040 b 168 p 144 c 44 u 16 r 16 Name: cap-color, dtype: int64 _________ f 4748 t 3376 Name: bruises, dtype: int64 _________ n 3528 f 2160 y 576 s 576 a 400 l 400 p 256 c 192 m 36 Name: odor, dtype: int64 _________ f 7914 a 210 Name: gill-attachment, dtype: int64 _________ c 6812 w 1312 Name: gill-spacing, dtype: int64 _________ b 5612 n 2512 Name: gill-size, dtype: int64 _________ b 1728 p 1492 w 1202 n 1048 g 752 h 732 u 492 k 408 e 96 y 86 o 64 r 24 Name: gill-color, dtype: int64 _________ t 4608 e 3516 Name: stalk-shape, dtype: int64 _________ b 3776 ? 2480 e 1120 c 556 r 192 Name: stalk-root, dtype: int64 _________ s 5176 k 2372 f 552 y 24 Name: stalk-surface-above-ring, dtype: int64 _________ s 4936 k 2304 f 600 y 284 Name: stalk-surface-below-ring, dtype: int64 _________ w 4464 p 1872 g 576 n 448 b 432 o 192 e 96 c 36 y 8 Name: stalk-color-above-ring, dtype: int64 _________ w 4384 p 1872 g 576 n 512 b 432 o 192 e 96 c 36 y 24 Name: stalk-color-below-ring, dtype: int64 _________ p 8124 Name: veil-type, dtype: int64 _________ w 7924 n 96 o 96 y 8 Name: veil-color, dtype: int64 _________ o 7488 t 600 n 36 Name: ring-number, dtype: int64 _________ p 3968 e 2776 l 1296 f 48 n 36 Name: ring-type, dtype: int64 _________ w 2388 n 1968 k 1872 h 1632 r 72 u 48 o 48 y 48 b 48 Name: spore-print-color, dtype: int64 _________ v 4040 y 1712 s 1248 n 400 a 384 c 340 Name: population, dtype: int64 _________ d 3148 g 2148 p 1144 l 832 u 368 m 292 w 192 Name: habitat, dtype: int64 _________ . df[&quot;veil-type&quot;].unique() . array([&#39;p&#39;], dtype=object) . df.isnull().sum() . class 0 cap-shape 0 cap-surface 0 cap-color 0 bruises 0 odor 0 gill-attachment 0 gill-spacing 0 gill-size 0 gill-color 0 stalk-shape 0 stalk-root 0 stalk-surface-above-ring 0 stalk-surface-below-ring 0 stalk-color-above-ring 0 stalk-color-below-ring 0 veil-type 0 veil-color 0 ring-number 0 ring-type 0 spore-print-color 0 population 0 habitat 0 dtype: int64 . We also didn&#39;t need stratification.&quot;HistGradientBoosterClassifier&quot;: HistGradientBoostingClassifier(learning_rate = 0.05, random_state = 42, scoring=) . Train Baseline Model and Evaluate Results . ohe = OneHotEncoder(drop = &quot;first&quot;, handle_unknown = &quot;ignore&quot;, sparse = False) ohe_train_data = ohe.fit_transform(X_train) . le = LabelEncoder() y_train_le = le.fit_transform(y_train) . y_test_le = le.transform(y_test) . base_model = DummyClassifier(random_state = 42, strategy = &quot;most_frequent&quot;) base_cross_val = cross_validate(base_model, ohe_train_data, y_train_le, scoring = [&quot;accuracy&quot;, &quot;precision&quot;, &quot;recall&quot;, &quot;f1&quot;, &quot;roc_auc&quot;]) . base_cross_val . {&#39;fit_time&#39;: array([0.00459409, 0.00267696, 0.00175118, 0.00171733, 0.00184631]), &#39;score_time&#39;: array([0.00764799, 0.00651956, 0.00638986, 0.00660157, 0.00642753]), &#39;test_accuracy&#39;: array([0.51461538, 0.51461538, 0.51384615, 0.51384615, 0.51424172]), &#39;test_precision&#39;: array([0., 0., 0., 0., 0.]), &#39;test_recall&#39;: array([0., 0., 0., 0., 0.]), &#39;test_f1&#39;: array([0., 0., 0., 0., 0.]), &#39;test_roc_auc&#39;: array([0.5, 0.5, 0.5, 0.5, 0.5])} . Model Selection . models = {&quot;LogisticRegression&quot;: LogisticRegression(random_state = 42), &quot;RidgeClassification&quot;: RidgeClassifier(random_state = 42), &quot;GaussianNB&quot;: GaussianNB(), &quot;RandomForestClassifier&quot;: RandomForestClassifier(n_estimators = 70, random_state = 42), &quot;XGBClassifier&quot;: XGBClassifier(n_estimators = 70, objective = &quot;binary:logistic&quot;, learning_rate = 0.05, n_jobs = -1, scoring = &quot;auc&quot;, random_state = 42)} . model_scores = {} for model_name, model in models.items(): cross_val = cross_validate(model, ohe_train_data, y_train_le, n_jobs = -1, scoring = [&quot;accuracy&quot;, &quot;precision&quot;, &quot;recall&quot;, &quot;f1&quot;, &quot;roc_auc&quot;]) del cross_val[&quot;fit_time&quot;] del cross_val[&quot;score_time&quot;] model_scores[model_name] = cross_val . model_scores_df = pd.DataFrame.from_dict(model_scores) for col in model_scores_df: model_scores_df[col] = model_scores_df[col].mean() . model_scores_df . LogisticRegression RidgeClassification GaussianNB RandomForestClassifier XGBClassifier . test_accuracy 1.000000 | 1.000000 | 0.963843 | 1.0 | 1.0 | . test_precision 0.998741 | 0.998741 | 0.958774 | 1.0 | 1.0 | . test_recall 1.000000 | 1.000000 | 0.963281 | 1.0 | 1.0 | . test_f1 1.000000 | 1.000000 | 0.947865 | 1.0 | 1.0 | . test_roc_auc 0.999370 | 0.999370 | 0.948100 | 1.0 | 1.0 | . Train Final Model and make predictions . forest_clas = RandomForestClassifier(random_state = 42) ohe = OneHotEncoder(drop = &quot;first&quot;, handle_unknown = &quot;ignore&quot;, sparse = False) forest_pipe = make_pipeline(ohe, forest_clas) forest_pipe.fit(X_train, y_train_le) . Pipeline(steps=[(&#39;onehotencoder&#39;, OneHotEncoder(drop=&#39;first&#39;, handle_unknown=&#39;ignore&#39;, sparse=False)), (&#39;randomforestclassifier&#39;, RandomForestClassifier(random_state=42))]) . preds = forest_pipe.predict(X_test) preds_in = le.inverse_transform(preds) preds_in . array([&#39;e&#39;, &#39;p&#39;, &#39;e&#39;, ..., &#39;e&#39;, &#39;e&#39;, &#39;p&#39;], dtype=object) . ConfusionMatrixDisplay.from_predictions(y_test, preds_in) . &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fb2bcf36690&gt; . accuracy_score(y_test, preds_in) . 1.0 . recall_score(y_test_le, preds) . 1.0 . f1_score(y_test_le, preds) . 1.0 . RocCurveDisplay.from_estimator(forest_pipe, X_test, y_test_le) . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7fb2bcf51050&gt; . Summary and Conclusion .",
            "url": "https://ncitshubham.github.io/blogs/2021/06/02/classifying-mushrooms-on-edibility.html",
            "relUrl": "/2021/06/02/classifying-mushrooms-on-edibility.html",
            "date": " • Jun 2, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Exploratory Data Analysis of Road Accidents in USA",
            "content": "Introduction . Every year 1.3 million people die as a result of a road traffic crash around the world. And 20 - 50 million people suffer non-fatal injuries, with many incurring a disability as a result of their injury.1 In USA alone, there were 33,244 fatal motor vehicle crashes in 2019 in which 36,096 deaths occurred.2 . In this project, we are going to study the &#39;US Accidents&#39; dataset provided by Sobhan Moosavi on https://www.kaggle.com. We can analyse this data to gain some really interesting insights about road accidents in USA, such as the impact of environmental stimuli on accidental occurance, the change in the occurance of accidents with change in months, or which 5 states have the highest (or lowest) number of accidents. [Note: This Dataset covers 49 mainland states of the USA (excluding Alaska) for the time period: February 2016 to Dec 2020] . We will use the approach of Exploratory Data Analysis (EDA) for studying this data, which is used to analyse datasets to summarize their main characteristics, often using statistical graphics and other data visualization methods. EDA can help in seeing what the data can tell us beyond the formal modeling or hypothesis testing task.3 . Because this dataset is huge, with dozens of features, it can be used to answer a lot of questions. For this project, we are going to limit ourselves to 5-10 questions and then try to answer these questions. . Setup . Import the necessary libraries and get the file_path for the dataset. . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # data visualization import seaborn as sns # data visualization # Input data files are available in the read-only &quot;../input/&quot; directory # The following code will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;US_Accidents_Dec20_updated.csv/&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . US_Accidents_Dec20_updated.csv/US_Accidents_Dec20_updated.csv . Data Preparation and Cleaning . file_path = &quot;US_Accidents_Dec20_updated.csv/US_Accidents_Dec20_updated.csv&quot; us_accidents = pd.read_csv(file_path) . pd.set_option(&quot;display.max_columns&quot;, None) us_accidents.head() . ID Severity Start_Time End_Time Start_Lat Start_Lng End_Lat End_Lng Distance(mi) Description Number Street Side City County State Zipcode Country Timezone Airport_Code Weather_Timestamp Temperature(F) Wind_Chill(F) Humidity(%) Pressure(in) Visibility(mi) Wind_Direction Wind_Speed(mph) Precipitation(in) Weather_Condition Amenity Bump Crossing Give_Way Junction No_Exit Railway Roundabout Station Stop Traffic_Calming Traffic_Signal Turning_Loop Sunrise_Sunset Civil_Twilight Nautical_Twilight Astronomical_Twilight . 0 A-2716600 | 3 | 2016-02-08 00:37:08 | 2016-02-08 06:37:08 | 40.10891 | -83.09286 | 40.11206 | -83.03187 | 3.230 | Between Sawmill Rd/Exit 20 and OH-315/Olentang... | NaN | Outerbelt E | R | Dublin | Franklin | OH | 43017 | US | US/Eastern | KOSU | 2016-02-08 00:53:00 | 42.1 | 36.1 | 58.0 | 29.76 | 10.0 | SW | 10.4 | 0.00 | Light Rain | False | False | False | False | False | False | False | False | False | False | False | False | False | Night | Night | Night | Night | . 1 A-2716601 | 2 | 2016-02-08 05:56:20 | 2016-02-08 11:56:20 | 39.86542 | -84.06280 | 39.86501 | -84.04873 | 0.747 | At OH-4/OH-235/Exit 41 - Accident. | NaN | I-70 E | R | Dayton | Montgomery | OH | 45424 | US | US/Eastern | KFFO | 2016-02-08 05:58:00 | 36.9 | NaN | 91.0 | 29.68 | 10.0 | Calm | NaN | 0.02 | Light Rain | False | False | False | False | False | False | False | False | False | False | False | False | False | Night | Night | Night | Night | . 2 A-2716602 | 2 | 2016-02-08 06:15:39 | 2016-02-08 12:15:39 | 39.10266 | -84.52468 | 39.10209 | -84.52396 | 0.055 | At I-71/US-50/Exit 1 - Accident. | NaN | I-75 S | R | Cincinnati | Hamilton | OH | 45203 | US | US/Eastern | KLUK | 2016-02-08 05:53:00 | 36.0 | NaN | 97.0 | 29.70 | 10.0 | Calm | NaN | 0.02 | Overcast | False | False | False | False | True | False | False | False | False | False | False | False | False | Night | Night | Night | Day | . 3 A-2716603 | 2 | 2016-02-08 06:15:39 | 2016-02-08 12:15:39 | 39.10148 | -84.52341 | 39.09841 | -84.52241 | 0.219 | At I-71/US-50/Exit 1 - Accident. | NaN | US-50 E | R | Cincinnati | Hamilton | OH | 45202 | US | US/Eastern | KLUK | 2016-02-08 05:53:00 | 36.0 | NaN | 97.0 | 29.70 | 10.0 | Calm | NaN | 0.02 | Overcast | False | False | False | False | True | False | False | False | False | False | False | False | False | Night | Night | Night | Day | . 4 A-2716604 | 2 | 2016-02-08 06:51:45 | 2016-02-08 12:51:45 | 41.06213 | -81.53784 | 41.06217 | -81.53547 | 0.123 | At Dart Ave/Exit 21 - Accident. | NaN | I-77 N | R | Akron | Summit | OH | 44311 | US | US/Eastern | KAKR | 2016-02-08 06:54:00 | 39.0 | NaN | 55.0 | 29.65 | 10.0 | Calm | NaN | NaN | Overcast | False | False | False | False | False | False | False | False | False | False | False | False | False | Night | Night | Day | Day | . Premilinary analysis of dataset . us_accidents.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1516064 entries, 0 to 1516063 Data columns (total 47 columns): # Column Non-Null Count Dtype -- -- 0 ID 1516064 non-null object 1 Severity 1516064 non-null int64 2 Start_Time 1516064 non-null object 3 End_Time 1516064 non-null object 4 Start_Lat 1516064 non-null float64 5 Start_Lng 1516064 non-null float64 6 End_Lat 1516064 non-null float64 7 End_Lng 1516064 non-null float64 8 Distance(mi) 1516064 non-null float64 9 Description 1516064 non-null object 10 Number 469969 non-null float64 11 Street 1516064 non-null object 12 Side 1516064 non-null object 13 City 1515981 non-null object 14 County 1516064 non-null object 15 State 1516064 non-null object 16 Zipcode 1515129 non-null object 17 Country 1516064 non-null object 18 Timezone 1513762 non-null object 19 Airport_Code 1511816 non-null object 20 Weather_Timestamp 1485800 non-null object 21 Temperature(F) 1473031 non-null float64 22 Wind_Chill(F) 1066748 non-null float64 23 Humidity(%) 1470555 non-null float64 24 Pressure(in) 1479790 non-null float64 25 Visibility(mi) 1471853 non-null float64 26 Wind_Direction 1474206 non-null object 27 Wind_Speed(mph) 1387202 non-null float64 28 Precipitation(in) 1005515 non-null float64 29 Weather_Condition 1472057 non-null object 30 Amenity 1516064 non-null bool 31 Bump 1516064 non-null bool 32 Crossing 1516064 non-null bool 33 Give_Way 1516064 non-null bool 34 Junction 1516064 non-null bool 35 No_Exit 1516064 non-null bool 36 Railway 1516064 non-null bool 37 Roundabout 1516064 non-null bool 38 Station 1516064 non-null bool 39 Stop 1516064 non-null bool 40 Traffic_Calming 1516064 non-null bool 41 Traffic_Signal 1516064 non-null bool 42 Turning_Loop 1516064 non-null bool 43 Sunrise_Sunset 1515981 non-null object 44 Civil_Twilight 1515981 non-null object 45 Nautical_Twilight 1515981 non-null object 46 Astronomical_Twilight 1515981 non-null object dtypes: bool(13), float64(13), int64(1), object(20) memory usage: 412.1+ MB . us_accidents.describe() . Severity Start_Lat Start_Lng End_Lat End_Lng Distance(mi) Number Temperature(F) Wind_Chill(F) Humidity(%) Pressure(in) Visibility(mi) Wind_Speed(mph) Precipitation(in) . count 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 4.699690e+05 | 1.473031e+06 | 1.066748e+06 | 1.470555e+06 | 1.479790e+06 | 1.471853e+06 | 1.387202e+06 | 1.005515e+06 | . mean 2.238630e+00 | 3.690056e+01 | -9.859919e+01 | 3.690061e+01 | -9.859901e+01 | 5.872617e-01 | 8.907533e+03 | 5.958460e+01 | 5.510976e+01 | 6.465960e+01 | 2.955495e+01 | 9.131755e+00 | 7.630812e+00 | 8.477855e-03 | . std 6.081481e-01 | 5.165653e+00 | 1.849602e+01 | 5.165629e+00 | 1.849590e+01 | 1.632659e+00 | 2.242190e+04 | 1.827316e+01 | 2.112735e+01 | 2.325986e+01 | 1.016756e+00 | 2.889112e+00 | 5.637364e+00 | 1.293168e-01 | . min 1.000000e+00 | 2.457022e+01 | -1.244976e+02 | 2.457011e+01 | -1.244978e+02 | 0.000000e+00 | 0.000000e+00 | -8.900000e+01 | -8.900000e+01 | 1.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 25% 2.000000e+00 | 3.385422e+01 | -1.182076e+02 | 3.385420e+01 | -1.182077e+02 | 0.000000e+00 | 1.212000e+03 | 4.700000e+01 | 4.080000e+01 | 4.800000e+01 | 2.944000e+01 | 1.000000e+01 | 4.600000e+00 | 0.000000e+00 | . 50% 2.000000e+00 | 3.735113e+01 | -9.438100e+01 | 3.735134e+01 | -9.437987e+01 | 1.780000e-01 | 4.000000e+03 | 6.100000e+01 | 5.700000e+01 | 6.800000e+01 | 2.988000e+01 | 1.000000e+01 | 7.000000e+00 | 0.000000e+00 | . 75% 2.000000e+00 | 4.072593e+01 | -8.087469e+01 | 4.072593e+01 | -8.087449e+01 | 5.940000e-01 | 1.010000e+04 | 7.300000e+01 | 7.100000e+01 | 8.400000e+01 | 3.004000e+01 | 1.000000e+01 | 1.040000e+01 | 0.000000e+00 | . max 4.000000e+00 | 4.900058e+01 | -6.711317e+01 | 4.907500e+01 | -6.710924e+01 | 1.551860e+02 | 9.999997e+06 | 1.706000e+02 | 1.130000e+02 | 1.000000e+02 | 5.804000e+01 | 1.400000e+02 | 9.840000e+02 | 2.400000e+01 | . Number of null values in the dataset, column vise . null_val_cols = us_accidents.isnull().sum() null_val_cols = null_val_cols[null_val_cols &gt; 0].sort_values(ascending = False) null_val_cols . Number 1046095 Precipitation(in) 510549 Wind_Chill(F) 449316 Wind_Speed(mph) 128862 Humidity(%) 45509 Visibility(mi) 44211 Weather_Condition 44007 Temperature(F) 43033 Wind_Direction 41858 Pressure(in) 36274 Weather_Timestamp 30264 Airport_Code 4248 Timezone 2302 Zipcode 935 City 83 Sunrise_Sunset 83 Civil_Twilight 83 Nautical_Twilight 83 Astronomical_Twilight 83 dtype: int64 . missing_percentage = null_val_cols / len(us_accidents) * 100 missing_percentage.plot(kind = &#39;barh&#39;, figsize = (14, 8)) plt.xlabel(&quot;Percentage of missing values&quot;) plt.ylabel(&quot;Columns&quot;) plt.show() . The Number column represents the street number in address record. It has over a million null values. Because we already have the location data from the Start_Lat, Start_Lng, County and State features, we can choose to drop this column from the dataset. With regards to columns representing weather conditions, we can choose to drop the columns having a significant number of null values, and impute other columns to the mean values from the same State and month, which should represent similar weather conditions. . us_accidents.drop(labels = [&quot;Number&quot;, &quot;Precipitation(in)&quot;], axis = 1, inplace = True) us_accidents.shape . (1516064, 45) . We&#39;ll also convert the Start_Time column to datetime dtype, so that we can use use it to apply datetime functions later on. . us_accidents.Start_Time = us_accidents.Start_Time.astype(&quot;datetime64&quot;) . Exploratory Analysis and Visualization . With these columns in mind, we will try to answer the following questions from the dataset: . Are there more accidents in warmer or colder areas? | Which 5 states have the highest number of accidents? | Among the top 100 cities in number of accidents, which states do they belong to most frequently. | What time of the day are accidents most frequent in? | Which days of the week have the most accidents? | Which months have the most accidents? | What is the trend of accidents year over year (decreasing/increasing?) | How does accident severity change with change in precipitation? | We can start with plotting longitude and lattitude data to get a sense of where is the data concentrated . lat, long = us_accidents.Start_Lat, us_accidents.Start_Lng . plt.figure(figsize = (10, 6)) sns.scatterplot(x = long, y = lat, s = 0.5) . &lt;AxesSubplot:xlabel=&#39;Start_Lng&#39;, ylabel=&#39;Start_Lat&#39;&gt; . It appears that accidents are more concentrated near the coasts, which are more populated areas of US. There seems to be a sharp decline in the mid - mid-western US. Apart from lack of accidents in those areas, it could also be a signal of poor data collection in those states. . Q1. Are there more accidents in warmer or colder areas? . First, we&#39;ll plot a histogram for the temperature data. . us_accidents[&quot;Temperature(F)&quot;].plot(kind = &quot;hist&quot;, logy = True) plt.xlabel(&quot;Temperature(F)&quot;) plt.show() . print(f&#39;&#39;&#39;Mean: {us_accidents[&quot;Temperature(F)&quot;].mean()} Skewness: {us_accidents[&quot;Temperature(F)&quot;].skew()} Kurtosis: {us_accidents[&quot;Temperature(F)&quot;].kurtosis()}&#39;&#39;&#39;) . Mean: 58.5816432896377 Skewness: -0.30533266112446805 Kurtosis: -0.13261978426888454 . We can see that the data is normally distributed with very low skewness and kurtosis values. Thus, the data doesn&#39;t support the idea that warmer areas have more accidents than colder ones or vice versa. . Q2. Which 5 states have the highest number of accidents? . accidents_by_states = us_accidents.State.value_counts() print(&quot;The top 5 states in terms of number of accidents are:&quot;) accidents_by_states.head() . The top 5 states in terms of number of accidents are: . CA 448833 FL 153007 OR 87484 TX 75142 NY 60974 Name: State, dtype: int64 . plt.figure(figsize = (16, 5)) sns.barplot(x = accidents_by_states.index, y = accidents_by_states.values) . &lt;AxesSubplot:&gt; . Q3. Among the top 100 cities in number of accidents, which states do they belong to most frequently. . city_accidents = us_accidents.City.value_counts() city_accidents . Los Angeles 39984 Miami 36233 Charlotte 22203 Houston 20843 Dallas 19497 ... Manzanita 1 West Brooklyn 1 Garfield Heights 1 Belding 1 American Fork-Pleasant Grove 1 Name: City, Length: 10657, dtype: int64 . city_states = us_accidents.groupby(&quot;City&quot;)[&quot;State&quot;].aggregate(pd.Series.mode) city_states . City Aaronsburg PA Abbeville SC Abbotsford WI Abbottstown PA Aberdeen MD .. Zortman MT Zumbro Falls MN Zumbrota MN Zuni VA Zwingle IA Name: State, Length: 8769, dtype: object . top_100_cities = pd.concat([city_accidents, city_states], axis = 1)[:100] top_100_cities . City State . Los Angeles 27760 | CA | . Miami 26831 | FL | . Orlando 10772 | FL | . Dallas 10522 | TX | . Charlotte 10312 | NC | . ... ... | ... | . Flint 1465 | MI | . Hollywood 1431 | FL | . Eugene 1427 | OR | . Silver Spring 1425 | MD | . Birmingham 1422 | AL | . 100 rows × 2 columns . most_freq_states = top_100_cities.State.value_counts() most_freq_states . CA 35 FL 13 TX 5 NY 4 OR 4 MI 3 VA 3 LA 3 PA 3 SC 2 MO 2 UT 2 AZ 2 TN 2 MN 2 NC 2 OH 2 OK 1 NJ 1 MD 1 KY 1 IN 1 CO 1 DC 1 WA 1 IL 1 GA 1 AL 1 Name: State, dtype: int64 . most_freq_states.plot(kind = &quot;bar&quot;, figsize = (15,5)) plt.xlabel(&quot;States&quot;) plt.ylabel(&quot;Frequency in top 100 cities by number of accidents&quot;) plt.title(&quot;Most frequent states in the top 100 cities in the number of accidents&quot;) plt.show() . Q4. What time of the day are accidents most frequent in? . Do more accidents tend to occur at a particular time of the day? . us_accidents.Start_Time.dt.hour.plot(kind = &#39;hist&#39;, bins = 24) plt.xlabel(&quot;Hour of the day&quot;) plt.show() . It appears that accidents tend to occur more in the morning between 7-9 and in the afternoon between 13-17. Office hours could be the reason behind this trend. We can examine this by separating the data for weekdays and weekends. . weekdays = us_accidents.Start_Time.dt.dayofweek &lt; 5 us_accidents.loc[weekdays].Start_Time.dt.hour.plot(kind = &#39;hist&#39;, bins = 24) plt.xlabel(&quot;Hour of the day&quot;) plt.title(&quot;Accidents frequency of Weekdays&quot;) plt.show() . us_accidents.loc[~weekdays].Start_Time.dt.hour.plot(kind = &#39;hist&#39;, bins = 24) plt.xlabel(&quot;Hour of the day&quot;) plt.title(&quot;Accidents frequency of Weekends&quot;) plt.show() . On weekends, there seems to be a breakaway from the pattern that we saw earlier, which supports the idea that the trend we saw earlier could be due to the traffic resulting from office timings, which are usually closed on weekends. On weekends, accidents tend to occur more during daylight, which should be due to more traffic during those hours. . Q5. Which days of the week have the most accidents? . sns.histplot(us_accidents.Start_Time.dt.dayofweek, bins = 7) plt.xlabel(&quot;Day of Week&quot;) plt.show() . Accidents occur more on weekdays and there is a sharp drop in their count on the weekends. . Q6. Which months have the most accidents? . sns.histplot(us_accidents.Start_Time.dt.month, bins = 12) plt.xlabel(&quot;Month&quot;) plt.show() . Later months of the year appear to witness more accidents. This cannot be due to temperature(winter season) because the count drops suddenly in the starting months of the year. Due to there being no other apparent cause, it demands further analysis. . We can start by looking at the trend year over year. . fig, axes = plt.subplots(2, 3, figsize = (15,10), ) year = 2016 for r in range(2): for c in range(3): if year &lt; 2021: year_data = us_accidents.loc[us_accidents.Start_Time.dt.year == year] sns.histplot(year_data.Start_Time.dt.month, bins = 12, ax = axes[r, c]) axes[r, c].set_title(year) axes[r, c].set_xlabel(&quot;Month of the year&quot;) year += 1 . We can see that probably because the data was started being collected in the year 2016, the starting months have a much lower number of datapoints. Also, in the year 2020, because of the coronavirus pandemic lockdown restrictions, there was a suddent drop in the middle of the year. And as the restrictions eased, more accidents started to occur. But this data requires further more analysis on the month wise trends. . Q7. What is the trend of accidents year over year (decreasing/increasing?) . us_accidents.Start_Time.dt.year.plot(kind = &#39;hist&#39;, bins = 5, title = &quot;Year wise trend&quot;, xticks = np.arange(2016, 2021), figsize = (7, 5)) plt.show() . The number of accidents year over year looks to be increasing. This can be attributed to better data collection in the later years. Thus, it need further analysis. . Summary and Conclusion . Many questions could be asked and explored, but here we analysed this dataset to answer 7 questions about road accidents in USA. To answer these questions, we first did some basic data preparation, and then went on to analyse the data. It is important to take note that the findings from the data analysis are correlational in nature and do not establish a causal relationship in any way. These findings are: . The number of accidents don&#39;t differ much between warmer and colder temperatures. | The top 5 states in number of accidents are California, Florida, Oregon, Texas and New York. | These 5 states are also the most frequent states in the top 100 cities by number of road accidents. | The number of accidents look to be highly correlated with the office hours. | This idea is also supported by the fact that weekdays see more accidents than weekends. | Although the number of accidents look to be increasing with years, but this could be attributed to better data collection in the later years. | . Thanks for reading. .",
            "url": "https://ncitshubham.github.io/blogs/2021/04/15/exploratory-data-analysis-of-road-accidents-in-usa.html",
            "relUrl": "/2021/04/15/exploratory-data-analysis-of-road-accidents-in-usa.html",
            "date": " • Apr 15, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a Data Analyst and a Software Developer in Python. .",
          "url": "https://ncitshubham.github.io/blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ncitshubham.github.io/blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}