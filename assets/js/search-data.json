{
  
    
        "post0": {
            "title": "Plotly Trial",
            "content": "import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # data visualization import seaborn as sns from sklearn.metrics import mean_squared_error # Scoring metric from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler, OneHotEncoder import psutil import GPUtil from tqdm import tqdm from bayes_opt import BayesianOptimization from sklearn.dummy import DummyRegressor from sklearn.linear_model import LinearRegression, Ridge from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor import lightgbm as lgb from sklearn.compose import make_column_transformer from sklearn.pipeline import make_pipeline import warnings warnings.filterwarnings(&quot;ignore&quot;) # get file path import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . df = pd.read_parquet(&quot;nyc_sample_train.parquet&quot;) mapp = df[[&#39;dropoff_longitude&#39;, &#39;dropoff_latitude&#39;, &#39;fare_amount&#39;]] mapp = mapp.loc[(df[&quot;dropoff_longitude&quot;] &gt;= -74.25) &amp; (df[&quot;dropoff_longitude&quot;] &lt;= -73.5) &amp; df[&#39;fare_amount&#39;] &lt;= 200] . # import holoviews as hv # from holoviews.element.tiles import EsriImagery # from holoviews.operation.datashader import datashade # hv.extension(&#39;bokeh&#39;) # map_tiles = EsriImagery().opts(alpha = 0.5, width=900, height=480, active_tools=[&#39;pan&#39;, &#39;wheel_zoom&#39;], bgcolor = &quot;black&quot;) # points = hv.Points(ds.utils.lnglat_to_meters(mapp[&#39;dropoff_longitude&#39;], mapp[&#39;dropoff_latitude&#39;])) # taxi_trips = datashade(points, x_sampling=0.00001, y_sampling=0.00001, cmap=cc.fire, width=900, height=480) # map_tiles * taxi_trips . # m = mapp.sample(frac = 0.001) . . # df = pd.read_csv(&#39;https://raw.githubusercontent.com/plotly/datasets/master/uber-rides-data1.csv&#39;) # dff = df.query(&#39;Lat &lt; 40.82&#39;).query(&#39;Lat &gt; 40.70&#39;).query(&#39;Lon &gt; -74.02&#39;).query(&#39;Lon &lt; -73.91&#39;) # import datashader as ds # cvs = ds.Canvas(plot_width=1000, plot_height=1000) # agg = cvs.points(dff, x=&#39;Lon&#39;, y=&#39;Lat&#39;) # # agg is an xarray object, see http://xarray.pydata.org/en/stable/ for more details # coords_lat, coords_lon = agg.coords[&#39;Lat&#39;].values, agg.coords[&#39;Lon&#39;].values # # Corners of the image, which need to be passed to mapbox # coordinates = [[coords_lon[0], coords_lat[0]], # [coords_lon[-1], coords_lat[0]], # [coords_lon[-1], coords_lat[-1]], # [coords_lon[0], coords_lat[-1]]] # from colorcet import fire # import datashader.transfer_functions as tf # img = tf.shade(agg, cmap=fire)[::-1].to_pil() # import plotly.express as px # # Trick to create rapidly a figure with mapbox axes # fig = px.scatter_mapbox(dff[:1], lat=&#39;Lat&#39;, lon=&#39;Lon&#39;, zoom=12) # # Add the datashader image as a mapbox layer image # fig.update_layout(mapbox_style=&quot;carto-darkmatter&quot;, # mapbox_layers = [ # { # &quot;sourcetype&quot;: &quot;image&quot;, # &quot;source&quot;: img, # &quot;coordinates&quot;: coordinates # }] # ) # fig.show() . mapp . dropoff_longitude dropoff_latitude fare_amount . 0 -73.841614 | 40.712276 | 4.500000 | . 1 -73.979271 | 40.782005 | 16.900000 | . 2 -73.991241 | 40.750561 | 5.700000 | . 3 -73.991570 | 40.758091 | 7.700000 | . 4 -73.956657 | 40.783764 | 5.300000 | . ... ... | ... | ... | . 549995 -73.972481 | 40.787483 | 20.500000 | . 549996 -73.983116 | 40.735619 | 25.870001 | . 549997 -73.959274 | 40.763645 | 8.500000 | . 549998 -73.984337 | 40.748650 | 14.100000 | . 549999 -73.965874 | 40.765377 | 6.100000 | . 536551 rows × 3 columns . import datashader as ds, colorcet as cc cvs = ds.Canvas(plot_width=850, plot_height=500, x_range = (-74.15, -73.7), y_range = (40.5, 41)) agg = cvs.points(mapp, &#39;dropoff_longitude&#39;, &#39;dropoff_latitude&#39;) img = ds.tf.shade(agg, cmap=cc.fire, how=&#39;log&#39;) . img . pickup = df[[&#39;pickup_longitude&#39;, &#39;pickup_latitude&#39;, &#39;fare_amount&#39;]] pickup = pickup.loc[(pickup[&#39;pickup_latitude&#39;] &gt;= 40.5) &amp; (pickup[&#39;pickup_latitude&#39;] &lt;= 41) &amp; (pickup[&#39;pickup_longitude&#39;] &gt;= -74.1) &amp; (pickup[&#39;pickup_longitude&#39;] &lt;= -73.7) &amp; (pickup[&#39;fare_amount&#39;] &gt; 0) &amp; (pickup[&#39;fare_amount&#39;] &lt;= 200)] Q1, Q3 = pickup[&#39;fare_amount&#39;].quantile(q = [0.25, 0.75]).values IQR = Q3 - Q1 fare_min, fare_max = Q1 - (1.5 * IQR), Q3 + (1.5 * IQR) fare_min, fare_max . (-3.75, 22.25) . plt.figure(figsize = (16, 12)) ax = sns.scatterplot(data = pickup, x=&#39;pickup_longitude&#39;, y=&#39;pickup_latitude&#39;, hue = pickup[&#39;fare_amount&#39;], palette = &#39;rocket_r&#39;, hue_norm = (fare_min, fare_max), s = 0.1) norm = plt.Normalize(fare_min, fare_max) sm = plt.cm.ScalarMappable(cmap=&quot;rocket_r&quot;, norm=norm) sm.set_array([]) # Remove the legend and add a colorbar ax.get_legend().remove() cbar = ax.figure.colorbar(sm) cbar.set_label(&#39;Fare Amount&#39;, rotation=270) plt.show() .",
            "url": "https://ncitshubham.github.io/blogs/2022/04/03/Introduction_plotly.html",
            "relUrl": "/2022/04/03/Introduction_plotly.html",
            "date": " • Apr 3, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Plotly Trial",
            "content": "import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # data visualization from sklearn.metrics import mean_squared_error # Scoring metric from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler, OneHotEncoder import psutil import GPUtil from tqdm import tqdm from bayes_opt import BayesianOptimization from sklearn.dummy import DummyRegressor from sklearn.linear_model import LinearRegression, Ridge from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor import lightgbm as lgb from sklearn.compose import make_column_transformer from sklearn.pipeline import make_pipeline import warnings warnings.filterwarnings(&quot;ignore&quot;) # get file path import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . df = pd.read_parquet(&quot;nyc_sample_train.parquet&quot;) mapp = df[[&#39;dropoff_longitude&#39;, &#39;dropoff_latitude&#39;, &#39;fare_amount&#39;]] mapp = mapp.loc[(df[&quot;dropoff_longitude&quot;] &gt;= -74.25) &amp; (df[&quot;dropoff_longitude&quot;] &lt;= -73.5) &amp; df[&#39;fare_amount&#39;] &lt;= 200] . # import holoviews as hv # from holoviews.element.tiles import EsriImagery # from holoviews.operation.datashader import datashade # hv.extension(&#39;bokeh&#39;) # map_tiles = EsriImagery().opts(alpha = 0.5, width=900, height=480, active_tools=[&#39;pan&#39;, &#39;wheel_zoom&#39;], bgcolor = &quot;black&quot;) # points = hv.Points(ds.utils.lnglat_to_meters(mapp[&#39;dropoff_longitude&#39;], mapp[&#39;dropoff_latitude&#39;])) # taxi_trips = datashade(points, x_sampling=0.00001, y_sampling=0.00001, cmap=cc.fire, width=900, height=480) # map_tiles * taxi_trips . import plotly.express as px m = mapp.sample(frac = 0.001) . px.scatter(m, x = &#39;dropoff_longitude&#39;, y = &#39;dropoff_latitude&#39;, color = &#39;fare_amount&#39;) .",
            "url": "https://ncitshubham.github.io/blogs/2022/04/01/Introduction_plotly.html",
            "relUrl": "/2022/04/01/Introduction_plotly.html",
            "date": " • Apr 1, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Predicting Customer Transactions for Santander Competition on Kaggle",
            "content": "Introduction . The Santander Group is a Spanish multinational financial services company based in Madrid and is the 16th largest financial institution in the world. It held a Kaggle competition in 2019 where the goal was to identify which customers would make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for the competition had the same structure as the real data the Santander Group has available to solve this problem. The competetion is one of the most popular competitions on Kaggle with over 8,000 participating teams. . Today, we&#39;ll also participate in this competition and work towards improving our standing on the competition leaderboard. The dataset provided has about 200 anonymized features, using which we have to predict the target class in the test dataset. The submission is evaluated based on the AUC ROC between the predicted probability and the observed target in the submission file. Thus, our goal is to make predictions with the highest possible AUC ROC score on the test dataset. We begin the work by first setting up the environment and importing the dataset files. . Setup . Import the required libraries, import the datasets and setup the training environment. . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # data visualization import seaborn as sns # data visualization from tqdm import tqdm # progress meter from sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate # training validation from sklearn.preprocessing import MinMaxScaler # numeric scaler from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, roc_auc_score, RocCurveDisplay, ConfusionMatrixDisplay # metrics from imblearn.over_sampling import SMOTE # oversampling imbalanced data from imblearn.pipeline import make_pipeline as make_imb_pipeline # imbalanced pipeline from bayes_opt import BayesianOptimization # hyperparameter tuning import psutil # cpu information # ML models from sklearn.dummy import DummyClassifier from sklearn.linear_model import LogisticRegression, RidgeClassifier from sklearn.naive_bayes import GaussianNB from sklearn.ensemble import RandomForestClassifier import lightgbm as lgb # ignore warnings import warnings warnings.filterwarnings(&quot;ignore&quot;) # set data_dir import os os.chdir(os.getcwd() + &quot; Santader_Transactions_Predictions&quot;) # file paths for dirname, _, filenames in os.walk(os.getcwd()): for filename in filenames: print(os.path.join(dirname, filename)) . C: Users ncits Downloads Santader_Transactions_Predictions final_submssion.csv C: Users ncits Downloads Santader_Transactions_Predictions private_LB.npy C: Users ncits Downloads Santader_Transactions_Predictions public_LB.npy C: Users ncits Downloads Santader_Transactions_Predictions sample_submission.csv C: Users ncits Downloads Santader_Transactions_Predictions synthetic_samples_indexes.npy C: Users ncits Downloads Santader_Transactions_Predictions test.csv C: Users ncits Downloads Santader_Transactions_Predictions train.csv C: Users ncits Downloads Santader_Transactions_Predictions .ipynb_checkpoints Untitled-checkpoint.ipynb . Here, the csv files are part of the official competition dataset. while all the other files are from a separate kernel and will be used for feature engineering in the later section. We have already added them for future use. . # file_paths train_path = r&quot;C: Users ncits Downloads Santader_Transactions_Predictions train.csv&quot; test_path = r&quot;C: Users ncits Downloads Santader_Transactions_Predictions test.csv&quot; submission_path = r&quot;C: Users ncits Downloads Santader_Transactions_Predictions sample_submission.csv&quot; . # training dataset file size !dir {train_path} /a/s print(&quot;-&quot; * 50) !dir {test_path} /a/s . Volume in drive C has no label. Volume Serial Number is 7257-2892 Directory of C: Users ncits Downloads Santader_Transactions_Predictions 11-12-2019 22:01 302,133,017 train.csv 1 File(s) 302,133,017 bytes Total Files Listed: 1 File(s) 302,133,017 bytes 0 Dir(s) 299,745,378,304 bytes free -- Volume in drive C has no label. Volume Serial Number is 7257-2892 Directory of C: Users ncits Downloads Santader_Transactions_Predictions 11-12-2019 22:00 301,526,706 test.csv 1 File(s) 301,526,706 bytes Total Files Listed: 1 File(s) 301,526,706 bytes 0 Dir(s) 299,745,378,304 bytes free . The training dataset file size is 289 MB and the test dataset file size is 288 MB. It is safe to import both datasets fully at once. . # import training dataset X = pd.read_csv(r&quot;C: Users ncits Downloads Santader_Transactions_Predictions train.csv&quot;, index_col = [&quot;ID_code&quot;]) # look at all columns pd.set_option(&quot;display.max_columns&quot;, None) X.head() . target var_0 var_1 var_2 var_3 var_4 var_5 var_6 var_7 var_8 var_9 var_10 var_11 var_12 var_13 var_14 var_15 var_16 var_17 var_18 var_19 var_20 var_21 var_22 var_23 var_24 var_25 var_26 var_27 var_28 var_29 var_30 var_31 var_32 var_33 var_34 var_35 var_36 var_37 var_38 var_39 var_40 var_41 var_42 var_43 var_44 var_45 var_46 var_47 var_48 var_49 var_50 var_51 var_52 var_53 var_54 var_55 var_56 var_57 var_58 var_59 var_60 var_61 var_62 var_63 var_64 var_65 var_66 var_67 var_68 var_69 var_70 var_71 var_72 var_73 var_74 var_75 var_76 var_77 var_78 var_79 var_80 var_81 var_82 var_83 var_84 var_85 var_86 var_87 var_88 var_89 var_90 var_91 var_92 var_93 var_94 var_95 var_96 var_97 var_98 var_99 var_100 var_101 var_102 var_103 var_104 var_105 var_106 var_107 var_108 var_109 var_110 var_111 var_112 var_113 var_114 var_115 var_116 var_117 var_118 var_119 var_120 var_121 var_122 var_123 var_124 var_125 var_126 var_127 var_128 var_129 var_130 var_131 var_132 var_133 var_134 var_135 var_136 var_137 var_138 var_139 var_140 var_141 var_142 var_143 var_144 var_145 var_146 var_147 var_148 var_149 var_150 var_151 var_152 var_153 var_154 var_155 var_156 var_157 var_158 var_159 var_160 var_161 var_162 var_163 var_164 var_165 var_166 var_167 var_168 var_169 var_170 var_171 var_172 var_173 var_174 var_175 var_176 var_177 var_178 var_179 var_180 var_181 var_182 var_183 var_184 var_185 var_186 var_187 var_188 var_189 var_190 var_191 var_192 var_193 var_194 var_195 var_196 var_197 var_198 var_199 . ID_code . train_0 0 | 8.9255 | -6.7863 | 11.9081 | 5.0930 | 11.4607 | -9.2834 | 5.1187 | 18.6266 | -4.9200 | 5.7470 | 2.9252 | 3.1821 | 14.0137 | 0.5745 | 8.7989 | 14.5691 | 5.7487 | -7.2393 | 4.2840 | 30.7133 | 10.5350 | 16.2191 | 2.5791 | 2.4716 | 14.3831 | 13.4325 | -5.1488 | -0.4073 | 4.9306 | 5.9965 | -0.3085 | 12.9041 | -3.8766 | 16.8911 | 11.1920 | 10.5785 | 0.6764 | 7.8871 | 4.6667 | 3.8743 | -5.2387 | 7.3746 | 11.5767 | 12.0446 | 11.6418 | -7.0170 | 5.9226 | -14.2136 | 16.0283 | 5.3253 | 12.9194 | 29.0460 | -0.6940 | 5.1736 | -0.7474 | 14.8322 | 11.2668 | 5.3822 | 2.0183 | 10.1166 | 16.1828 | 4.9590 | 2.0771 | -0.2154 | 8.6748 | 9.5319 | 5.8056 | 22.4321 | 5.0109 | -4.7010 | 21.6374 | 0.5663 | 5.1999 | 8.8600 | 43.1127 | 18.3816 | -2.3440 | 23.4104 | 6.5199 | 12.1983 | 13.6468 | 13.8372 | 1.3675 | 2.9423 | -4.5213 | 21.4669 | 9.3225 | 16.4597 | 7.9984 | -1.7069 | -21.4494 | 6.7806 | 11.0924 | 9.9913 | 14.8421 | 0.1812 | 8.9642 | 16.2572 | 2.1743 | -3.4132 | 9.4763 | 13.3102 | 26.5376 | 1.4403 | 14.7100 | 6.0454 | 9.5426 | 17.1554 | 14.1104 | 24.3627 | 2.0323 | 6.7602 | 3.9141 | -0.4851 | 2.5240 | 1.5093 | 2.5516 | 15.5752 | -13.4221 | 7.2739 | 16.0094 | 9.7268 | 0.8897 | 0.7754 | 4.2218 | 12.0039 | 13.8571 | -0.7338 | -1.9245 | 15.4462 | 12.8287 | 0.3587 | 9.6508 | 6.5674 | 5.1726 | 3.1345 | 29.4547 | 31.4045 | 2.8279 | 15.6599 | 8.3307 | -5.6011 | 19.0614 | 11.2663 | 8.6989 | 8.3694 | 11.5659 | -16.4727 | 4.0288 | 17.9244 | 18.5177 | 10.7800 | 9.0056 | 16.6964 | 10.4838 | 1.6573 | 12.1749 | -13.1324 | 17.6054 | 11.5423 | 15.4576 | 5.3133 | 3.6159 | 5.0384 | 6.6760 | 12.6644 | 2.7004 | -0.6975 | 9.5981 | 5.4879 | -4.7645 | -8.4254 | 20.8773 | 3.1531 | 18.5618 | 7.7423 | -10.1245 | 13.7241 | -3.5189 | 1.7202 | -8.4051 | 9.0164 | 3.0657 | 14.3691 | 25.8398 | 5.8764 | 11.8411 | -19.7159 | 17.5743 | 0.5857 | 4.4354 | 3.9642 | 3.1364 | 1.6910 | 18.5227 | -2.3978 | 7.8784 | 8.5635 | 12.7803 | -1.0914 | . train_1 0 | 11.5006 | -4.1473 | 13.8588 | 5.3890 | 12.3622 | 7.0433 | 5.6208 | 16.5338 | 3.1468 | 8.0851 | -0.4032 | 8.0585 | 14.0239 | 8.4135 | 5.4345 | 13.7003 | 13.8275 | -15.5849 | 7.8000 | 28.5708 | 3.4287 | 2.7407 | 8.5524 | 3.3716 | 6.9779 | 13.8910 | -11.7684 | -2.5586 | 5.0464 | 0.5481 | -9.2987 | 7.8755 | 1.2859 | 19.3710 | 11.3702 | 0.7399 | 2.7995 | 5.8434 | 10.8160 | 3.6783 | -11.1147 | 1.8730 | 9.8775 | 11.7842 | 1.2444 | -47.3797 | 7.3718 | 0.1948 | 34.4014 | 25.7037 | 11.8343 | 13.2256 | -4.1083 | 6.6885 | -8.0946 | 18.5995 | 19.3219 | 7.0118 | 1.9210 | 8.8682 | 8.0109 | -7.2417 | 1.7944 | -1.3147 | 8.1042 | 1.5365 | 5.4007 | 7.9344 | 5.0220 | 2.2302 | 40.5632 | 0.5134 | 3.1701 | 20.1068 | 7.7841 | 7.0529 | 3.2709 | 23.4822 | 5.5075 | 13.7814 | 2.5462 | 18.1782 | 0.3683 | -4.8210 | -5.4850 | 13.7867 | -13.5901 | 11.0993 | 7.9022 | 12.2301 | 0.4768 | 6.8852 | 8.0905 | 10.9631 | 11.7569 | -1.2722 | 24.7876 | 26.6881 | 1.8944 | 0.6939 | -13.6950 | 8.4068 | 35.4734 | 1.7093 | 15.1866 | 2.6227 | 7.3412 | 32.0888 | 13.9550 | 13.0858 | 6.6203 | 7.1051 | 5.3523 | 8.5426 | 3.6159 | 4.1569 | 3.0454 | 7.8522 | -11.5100 | 7.5109 | 31.5899 | 9.5018 | 8.2736 | 10.1633 | 0.1225 | 12.5942 | 14.5697 | 2.4354 | 0.8194 | 16.5346 | 12.4205 | -0.1780 | 5.7582 | 7.0513 | 1.9568 | -8.9921 | 9.7797 | 18.1577 | -1.9721 | 16.1622 | 3.6937 | 6.6803 | -0.3243 | 12.2806 | 8.6086 | 11.0738 | 8.9231 | 11.7700 | 4.2578 | -4.4223 | 20.6294 | 14.8743 | 9.4317 | 16.7242 | -0.5687 | 0.1898 | 12.2419 | -9.6953 | 22.3949 | 10.6261 | 29.4846 | 5.8683 | 3.8208 | 15.8348 | -5.0121 | 15.1345 | 3.2003 | 9.3192 | 3.8821 | 5.7999 | 5.5378 | 5.0988 | 22.0330 | 5.5134 | 30.2645 | 10.4968 | -7.2352 | 16.5721 | -7.3477 | 11.0752 | -5.5937 | 9.4878 | -14.9100 | 9.4245 | 22.5441 | -4.8622 | 7.6543 | -15.9319 | 13.3175 | -0.3566 | 7.6421 | 7.7214 | 2.5837 | 10.9516 | 15.4305 | 2.0339 | 8.1267 | 8.7889 | 18.3560 | 1.9518 | . train_2 0 | 8.6093 | -2.7457 | 12.0805 | 7.8928 | 10.5825 | -9.0837 | 6.9427 | 14.6155 | -4.9193 | 5.9525 | -0.3249 | -11.2648 | 14.1929 | 7.3124 | 7.5244 | 14.6472 | 7.6782 | -1.7395 | 4.7011 | 20.4775 | 17.7559 | 18.1377 | 1.2145 | 3.5137 | 5.6777 | 13.2177 | -7.9940 | -2.9029 | 5.8463 | 6.1439 | -11.1025 | 12.4858 | -2.2871 | 19.0422 | 11.0449 | 4.1087 | 4.6974 | 6.9346 | 10.8917 | 0.9003 | -13.5174 | 2.2439 | 11.5283 | 12.0406 | 4.1006 | -7.9078 | 11.1405 | -5.7864 | 20.7477 | 6.8874 | 12.9143 | 19.5856 | 0.7268 | 6.4059 | 9.3124 | 6.2846 | 15.6372 | 5.8200 | 1.1000 | 9.1854 | 12.5963 | -10.3734 | 0.8748 | 5.8042 | 3.7163 | -1.1016 | 7.3667 | 9.8565 | 5.0228 | -5.7828 | 2.3612 | 0.8520 | 6.3577 | 12.1719 | 19.7312 | 19.4465 | 4.5048 | 23.2378 | 6.3191 | 12.8046 | 7.4729 | 15.7811 | 13.3529 | 10.1852 | 5.4604 | 19.0773 | -4.4577 | 9.5413 | 11.9052 | 2.1447 | -22.4038 | 7.0883 | 14.1613 | 10.5080 | 14.2621 | 0.2647 | 20.4031 | 17.0360 | 1.6981 | -0.0269 | -0.3939 | 12.6317 | 14.8863 | 1.3854 | 15.0284 | 3.9995 | 5.3683 | 8.6273 | 14.1963 | 20.3882 | 3.2304 | 5.7033 | 4.5255 | 2.1929 | 3.1290 | 2.9044 | 1.1696 | 28.7632 | -17.2738 | 2.1056 | 21.1613 | 8.9573 | 2.7768 | -2.1746 | 3.6932 | 12.4653 | 14.1978 | -2.5511 | -0.9479 | 17.1092 | 11.5419 | 0.0975 | 8.8186 | 6.6231 | 3.9358 | -11.7218 | 24.5437 | 15.5827 | 3.8212 | 8.6674 | 7.3834 | -2.4438 | 10.2158 | 7.4844 | 9.1104 | 4.3649 | 11.4934 | 1.7624 | 4.0714 | -1.2681 | 14.3330 | 8.0088 | 4.4015 | 14.1479 | -5.1747 | 0.5778 | 14.5362 | -1.7624 | 33.8820 | 11.6041 | 13.2070 | 5.8442 | 4.7086 | 5.7141 | -1.0410 | 20.5092 | 3.2790 | -5.5952 | 7.3176 | 5.7690 | -7.0927 | -3.9116 | 7.2569 | -5.8234 | 25.6820 | 10.9202 | -0.3104 | 8.8438 | -9.7009 | 2.4013 | -4.2935 | 9.3908 | -13.2648 | 3.1545 | 23.0866 | -5.3000 | 5.3745 | -6.2660 | 10.1934 | -0.8417 | 2.9057 | 9.7905 | 1.6704 | 1.6858 | 21.6042 | 3.1417 | -6.5213 | 8.2675 | 14.7222 | 0.3965 | . train_3 0 | 11.0604 | -2.1518 | 8.9522 | 7.1957 | 12.5846 | -1.8361 | 5.8428 | 14.9250 | -5.8609 | 8.2450 | 2.3061 | 2.8102 | 13.8463 | 11.9704 | 6.4569 | 14.8372 | 10.7430 | -0.4299 | 15.9426 | 13.7257 | 20.3010 | 12.5579 | 6.8202 | 2.7229 | 12.1354 | 13.7367 | 0.8135 | -0.9059 | 5.9070 | 2.8407 | -15.2398 | 10.4407 | -2.5731 | 6.1796 | 10.6093 | -5.9158 | 8.1723 | 2.8521 | 9.1738 | 0.6665 | -3.8294 | -1.0370 | 11.7770 | 11.2834 | 8.0485 | -24.6840 | 12.7404 | -35.1659 | 0.7613 | 8.3838 | 12.6832 | 9.5503 | 1.7895 | 5.2091 | 8.0913 | 12.3972 | 14.4698 | 6.5850 | 3.3164 | 9.4638 | 15.7820 | -25.0222 | 3.4418 | -4.3923 | 8.6464 | 6.3072 | 5.6221 | 23.6143 | 5.0220 | -3.9989 | 4.0462 | 0.2500 | 1.2516 | 24.4187 | 4.5290 | 15.4235 | 11.6875 | 23.6273 | 4.0806 | 15.2733 | 0.7839 | 10.5404 | 1.6212 | -5.2896 | 1.6027 | 17.9762 | -2.3174 | 15.6298 | 4.5474 | 7.5509 | -7.5866 | 7.0364 | 14.4027 | 10.7795 | 7.2887 | -1.0930 | 11.3596 | 18.1486 | 2.8344 | 1.9480 | -19.8592 | 22.5316 | 18.6129 | 1.3512 | 9.3291 | 4.2835 | 10.3907 | 7.0874 | 14.3256 | 14.4135 | 4.2827 | 6.9750 | 1.6480 | 11.6896 | 2.5762 | -2.5459 | 5.3446 | 38.1015 | 3.5732 | 5.0988 | 30.5644 | 11.3025 | 3.9618 | -8.2464 | 2.7038 | 12.3441 | 12.5431 | -1.3683 | 3.5974 | 13.9761 | 14.3003 | 1.0486 | 8.9500 | 7.1954 | -1.1984 | 1.9586 | 27.5609 | 24.6065 | -2.8233 | 8.9821 | 3.8873 | 15.9638 | 10.0142 | 7.8388 | 9.9718 | 2.9253 | 10.4994 | 4.1622 | 3.7613 | 2.3701 | 18.0984 | 17.1765 | 7.6508 | 18.2452 | 17.0336 | -10.9370 | 12.0500 | -1.2155 | 19.9750 | 12.3892 | 31.8833 | 5.9684 | 7.2084 | 3.8899 | -11.0882 | 17.2502 | 2.5881 | -2.7018 | 0.5641 | 5.3430 | -7.1541 | -6.1920 | 18.2366 | 11.7134 | 14.7483 | 8.1013 | 11.8771 | 13.9552 | -10.4701 | 5.6961 | -3.7546 | 8.4117 | 1.8986 | 7.2601 | -0.4639 | -0.0498 | 7.9336 | -12.8279 | 12.4124 | 1.8489 | 4.4666 | 4.7433 | 0.7178 | 1.4214 | 23.0347 | -1.2706 | -2.9275 | 10.2922 | 17.9697 | -8.9996 | . train_4 0 | 9.8369 | -1.4834 | 12.8746 | 6.6375 | 12.2772 | 2.4486 | 5.9405 | 19.2514 | 6.2654 | 7.6784 | -9.4458 | -12.1419 | 13.8481 | 7.8895 | 7.7894 | 15.0553 | 8.4871 | -3.0680 | 6.5263 | 11.3152 | 21.4246 | 18.9608 | 10.1102 | 2.7142 | 14.2080 | 13.5433 | 3.1736 | -3.3423 | 5.9015 | 7.9352 | -3.1582 | 9.4668 | -0.0083 | 19.3239 | 12.4057 | 0.6329 | 2.7922 | 5.8184 | 19.3038 | 1.4450 | -5.5963 | 14.0685 | 11.9171 | 11.5111 | 6.9087 | -65.4863 | 13.8657 | 0.0444 | -0.1346 | 14.4268 | 13.3273 | 10.4857 | -1.4367 | 5.7555 | -8.5414 | 14.1482 | 16.9840 | 6.1812 | 1.9548 | 9.2048 | 8.6591 | -27.7439 | -0.4952 | -1.7839 | 5.2670 | -4.3205 | 6.9860 | 1.6184 | 5.0301 | -3.2431 | 40.1236 | 0.7737 | -0.7264 | 4.5886 | -4.5346 | 23.3521 | 1.0273 | 19.1600 | 7.1734 | 14.3937 | 2.9598 | 13.3317 | -9.2587 | -6.7075 | 7.8984 | 14.5265 | 7.0799 | 20.1670 | 8.0053 | 3.7954 | -39.7997 | 7.0065 | 9.3627 | 10.4316 | 14.0553 | 0.0213 | 14.7246 | 35.2988 | 1.6844 | 0.6715 | -22.9264 | 12.3562 | 17.3410 | 1.6940 | 7.1179 | 5.1934 | 8.8230 | 10.6617 | 14.0837 | 28.2749 | -0.1937 | 5.9654 | 1.0719 | 7.9923 | 2.9138 | -3.6135 | 1.4684 | 25.6795 | 13.8224 | 4.7478 | 41.1037 | 12.7140 | 5.2964 | 9.7289 | 3.9370 | 12.1316 | 12.5815 | 7.0642 | 5.6518 | 10.9346 | 11.4266 | 0.9442 | 7.7532 | 6.6173 | -6.8304 | 6.4730 | 17.1728 | 25.8128 | 2.6791 | 13.9547 | 6.6289 | -4.3965 | 11.7159 | 16.1080 | 7.6874 | 9.1570 | 11.5670 | -12.7047 | 3.7574 | 9.9110 | 20.1461 | 1.2995 | 5.8493 | 19.8234 | 4.7022 | 10.6101 | 13.0021 | -12.6068 | 27.0846 | 8.0913 | 33.5107 | 5.6953 | 5.4663 | 18.2201 | 6.5769 | 21.2607 | 3.2304 | -1.7759 | 3.1283 | 5.5518 | 1.4493 | -2.6627 | 19.8056 | 2.3705 | 18.4685 | 16.3309 | -3.3456 | 13.5261 | 1.7189 | 5.1743 | -7.6938 | 9.7685 | 4.8910 | 12.2198 | 11.8503 | -7.8931 | 6.4209 | 5.9270 | 16.0201 | -0.2829 | -1.4905 | 9.5214 | -0.1508 | 9.1942 | 13.2876 | -1.5121 | 3.9267 | 9.5031 | 17.9974 | -8.8104 | . # separate target class y = X.pop(&quot;target&quot;) . # import test dataset test_df = pd.read_csv(test_path, index_col = [&quot;ID_code&quot;]) test_df.head() . var_0 var_1 var_2 var_3 var_4 var_5 var_6 var_7 var_8 var_9 var_10 var_11 var_12 var_13 var_14 var_15 var_16 var_17 var_18 var_19 var_20 var_21 var_22 var_23 var_24 var_25 var_26 var_27 var_28 var_29 var_30 var_31 var_32 var_33 var_34 var_35 var_36 var_37 var_38 var_39 var_40 var_41 var_42 var_43 var_44 var_45 var_46 var_47 var_48 var_49 var_50 var_51 var_52 var_53 var_54 var_55 var_56 var_57 var_58 var_59 var_60 var_61 var_62 var_63 var_64 var_65 var_66 var_67 var_68 var_69 var_70 var_71 var_72 var_73 var_74 var_75 var_76 var_77 var_78 var_79 var_80 var_81 var_82 var_83 var_84 var_85 var_86 var_87 var_88 var_89 var_90 var_91 var_92 var_93 var_94 var_95 var_96 var_97 var_98 var_99 var_100 var_101 var_102 var_103 var_104 var_105 var_106 var_107 var_108 var_109 var_110 var_111 var_112 var_113 var_114 var_115 var_116 var_117 var_118 var_119 var_120 var_121 var_122 var_123 var_124 var_125 var_126 var_127 var_128 var_129 var_130 var_131 var_132 var_133 var_134 var_135 var_136 var_137 var_138 var_139 var_140 var_141 var_142 var_143 var_144 var_145 var_146 var_147 var_148 var_149 var_150 var_151 var_152 var_153 var_154 var_155 var_156 var_157 var_158 var_159 var_160 var_161 var_162 var_163 var_164 var_165 var_166 var_167 var_168 var_169 var_170 var_171 var_172 var_173 var_174 var_175 var_176 var_177 var_178 var_179 var_180 var_181 var_182 var_183 var_184 var_185 var_186 var_187 var_188 var_189 var_190 var_191 var_192 var_193 var_194 var_195 var_196 var_197 var_198 var_199 . ID_code . test_0 11.0656 | 7.7798 | 12.9536 | 9.4292 | 11.4327 | -2.3805 | 5.8493 | 18.2675 | 2.1337 | 8.8100 | -2.0248 | -4.3554 | 13.9696 | 0.3458 | 7.5408 | 14.5001 | 7.7028 | -19.0919 | 15.5806 | 16.1763 | 3.7088 | 18.8064 | 1.5899 | 3.0654 | 6.4509 | 14.1192 | -9.4902 | -2.1917 | 5.7107 | 3.7864 | -1.7981 | 9.2645 | 2.0657 | 12.7753 | 11.3334 | 8.1462 | -0.0610 | 3.5331 | 9.7804 | 8.7625 | -15.6305 | 18.8766 | 11.2864 | 11.8362 | 13.3680 | -31.9891 | 12.1776 | 8.7714 | 17.2011 | 16.8508 | 13.0534 | 14.4069 | -4.8525 | 7.3213 | -0.5259 | 16.6365 | 19.3036 | 6.4129 | -5.3948 | 9.3269 | 11.9314 | -3.5750 | -0.7706 | 0.8705 | 6.9282 | 2.8914 | 5.9744 | 17.4851 | 5.0125 | -1.4230 | 33.3401 | 0.8018 | -4.7906 | 30.2708 | 26.8339 | 21.7205 | 7.3075 | 14.0810 | 3.1192 | 17.4265 | 9.4883 | 16.9060 | 14.5117 | 10.0276 | -0.9706 | 20.4588 | 4.7945 | 20.4160 | 13.1633 | 7.9307 | -7.6509 | 7.0834 | 15.2324 | 10.1416 | 5.9156 | -0.5775 | 5.7600 | 30.3238 | 2.1251 | 1.8585 | -9.2198 | 17.3089 | 30.9548 | 1.4918 | 12.8721 | 3.4902 | 8.2856 | 11.9794 | 14.0176 | 15.0763 | 3.7662 | 6.0426 | 4.4243 | 14.1799 | 2.0921 | 1.5493 | 3.2206 | 0.0172 | -6.6602 | 8.4785 | 42.0248 | 11.4164 | 0.4564 | 9.4006 | 0.9685 | 12.4929 | 14.1240 | 4.0388 | -4.4442 | 16.6684 | 12.5380 | 0.9205 | 10.5998 | 7.5147 | -4.1748 | -0.4824 | 10.5267 | 17.7547 | -6.5226 | -2.5502 | -5.1547 | -2.1246 | 19.8319 | 13.0752 | 9.2275 | 3.0213 | 11.6793 | -11.6827 | 4.1017 | 5.2954 | 18.7741 | 9.8892 | 7.5219 | 14.9745 | 18.9880 | 1.0842 | 11.9125 | -4.5103 | 16.1361 | 11.0067 | 5.9232 | 5.4113 | 3.8302 | 5.7380 | -8.6105 | 22.9530 | 2.5531 | -0.2836 | 4.3416 | 5.1855 | 4.2603 | 1.6779 | 29.0849 | 8.4685 | 18.1317 | 12.2818 | -0.6912 | 10.2226 | -5.5579 | 2.2926 | -4.5358 | 10.3903 | -15.4937 | 3.9697 | 31.3521 | -1.1651 | 9.2874 | -23.5705 | 13.2643 | 1.6591 | -2.1556 | 11.8495 | -1.4300 | 2.4508 | 13.7112 | 2.4669 | 4.3654 | 10.7200 | 15.4722 | -8.7197 | . test_1 8.5304 | 1.2543 | 11.3047 | 5.1858 | 9.1974 | -4.0117 | 6.0196 | 18.6316 | -4.4131 | 5.9739 | -1.3809 | -0.3310 | 14.1129 | 2.5667 | 5.4988 | 14.1853 | 7.0196 | 4.6564 | 29.1609 | 0.0910 | 12.1469 | 3.1389 | 5.2578 | 2.4228 | 16.2064 | 13.5023 | -5.2341 | -3.6648 | 5.7080 | 2.9965 | -10.4720 | 11.4938 | -0.9660 | 15.3445 | 10.6361 | 0.8966 | 6.7428 | 2.3421 | 12.8678 | -1.5536 | 10.0309 | 3.1337 | 10.5742 | 11.7664 | 2.1782 | -41.1924 | 13.5322 | -17.3834 | 6.3806 | 12.5589 | 11.6887 | 25.3930 | 1.5776 | 6.8481 | 8.7348 | 16.4239 | 21.7056 | 6.9345 | 1.6678 | 9.5249 | 5.3383 | -18.7083 | 1.3382 | -1.7401 | 5.8398 | 3.1051 | 4.4307 | 16.0005 | 5.0306 | -7.3365 | 12.2806 | 0.6992 | -0.7772 | 21.5123 | 6.7803 | 18.1896 | 6.9388 | 22.1336 | 6.3755 | 13.1525 | 1.9772 | 14.0406 | 6.6904 | 9.9732 | -11.5679 | 20.4525 | 9.4951 | 9.6343 | 8.1252 | 2.6059 | -17.4201 | 7.1848 | 15.3484 | 10.6522 | 5.9897 | 0.3392 | 10.3516 | 29.8204 | 1.9998 | -1.4166 | -1.7257 | 15.4712 | 35.6020 | 1.6570 | 13.0783 | 2.7752 | 6.4986 | 4.6835 | 13.7963 | 17.7261 | 1.7375 | 5.5689 | 3.6609 | 8.9725 | 4.1159 | 1.0693 | 2.0234 | 8.2760 | -6.8610 | 0.2780 | 17.0488 | 11.6704 | 3.1215 | 8.5093 | 5.6367 | 12.0099 | 14.2372 | -6.1600 | -5.6690 | 8.9094 | 11.0605 | 0.4583 | 9.7974 | 7.0891 | 2.6849 | 8.4970 | 15.7774 | 4.8775 | 3.6129 | 6.7530 | 11.1003 | 15.3593 | 2.2105 | 8.2280 | 9.0717 | -5.0947 | 8.7644 | -2.2873 | 4.1240 | -13.3006 | 18.7454 | 9.3783 | 1.5284 | 16.0407 | 7.7732 | 1.4316 | 14.8679 | 3.3619 | 11.5799 | 14.2058 | 30.9641 | 5.6723 | 3.6873 | 13.0429 | -10.6572 | 15.5134 | 3.2185 | 9.0535 | 7.0535 | 5.3924 | -0.7720 | -8.1783 | 29.9227 | -5.6274 | 10.5018 | 9.6083 | -0.4935 | 8.1696 | -4.3605 | 5.2110 | 0.4087 | 12.0030 | -10.3812 | 5.8496 | 25.1958 | -8.8468 | 11.8263 | -8.7112 | 15.9072 | 0.9812 | 10.6165 | 8.8349 | 0.9403 | 10.1282 | 15.5765 | 0.4773 | -1.4852 | 9.8714 | 19.1293 | -20.9760 | . test_2 5.4827 | -10.3581 | 10.1407 | 7.0479 | 10.2628 | 9.8052 | 4.8950 | 20.2537 | 1.5233 | 8.3442 | -4.7057 | -3.0422 | 13.6751 | 3.8183 | 10.8535 | 14.2126 | 9.8837 | 2.6541 | 21.2181 | 20.8163 | 12.4666 | 12.3696 | 4.7473 | 2.7936 | 5.2189 | 13.5670 | -15.4246 | -0.1655 | 7.2633 | 3.4310 | -9.1508 | 9.7320 | 3.1062 | 22.3076 | 11.9593 | 9.9255 | 4.0702 | 4.9934 | 8.0667 | 0.8804 | -19.0841 | 5.2272 | 9.5977 | 12.1801 | 8.3565 | 15.1170 | 10.0921 | -20.8504 | 8.6758 | 8.1292 | 11.8932 | 10.6869 | -0.6434 | 5.6510 | 9.3742 | 25.8831 | 19.8701 | 5.4834 | -4.0304 | 8.5160 | 8.9776 | -5.6619 | 2.8117 | 2.5996 | 9.0986 | 7.1167 | 4.9466 | 13.8268 | 5.0093 | 4.7782 | 19.2081 | 0.4340 | 0.8459 | 34.8598 | 20.7048 | 16.4953 | -9.7077 | 19.6357 | 7.6587 | 15.5744 | 16.1691 | 14.3299 | 1.3360 | -0.4412 | -0.2830 | 14.9105 | -3.9016 | 14.6881 | 7.3220 | -5.1443 | -34.3488 | 7.0194 | 12.4785 | 9.6665 | 13.2595 | -0.5624 | 5.6347 | 9.5853 | 1.4515 | 1.7818 | -3.5065 | 14.1663 | 28.0256 | 1.3935 | 10.8257 | 4.2954 | 8.2125 | 26.2595 | 14.0232 | 19.4604 | 8.6896 | 8.1036 | 1.2057 | 8.9156 | 0.9777 | 2.3797 | 3.1638 | 37.8664 | -3.3864 | -2.4090 | 29.7978 | 12.2056 | 4.7688 | 7.9344 | 2.2102 | 12.6482 | 14.3377 | 2.3268 | 2.3930 | 13.7005 | 12.7047 | 0.7507 | 7.7726 | 6.5950 | 0.2990 | 12.9154 | 29.9162 | 6.8031 | 10.5031 | -6.0452 | -4.5298 | 1.3903 | 5.0469 | 12.9740 | 9.3878 | -0.1113 | 11.6749 | 16.8588 | 4.2600 | 14.6476 | 14.4431 | 14.1649 | 9.4875 | 16.5769 | 7.2638 | -2.2008 | 12.5953 | 7.4487 | 23.1407 | 10.4597 | 39.3654 | 5.5228 | 3.3159 | 4.3324 | -0.5382 | 13.3009 | 3.1243 | -4.1731 | 1.2330 | 6.1513 | -0.0391 | 1.4950 | 16.8874 | -2.9787 | 27.4035 | 15.8819 | -10.9660 | 15.6415 | -9.4056 | 4.4611 | -3.0835 | 8.5549 | -2.8517 | 13.4770 | 24.4721 | -3.4824 | 4.9178 | -2.0720 | 11.5390 | 1.1821 | -0.7484 | 10.9935 | 1.9803 | 2.1800 | 12.9813 | 2.1281 | -7.1086 | 7.0618 | 19.8956 | -23.1794 | . test_3 8.5374 | -1.3222 | 12.0220 | 6.5749 | 8.8458 | 3.1744 | 4.9397 | 20.5660 | 3.3755 | 7.4578 | 0.0095 | -5.0659 | 14.0526 | 13.5010 | 8.7660 | 14.7352 | 10.0383 | -15.3508 | 2.1273 | 21.4797 | 14.5372 | 12.5527 | 2.9707 | 4.2398 | 13.7796 | 14.1408 | 1.0061 | -1.3479 | 5.2570 | 6.5911 | 6.2161 | 9.5540 | 2.3628 | 10.2124 | 10.8047 | -2.5588 | 6.0720 | 3.2613 | 16.5632 | 8.8336 | -4.8327 | 0.9554 | 12.3754 | 11.4241 | 6.6917 | -12.9761 | 13.7343 | 5.0150 | 31.3923 | 5.8555 | 12.6082 | 1.4182 | -4.1185 | 6.2536 | 1.4257 | 13.5426 | 15.4090 | 6.8761 | 1.7476 | 10.0413 | 15.2857 | -4.1378 | 0.7928 | 2.5301 | 8.1458 | 2.5738 | 5.9876 | 13.0758 | 5.0087 | -9.7824 | 8.9289 | 0.4205 | -2.5463 | 2.9428 | 10.7087 | 12.2008 | 12.5465 | 19.4201 | 5.5060 | 14.1586 | 17.5941 | 15.4375 | -13.2668 | 14.0885 | 4.0357 | 22.3119 | 1.8571 | 16.5210 | 10.8149 | 0.3256 | -21.4797 | 6.9174 | 9.9483 | 10.3696 | 11.0362 | 0.1892 | 19.4321 | 40.3383 | 1.4105 | 2.6165 | 1.7021 | 2.5363 | 3.8763 | 1.5173 | 13.4083 | 2.8965 | 7.0919 | 21.6304 | 14.2000 | 23.0368 | 10.3445 | 6.0369 | 5.0227 | 12.6600 | 2.1278 | 4.0592 | 1.9084 | 11.6095 | 7.5397 | 8.1972 | 20.0844 | 10.4440 | 8.4676 | 5.0350 | 4.3103 | 12.0067 | 13.7149 | 1.6143 | -1.2328 | 22.7248 | 12.6609 | 0.8039 | 4.7666 | 6.7888 | 5.8537 | -4.5434 | 19.0111 | 12.6907 | -2.9322 | 12.7898 | 12.0466 | 13.1646 | 7.7063 | 11.6549 | 9.8274 | 1.8061 | 8.6963 | 1.8057 | 3.8265 | -16.3027 | 13.7106 | 9.7908 | 5.8497 | 15.4378 | 5.0372 | -8.7673 | 13.6035 | -3.5002 | 13.9785 | 14.6118 | 19.7251 | 5.3882 | 3.6775 | 7.4753 | -11.0780 | 24.8712 | 2.6415 | 2.2673 | 7.2788 | 5.6406 | 7.2048 | 3.4504 | 2.4130 | 11.1674 | 14.5499 | 10.6151 | -5.7922 | 13.9407 | 7.1078 | 1.1019 | 9.4590 | 9.8243 | 5.9917 | 5.1634 | 8.1154 | 3.6638 | 3.3102 | -19.7819 | 13.4499 | 1.3104 | 9.5702 | 9.0766 | 1.6580 | 3.5813 | 15.1874 | 3.1656 | 3.9567 | 9.2295 | 13.0168 | -4.2108 | . test_4 11.7058 | -0.1327 | 14.1295 | 7.7506 | 9.1035 | -8.5848 | 6.8595 | 10.6048 | 2.9890 | 7.1437 | 5.1025 | -3.2827 | 14.1013 | 8.9672 | 4.7276 | 14.5811 | 11.8615 | 3.1480 | 18.0126 | 13.8006 | 1.6026 | 16.3059 | 6.7954 | 3.6015 | 13.6569 | 13.8807 | 8.6228 | -2.2654 | 5.2255 | 7.0165 | -15.6961 | 10.6239 | -4.7674 | 17.5447 | 11.8668 | 3.0154 | 4.2546 | 6.7601 | 5.9613 | 0.3695 | -14.4364 | 5.1392 | 11.6336 | 12.0338 | 18.9670 | 12.0144 | 16.2096 | -2.1966 | 1.1174 | 13.4532 | 12.7925 | 4.3775 | -0.1543 | 5.6794 | 0.8210 | 19.1358 | 12.6589 | 6.4394 | 4.3425 | 8.7003 | 12.0586 | -10.4753 | -0.0337 | 5.6603 | 6.2529 | 1.5238 | 4.5356 | 20.1344 | 5.0267 | -1.8628 | 39.8219 | 1.0498 | -0.9113 | 38.5076 | 2.2201 | 9.5235 | 8.1522 | 14.9224 | 6.1573 | 15.5221 | 11.8133 | 16.7661 | -14.6524 | -0.4469 | 0.0306 | 22.5276 | 6.9774 | 2.2563 | 3.5779 | 1.4268 | 9.0680 | 7.0197 | 19.7765 | 10.0499 | 11.4803 | 0.2548 | 16.7029 | 45.5510 | 1.5795 | 0.1148 | -14.3858 | 17.8630 | 23.2274 | 1.4375 | 14.4838 | 4.3806 | 10.6976 | 18.4023 | 14.2212 | 16.0638 | 6.3933 | 6.8699 | 2.7253 | 12.6458 | 3.2376 | 3.4218 | -0.5658 | -5.6840 | 4.7753 | 10.3320 | 39.7127 | 11.2319 | -1.2978 | 12.4827 | 6.5034 | 12.7157 | 13.3054 | -1.9678 | -1.2363 | 11.5686 | 12.6428 | 0.4792 | 7.1984 | 7.1434 | -0.2056 | -16.3908 | 27.1589 | 23.5997 | -4.6175 | 11.7989 | 12.5683 | -3.6145 | 22.1069 | 9.5539 | 9.2721 | -1.6214 | 12.9327 | 6.8080 | 4.2135 | 22.1044 | 20.0502 | 6.9953 | 9.3823 | 20.5534 | 3.4368 | -15.2208 | 13.0974 | -14.0888 | 11.7586 | 14.5259 | 22.8700 | 5.6688 | 6.1159 | 13.2433 | -11.9785 | 26.2040 | 3.2348 | -5.5775 | 5.7036 | 6.1717 | -1.6039 | -2.4866 | 17.2728 | 2.3640 | 14.0037 | 12.9165 | -12.0311 | 10.1161 | -8.7562 | 6.0889 | -1.3620 | 10.3559 | -7.4915 | 9.4588 | 3.9829 | 5.8580 | 8.3635 | -24.8254 | 11.4928 | 1.6321 | 4.2259 | 9.1723 | 1.2835 | 3.3778 | 19.5542 | -0.2860 | -5.1612 | 7.2882 | 13.9260 | -9.1846 | . We&#39;ll also import the submission file which will be used for adding targets from the trained ML models. It will then be exported as a csv file, which will be submitted on Kaggle. . # import submssion file submission_df = pd.read_csv(submission_path) submission_df.head() . ID_code target . 0 test_0 | 0 | . 1 test_1 | 0 | . 2 test_2 | 0 | . 3 test_3 | 0 | . 4 test_4 | 0 | . This submission file will be used for adding targets from the trained ML models. It will then be exported as a csv file, which will be submitted on Kaggle. . # random state seed seed = 42 . EDA and Data Preparation . # Basic overview X.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 200000 entries, train_0 to train_199999 Columns: 200 entries, var_0 to var_199 dtypes: float64(200) memory usage: 306.7+ MB . Observations . The data is of 200000 * 200 shape | All of the values are numeric stored as dtype float64. | The 200 columns are named var_0 to var_199. | . Basic statistics . First we&#39;ll look at the basic statistics of both train and test datasets. . X.describe() . var_0 var_1 var_2 var_3 var_4 var_5 var_6 var_7 var_8 var_9 var_10 var_11 var_12 var_13 var_14 var_15 var_16 var_17 var_18 var_19 var_20 var_21 var_22 var_23 var_24 var_25 var_26 var_27 var_28 var_29 var_30 var_31 var_32 var_33 var_34 var_35 var_36 var_37 var_38 var_39 var_40 var_41 var_42 var_43 var_44 var_45 var_46 var_47 var_48 var_49 var_50 var_51 var_52 var_53 var_54 var_55 var_56 var_57 var_58 var_59 var_60 var_61 var_62 var_63 var_64 var_65 var_66 var_67 var_68 var_69 var_70 var_71 var_72 var_73 var_74 var_75 var_76 var_77 var_78 var_79 var_80 var_81 var_82 var_83 var_84 var_85 var_86 var_87 var_88 var_89 var_90 var_91 var_92 var_93 var_94 var_95 var_96 var_97 var_98 var_99 var_100 var_101 var_102 var_103 var_104 var_105 var_106 var_107 var_108 var_109 var_110 var_111 var_112 var_113 var_114 var_115 var_116 var_117 var_118 var_119 var_120 var_121 var_122 var_123 var_124 var_125 var_126 var_127 var_128 var_129 var_130 var_131 var_132 var_133 var_134 var_135 var_136 var_137 var_138 var_139 var_140 var_141 var_142 var_143 var_144 var_145 var_146 var_147 var_148 var_149 var_150 var_151 var_152 var_153 var_154 var_155 var_156 var_157 var_158 var_159 var_160 var_161 var_162 var_163 var_164 var_165 var_166 var_167 var_168 var_169 var_170 var_171 var_172 var_173 var_174 var_175 var_176 var_177 var_178 var_179 var_180 var_181 var_182 var_183 var_184 var_185 var_186 var_187 var_188 var_189 var_190 var_191 var_192 var_193 var_194 var_195 var_196 var_197 var_198 var_199 . count 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | . mean 10.679914 | -1.627622 | 10.715192 | 6.796529 | 11.078333 | -5.065317 | 5.408949 | 16.545850 | 0.284162 | 7.567236 | 0.394340 | -3.245596 | 14.023978 | 8.530232 | 7.537606 | 14.573126 | 9.333264 | -5.696731 | 15.244013 | 12.438567 | 13.290894 | 17.257883 | 4.305430 | 3.019540 | 10.584400 | 13.667496 | -4.055133 | -1.137908 | 5.532980 | 5.053874 | -7.687740 | 10.393046 | -0.512886 | 14.774147 | 11.434250 | 3.842499 | 2.187230 | 5.868899 | 10.642131 | 0.662956 | -6.725505 | 9.299858 | 11.222356 | 11.569954 | 8.948289 | -12.699667 | 11.326488 | -12.471737 | 14.704713 | 16.682499 | 12.740986 | 13.428912 | -2.528816 | 6.008569 | 1.137117 | 12.745852 | 16.629165 | 6.272014 | 3.177633 | 8.931124 | 12.155618 | -11.946744 | 0.874170 | 0.661173 | 6.369157 | 0.982891 | 5.794039 | 11.943223 | 5.018893 | -3.331515 | 24.446811 | 0.669756 | 0.640553 | 19.610888 | 19.518846 | 16.853732 | 6.050871 | 19.066993 | 5.349479 | 14.402136 | 5.795044 | 14.719024 | -3.471273 | 1.025817 | -2.590209 | 18.362721 | 5.621058 | 11.351483 | 8.702924 | 3.725208 | -16.548147 | 6.987541 | 12.739578 | 10.556740 | 10.999162 | -0.084344 | 14.400433 | 18.539645 | 1.752012 | -0.746296 | -6.600518 | 13.413526 | 22.294908 | 1.568393 | 11.509834 | 4.244744 | 8.617657 | 17.796266 | 14.224435 | 18.458001 | 5.513238 | 6.312603 | 3.317843 | 8.136542 | 3.081191 | 2.213717 | 2.402570 | 16.102233 | -5.305132 | 3.032849 | 24.521078 | 11.310591 | 1.192984 | 7.076254 | 4.272740 | 12.489165 | 13.202326 | 0.851507 | -1.127952 | 15.460314 | 12.257151 | 0.544674 | 7.799676 | 6.813270 | -4.826053 | -4.259472 | 22.968602 | 17.613651 | 1.210792 | 7.760193 | 3.423636 | 2.897596 | 11.983489 | 12.333698 | 8.647632 | 4.841328 | 10.341178 | -3.300779 | 3.990726 | 5.296237 | 16.817671 | 10.141542 | 7.633199 | 16.727902 | 6.974955 | -2.074128 | 13.209272 | -4.813552 | 17.914591 | 10.223282 | 24.259300 | 5.633293 | 5.362896 | 11.002170 | -2.871906 | 19.315753 | 2.963335 | -4.151155 | 4.937124 | 5.636008 | -0.004962 | -0.831777 | 19.817094 | -0.677967 | 20.210677 | 11.640613 | -2.799585 | 11.882933 | -1.014064 | 2.591444 | -2.741666 | 10.085518 | 0.719109 | 8.769088 | 12.756676 | -3.983261 | 8.970274 | -10.335043 | 15.377174 | 0.746072 | 3.234440 | 7.438408 | 1.927839 | 3.331774 | 17.993784 | -0.142088 | 2.303335 | 8.908158 | 15.870720 | -3.326537 | . std 3.040051 | 4.050044 | 2.640894 | 2.043319 | 1.623150 | 7.863267 | 0.866607 | 3.418076 | 3.332634 | 1.235070 | 5.500793 | 5.970253 | 0.190059 | 4.639536 | 2.247908 | 0.411711 | 2.557421 | 6.712612 | 7.851370 | 7.996694 | 5.876254 | 8.196564 | 2.847958 | 0.526893 | 3.777245 | 0.285535 | 5.922210 | 1.523714 | 0.783367 | 2.615942 | 7.965198 | 2.159891 | 2.587830 | 4.322325 | 0.541614 | 5.179559 | 3.119978 | 2.249730 | 4.278903 | 4.068845 | 8.279259 | 5.938088 | 0.695991 | 0.309599 | 5.903073 | 21.404912 | 2.860511 | 10.579862 | 11.384332 | 7.855762 | 0.691709 | 8.187306 | 4.985532 | 0.764753 | 8.414241 | 5.690072 | 3.540174 | 0.795026 | 4.296686 | 0.854798 | 4.222389 | 11.622948 | 2.026238 | 3.113089 | 1.485854 | 3.786493 | 1.121366 | 7.365115 | 0.007186 | 3.955723 | 11.951742 | 0.266696 | 3.944703 | 7.466303 | 14.112591 | 6.055322 | 7.938351 | 3.817292 | 1.993792 | 1.309055 | 7.436737 | 2.299567 | 8.479255 | 8.297229 | 6.225305 | 3.908536 | 7.751142 | 5.661867 | 2.491460 | 3.560554 | 13.152810 | 0.152641 | 4.186252 | 0.543341 | 2.768099 | 0.621125 | 8.525400 | 12.642382 | 0.715836 | 1.862550 | 9.181683 | 4.950537 | 8.628179 | 0.185020 | 1.970520 | 0.855698 | 1.894899 | 7.604723 | 0.171091 | 4.355031 | 3.823253 | 1.082404 | 1.591170 | 4.459077 | 0.985396 | 2.621851 | 1.650912 | 13.297662 | 8.799268 | 4.182796 | 12.121016 | 1.714416 | 5.168479 | 6.147345 | 2.736821 | 0.318100 | 0.776056 | 3.137684 | 3.238043 | 4.136453 | 0.832199 | 0.456280 | 1.456486 | 0.375603 | 6.166126 | 7.617732 | 10.382235 | 8.890516 | 4.551750 | 7.686433 | 4.896325 | 6.715637 | 5.691936 | 2.934706 | 0.922469 | 3.899281 | 2.518883 | 7.413301 | 0.199192 | 10.385133 | 2.464157 | 3.962426 | 3.005373 | 2.014200 | 4.961678 | 5.771261 | 0.955140 | 5.570272 | 7.885579 | 4.122912 | 10.880263 | 0.217938 | 1.419612 | 5.262056 | 5.457784 | 5.024182 | 0.369684 | 7.798020 | 3.105986 | 0.369437 | 4.424621 | 5.378008 | 8.674171 | 5.966674 | 7.136427 | 2.892167 | 7.513939 | 2.628895 | 8.579810 | 2.798956 | 5.261243 | 1.371862 | 8.963434 | 4.474924 | 9.318280 | 4.725167 | 3.189759 | 11.574708 | 3.944604 | 0.976348 | 4.559922 | 3.023272 | 1.478423 | 3.992030 | 3.135162 | 1.429372 | 5.454369 | 0.921625 | 3.010945 | 10.438015 | . min 0.408400 | -15.043400 | 2.117100 | -0.040200 | 5.074800 | -32.562600 | 2.347300 | 5.349700 | -10.505500 | 3.970500 | -20.731300 | -26.095000 | 13.434600 | -6.011100 | 1.013300 | 13.076900 | 0.635100 | -33.380200 | -10.664200 | -12.402500 | -5.432200 | -10.089000 | -5.322500 | 1.209800 | -0.678400 | 12.720000 | -24.243100 | -6.166800 | 2.089600 | -4.787200 | -34.798400 | 2.140600 | -8.986100 | 1.508500 | 9.816900 | -16.513600 | -8.095100 | -1.183400 | -6.337100 | -14.545700 | -35.211700 | -8.535900 | 8.859000 | 10.652800 | -9.939600 | -90.252500 | 1.206200 | -47.686200 | -23.902200 | -8.070700 | 10.385500 | -15.046200 | -24.721400 | 3.344900 | -26.778600 | -3.782600 | 2.761800 | 3.442300 | -12.600900 | 6.184000 | -2.100600 | -48.802700 | -6.328900 | -10.554400 | 1.611700 | -14.088800 | 1.336800 | -19.544300 | 4.993800 | -16.309400 | -17.027500 | -0.224000 | -12.383400 | -1.665800 | -34.101500 | -1.293600 | -21.633300 | 7.425700 | -1.818300 | 10.445400 | -18.042200 | 7.586500 | -30.026600 | -24.220100 | -24.439800 | 7.023000 | -19.272200 | -8.481600 | 1.350200 | -9.601400 | -61.718000 | 6.521800 | -1.018500 | 8.491600 | 2.819000 | -2.432400 | -12.158400 | -21.740000 | -0.603500 | -7.280600 | -39.179100 | 0.075700 | -7.382900 | 0.979300 | 4.084600 | 0.715300 | 0.942400 | -5.898000 | 13.729000 | 5.769700 | -9.239800 | 2.194200 | -2.030200 | -5.513900 | -0.050500 | -6.858600 | -3.163000 | -31.836900 | -37.527700 | -9.774200 | -18.696200 | 6.305200 | -15.194000 | -12.405900 | -7.053800 | 11.486100 | 11.265400 | -8.876900 | -11.755900 | 2.186300 | 9.528300 | -0.954800 | 2.890000 | 5.359300 | -24.254600 | -31.380800 | -9.949300 | -9.851000 | -16.468400 | -21.274300 | -15.459500 | -16.693700 | -7.108000 | 2.806800 | 5.444300 | -8.273400 | 0.427400 | -29.984000 | 3.320500 | -41.168300 | 9.242000 | -2.191500 | -2.880000 | 11.030800 | -8.196600 | -21.840900 | 9.996500 | -22.990400 | -4.554400 | -4.641600 | -7.452200 | 4.852600 | 0.623100 | -6.531700 | -19.997700 | 3.816700 | 1.851200 | -35.969500 | -5.250200 | 4.258800 | -14.506000 | -22.479300 | -11.453300 | -22.748700 | -2.995300 | 3.241500 | -29.116500 | 4.952100 | -29.273400 | -7.856100 | -22.037400 | 5.416500 | -26.001100 | -4.808200 | -18.489700 | -22.583300 | -3.022300 | -47.753600 | 4.412300 | -2.554300 | -14.093300 | -2.691700 | -3.814500 | -11.783400 | 8.694400 | -5.261000 | -14.209600 | 5.960600 | 6.299300 | -38.852800 | . 25% 8.453850 | -4.740025 | 8.722475 | 5.254075 | 9.883175 | -11.200350 | 4.767700 | 13.943800 | -2.317800 | 6.618800 | -3.594950 | -7.510600 | 13.894000 | 5.072800 | 5.781875 | 14.262800 | 7.452275 | -10.476225 | 9.177950 | 6.276475 | 8.627800 | 11.551000 | 2.182400 | 2.634100 | 7.613000 | 13.456400 | -8.321725 | -2.307900 | 4.992100 | 3.171700 | -13.766175 | 8.870000 | -2.500875 | 11.456300 | 11.032300 | 0.116975 | -0.007125 | 4.125475 | 7.591050 | -2.199500 | -12.831825 | 4.519575 | 10.713200 | 11.343800 | 5.313650 | -28.730700 | 9.248750 | -20.654525 | 6.351975 | 10.653475 | 12.269000 | 7.267625 | -6.065025 | 5.435600 | -5.147625 | 8.163900 | 14.097875 | 5.687500 | 0.183500 | 8.312400 | 8.912750 | -20.901725 | -0.572400 | -1.588700 | 5.293500 | -1.702800 | 4.973800 | 6.753200 | 5.014000 | -6.336625 | 15.256625 | 0.472300 | -2.197100 | 14.097275 | 9.595975 | 12.480975 | 0.596300 | 16.014700 | 3.817275 | 13.375400 | 0.694475 | 13.214775 | -10.004950 | -5.106400 | -7.216125 | 15.338575 | 0.407550 | 7.247175 | 6.918775 | 1.140500 | -26.665600 | 6.869900 | 9.670300 | 10.195600 | 8.828000 | -0.527400 | 7.796950 | 8.919525 | 1.267675 | -2.106200 | -13.198700 | 9.639800 | 16.047975 | 1.428900 | 10.097900 | 3.639600 | 7.282300 | 12.168075 | 14.098900 | 15.107175 | 2.817475 | 5.510100 | 2.092675 | 4.803250 | 2.388775 | 0.399700 | 1.171875 | 6.373500 | -11.587850 | -0.161975 | 15.696275 | 9.996400 | -2.565200 | 2.817050 | 2.353600 | 12.245400 | 12.608400 | -1.502325 | -3.580725 | 12.514475 | 11.619300 | 0.207800 | 6.724375 | 6.543500 | -9.625700 | -9.957100 | 14.933900 | 10.656550 | -2.011825 | 2.387575 | -0.121700 | -2.153725 | 7.900000 | 10.311200 | 7.968075 | 1.885875 | 8.646900 | -8.751450 | 3.853600 | -1.903200 | 14.952200 | 7.064600 | 5.567900 | 15.233000 | 3.339900 | -6.266025 | 12.475100 | -8.939950 | 12.109200 | 7.243525 | 15.696125 | 5.470500 | 4.326100 | 7.029600 | -7.094025 | 15.744550 | 2.699000 | -9.643100 | 2.703200 | 5.374600 | -3.258500 | -4.720350 | 13.731775 | -5.009525 | 15.064600 | 9.371600 | -8.386500 | 9.808675 | -7.395700 | 0.625575 | -6.673900 | 9.084700 | -6.064425 | 5.423100 | 5.663300 | -7.360000 | 6.715200 | -19.205125 | 12.501550 | 0.014900 | -0.058825 | 5.157400 | 0.889775 | 0.584600 | 15.629800 | -1.170700 | -1.946925 | 8.252800 | 13.829700 | -11.208475 | . 50% 10.524750 | -1.608050 | 10.580000 | 6.825000 | 11.108250 | -4.833150 | 5.385100 | 16.456800 | 0.393700 | 7.629600 | 0.487300 | -3.286950 | 14.025500 | 8.604250 | 7.520300 | 14.574100 | 9.232050 | -5.666350 | 15.196250 | 12.453900 | 13.196800 | 17.234250 | 4.275150 | 3.008650 | 10.380350 | 13.662500 | -4.196900 | -1.132100 | 5.534850 | 4.950200 | -7.411750 | 10.365650 | -0.497650 | 14.576000 | 11.435200 | 3.917750 | 2.198000 | 5.900650 | 10.562700 | 0.672300 | -6.617450 | 9.162650 | 11.243400 | 11.565000 | 9.437200 | -12.547200 | 11.310750 | -12.482400 | 14.559200 | 16.672400 | 12.745600 | 13.444400 | -2.502450 | 6.027800 | 1.274050 | 12.594100 | 16.648150 | 6.262500 | 3.170100 | 8.901000 | 12.064350 | -11.892000 | 0.794700 | 0.681700 | 6.377700 | 1.021350 | 5.782000 | 11.922000 | 5.019100 | -3.325500 | 24.445000 | 0.668400 | 0.646450 | 19.309750 | 19.536650 | 16.844200 | 6.297800 | 18.967850 | 5.440050 | 14.388850 | 6.061750 | 14.844500 | -3.284450 | 1.069700 | -2.517950 | 18.296450 | 6.006700 | 11.288000 | 8.616200 | 3.642550 | -16.482600 | 6.986500 | 12.673500 | 10.582200 | 10.983850 | -0.098600 | 14.369900 | 18.502150 | 1.768300 | -0.771300 | -6.401500 | 13.380850 | 22.306850 | 1.566000 | 11.497950 | 4.224500 | 8.605150 | 17.573200 | 14.226600 | 18.281350 | 5.394300 | 6.340100 | 3.408400 | 8.148550 | 3.083800 | 2.249850 | 2.456300 | 15.944850 | -5.189500 | 3.023950 | 24.354700 | 11.239700 | 1.200700 | 7.234300 | 4.302100 | 12.486300 | 13.166800 | 0.925000 | -1.101750 | 15.426800 | 12.264650 | 0.556600 | 7.809100 | 6.806700 | -4.704250 | -4.111900 | 22.948300 | 17.257250 | 1.211750 | 8.066250 | 3.564700 | 2.975500 | 11.855900 | 12.356350 | 8.651850 | 4.904700 | 10.395600 | -3.178700 | 3.996000 | 5.283250 | 16.736950 | 10.127900 | 7.673700 | 16.649750 | 6.994050 | -2.066100 | 13.184300 | -4.868400 | 17.630450 | 10.217550 | 23.864500 | 5.633500 | 5.359700 | 10.788700 | -2.637800 | 19.270800 | 2.960200 | -4.011600 | 4.761600 | 5.634300 | 0.002800 | -0.807350 | 19.748000 | -0.569750 | 20.206100 | 11.679800 | -2.538450 | 11.737250 | -0.942050 | 2.512300 | -2.688800 | 10.036050 | 0.720200 | 8.600000 | 12.521000 | -3.946950 | 8.902150 | -10.209750 | 15.239450 | 0.742600 | 3.203600 | 7.347750 | 1.901300 | 3.396350 | 17.957950 | -0.172700 | 2.408900 | 8.888200 | 15.934050 | -2.819550 | . 75% 12.758200 | 1.358625 | 12.516700 | 8.324100 | 12.261125 | 0.924800 | 6.003000 | 19.102900 | 2.937900 | 8.584425 | 4.382925 | 0.852825 | 14.164200 | 12.274775 | 9.270425 | 14.874500 | 11.055900 | -0.810775 | 21.013325 | 18.433300 | 17.879400 | 23.089050 | 6.293200 | 3.403800 | 13.479600 | 13.863700 | -0.090200 | 0.015625 | 6.093700 | 6.798925 | -1.443450 | 11.885000 | 1.469100 | 18.097125 | 11.844400 | 7.487725 | 4.460400 | 7.542400 | 13.598925 | 3.637825 | -0.880875 | 13.754800 | 11.756900 | 11.804600 | 13.087300 | 3.150525 | 13.318300 | -4.244525 | 23.028650 | 22.549050 | 13.234500 | 19.385650 | 0.944350 | 6.542900 | 7.401825 | 17.086625 | 19.289700 | 6.845000 | 6.209700 | 9.566525 | 15.116500 | -3.225450 | 2.228200 | 3.020300 | 7.490600 | 3.739200 | 6.586200 | 17.037650 | 5.024100 | -0.498875 | 33.633150 | 0.864400 | 3.510700 | 25.207125 | 29.620700 | 21.432225 | 11.818800 | 22.041100 | 6.867200 | 15.383100 | 11.449125 | 16.340800 | 3.101725 | 7.449900 | 1.986700 | 21.358850 | 11.158375 | 15.433225 | 10.567025 | 6.146200 | -6.409375 | 7.101400 | 15.840225 | 10.944900 | 13.089100 | 0.329100 | 20.819375 | 28.158975 | 2.260900 | 0.528500 | 0.132100 | 17.250225 | 28.682225 | 1.705400 | 12.902100 | 4.822200 | 9.928900 | 23.348600 | 14.361800 | 21.852900 | 8.104325 | 7.080300 | 4.577400 | 11.596200 | 3.811900 | 4.121500 | 3.665100 | 25.780825 | 0.971800 | 6.098400 | 33.105275 | 12.619425 | 5.091700 | 11.734750 | 6.192200 | 12.718100 | 13.811700 | 3.293000 | 1.351700 | 18.480400 | 12.876700 | 0.901000 | 8.911425 | 7.070800 | -0.178800 | 1.125950 | 31.042425 | 24.426025 | 4.391225 | 13.232525 | 7.078525 | 8.192425 | 16.073925 | 14.461050 | 9.315000 | 7.676925 | 12.113225 | 2.028275 | 4.131600 | 12.688225 | 18.682500 | 13.057600 | 9.817300 | 18.263900 | 10.766350 | 1.891750 | 13.929300 | -0.988575 | 23.875325 | 13.094525 | 32.622850 | 5.792000 | 6.371200 | 14.623900 | 1.323600 | 23.024025 | 3.241500 | 1.318725 | 7.020025 | 5.905400 | 3.096400 | 2.956800 | 25.907725 | 3.619900 | 25.641225 | 13.745500 | 2.704400 | 13.931300 | 5.338750 | 4.391125 | 0.996200 | 11.011300 | 7.499175 | 12.127425 | 19.456150 | -0.590650 | 11.193800 | -1.466000 | 18.345225 | 1.482900 | 6.406200 | 9.512525 | 2.949500 | 6.205800 | 20.396525 | 0.829600 | 6.556725 | 9.593300 | 18.064725 | 4.836800 | . max 20.315000 | 10.376800 | 19.353000 | 13.188300 | 16.671400 | 17.251600 | 8.447700 | 27.691800 | 10.151300 | 11.150600 | 18.670200 | 17.188700 | 14.654500 | 22.331500 | 14.937700 | 15.863300 | 17.950600 | 19.025900 | 41.748000 | 35.183000 | 31.285900 | 49.044300 | 14.594500 | 4.875200 | 25.446000 | 14.654600 | 15.675100 | 3.243100 | 8.787400 | 13.143100 | 15.651500 | 20.171900 | 6.787100 | 29.546600 | 13.287800 | 21.528900 | 14.245600 | 11.863800 | 29.823500 | 15.322300 | 18.105600 | 26.165800 | 13.469600 | 12.577900 | 34.196100 | 62.084400 | 21.293900 | 20.685400 | 54.273800 | 41.153000 | 15.317200 | 40.689000 | 17.096800 | 8.231500 | 28.572400 | 29.092100 | 29.074100 | 9.160900 | 20.483300 | 11.986700 | 25.195500 | 27.102900 | 7.753600 | 11.231700 | 11.153700 | 15.731300 | 9.713200 | 39.396800 | 5.046900 | 8.547300 | 64.464400 | 1.571900 | 14.150000 | 44.536100 | 70.272000 | 36.156700 | 34.435200 | 30.956900 | 11.350700 | 18.225600 | 30.476900 | 23.132400 | 21.893400 | 27.714300 | 17.742400 | 32.901100 | 34.563700 | 33.354100 | 17.459400 | 15.481600 | 27.271300 | 7.489500 | 26.997600 | 12.534300 | 18.975000 | 1.804000 | 40.880600 | 58.287900 | 4.502800 | 5.076400 | 25.140900 | 28.459400 | 51.326500 | 2.188700 | 19.020600 | 7.169200 | 15.307400 | 46.379500 | 14.743000 | 32.059100 | 19.519300 | 9.800200 | 8.431700 | 21.542100 | 6.585000 | 11.950400 | 8.120700 | 64.810900 | 25.263500 | 15.688500 | 74.032100 | 17.307400 | 18.471400 | 26.874900 | 14.991500 | 13.664200 | 15.515600 | 10.597600 | 9.809600 | 31.203600 | 14.989500 | 2.192300 | 12.465000 | 8.309100 | 12.723600 | 21.412800 | 54.579400 | 44.437600 | 18.818700 | 36.097100 | 21.121900 | 23.965800 | 32.891100 | 22.691600 | 11.810100 | 16.008300 | 20.437300 | 22.149400 | 4.752800 | 48.424000 | 25.435700 | 21.124500 | 18.384600 | 24.007500 | 23.242800 | 16.831600 | 16.497000 | 11.972100 | 44.779500 | 25.120000 | 58.394200 | 6.309900 | 10.134400 | 27.564800 | 12.119300 | 38.332200 | 4.220400 | 21.276600 | 14.886100 | 7.089000 | 16.731900 | 17.917300 | 53.591900 | 18.855400 | 43.546800 | 20.854800 | 20.245200 | 20.596500 | 29.841300 | 13.448700 | 12.750500 | 14.393900 | 29.248700 | 23.704900 | 44.363400 | 12.997500 | 21.739200 | 22.786100 | 29.330300 | 4.034100 | 18.440900 | 16.716500 | 8.402400 | 18.281800 | 27.928800 | 4.272900 | 18.321500 | 12.000400 | 26.079100 | 28.500700 | . test_df.describe() . var_0 var_1 var_2 var_3 var_4 var_5 var_6 var_7 var_8 var_9 var_10 var_11 var_12 var_13 var_14 var_15 var_16 var_17 var_18 var_19 var_20 var_21 var_22 var_23 var_24 var_25 var_26 var_27 var_28 var_29 var_30 var_31 var_32 var_33 var_34 var_35 var_36 var_37 var_38 var_39 var_40 var_41 var_42 var_43 var_44 var_45 var_46 var_47 var_48 var_49 var_50 var_51 var_52 var_53 var_54 var_55 var_56 var_57 var_58 var_59 var_60 var_61 var_62 var_63 var_64 var_65 var_66 var_67 var_68 var_69 var_70 var_71 var_72 var_73 var_74 var_75 var_76 var_77 var_78 var_79 var_80 var_81 var_82 var_83 var_84 var_85 var_86 var_87 var_88 var_89 var_90 var_91 var_92 var_93 var_94 var_95 var_96 var_97 var_98 var_99 var_100 var_101 var_102 var_103 var_104 var_105 var_106 var_107 var_108 var_109 var_110 var_111 var_112 var_113 var_114 var_115 var_116 var_117 var_118 var_119 var_120 var_121 var_122 var_123 var_124 var_125 var_126 var_127 var_128 var_129 var_130 var_131 var_132 var_133 var_134 var_135 var_136 var_137 var_138 var_139 var_140 var_141 var_142 var_143 var_144 var_145 var_146 var_147 var_148 var_149 var_150 var_151 var_152 var_153 var_154 var_155 var_156 var_157 var_158 var_159 var_160 var_161 var_162 var_163 var_164 var_165 var_166 var_167 var_168 var_169 var_170 var_171 var_172 var_173 var_174 var_175 var_176 var_177 var_178 var_179 var_180 var_181 var_182 var_183 var_184 var_185 var_186 var_187 var_188 var_189 var_190 var_191 var_192 var_193 var_194 var_195 var_196 var_197 var_198 var_199 . count 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.00000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | . mean 10.658737 | -1.624244 | 10.707452 | 6.788214 | 11.076399 | -5.050558 | 5.415164 | 16.529143 | 0.277135 | 7.569407 | 0.371335 | -3.268551 | 14.022662 | 8.540872 | 7.532703 | 14.573704 | 9.321669 | -5.70445 | 15.265776 | 12.456675 | 13.298428 | 17.230598 | 4.299010 | 3.019707 | 10.567479 | 13.666970 | -3.983721 | -1.129536 | 5.530656 | 5.047247 | -7.687695 | 10.404920 | -0.524830 | 14.762686 | 11.434861 | 3.870130 | 2.213288 | 5.875048 | 10.647806 | 0.672667 | -6.736054 | 9.270646 | 11.221732 | 11.568972 | 8.952581 | -12.666095 | 11.299741 | -12.455577 | 14.685615 | 16.687280 | 12.745666 | 13.391711 | -2.516943 | 6.015355 | 1.133257 | 12.756876 | 16.604447 | 6.273477 | 3.190170 | 8.928052 | 12.161971 | -11.956023 | 0.882443 | 0.673228 | 6.368548 | 0.976359 | 5.786662 | 11.881600 | 5.018852 | -3.334390 | 24.442541 | 0.670278 | 0.648746 | 19.613597 | 19.445254 | 16.876525 | 6.071365 | 19.071853 | 5.348848 | 14.407224 | 5.815081 | 14.729931 | -3.426199 | 1.021512 | -2.565281 | 18.386720 | 5.637935 | 11.379621 | 8.738049 | 3.728791 | -16.334976 | 6.988214 | 12.735527 | 10.556921 | 10.989159 | -0.088772 | 14.425378 | 18.603350 | 1.751693 | -0.745673 | -6.674013 | 13.389322 | 22.334372 | 1.568707 | 11.523125 | 4.244311 | 8.621051 | 17.824277 | 14.225107 | 18.455457 | 5.497816 | 6.313691 | 3.311793 | 8.145250 | 3.086162 | 2.199727 | 2.392058 | 16.039925 | -5.310524 | 3.060541 | 24.536099 | 11.312735 | 1.191748 | 7.023936 | 4.279579 | 12.488117 | 13.205603 | 0.847175 | -1.132617 | 15.435012 | 12.256858 | 0.547403 | 7.801597 | 6.814902 | -4.806086 | -4.309017 | 23.019125 | 17.647446 | 1.243290 | 7.765888 | 3.411716 | 2.873279 | 11.971068 | 12.335213 | 8.644856 | 4.860353 | 10.362214 | -3.238206 | 3.991193 | 5.276210 | 16.820051 | 10.156002 | 7.628824 | 16.721077 | 6.971188 | -2.043747 | 13.208797 | -4.836848 | 17.924734 | 10.239229 | 24.146181 | 5.635300 | 5.360975 | 11.026376 | -2.857328 | 19.320760 | 2.962821 | -4.189133 | 4.930356 | 5.633716 | -0.020824 | -0.805148 | 19.779528 | -0.666240 | 20.264135 | 11.635715 | -2.776134 | 11.864538 | -0.949318 | 2.582604 | -2.722636 | 10.080827 | 0.651432 | 8.768929 | 12.719302 | -3.963045 | 8.978800 | -10.291919 | 15.366094 | 0.755673 | 3.189766 | 7.458269 | 1.925944 | 3.322016 | 17.996967 | -0.133657 | 2.290899 | 8.912428 | 15.869184 | -3.246342 | . std 3.036716 | 4.040509 | 2.633888 | 2.052724 | 1.616456 | 7.869293 | 0.864686 | 3.424482 | 3.333375 | 1.231865 | 5.508661 | 5.961443 | 0.190071 | 4.628712 | 2.255257 | 0.411592 | 2.544860 | 6.74646 | 7.846983 | 7.989812 | 5.884245 | 8.199877 | 2.844023 | 0.527951 | 3.771047 | 0.285454 | 5.945853 | 1.524765 | 0.785618 | 2.610078 | 7.971581 | 2.156324 | 2.588700 | 4.325727 | 0.541040 | 5.170614 | 3.120685 | 2.257235 | 4.260820 | 4.078592 | 8.298827 | 5.944335 | 0.694831 | 0.309927 | 5.911181 | 21.402708 | 2.860911 | 10.535003 | 11.382443 | 7.877107 | 0.690394 | 8.192913 | 4.969897 | 0.764381 | 8.418369 | 5.699920 | 3.549413 | 0.798123 | 4.303706 | 0.852982 | 4.211900 | 11.632700 | 2.029280 | 3.111174 | 1.480398 | 3.789367 | 1.124349 | 7.370539 | 0.007194 | 3.959641 | 11.933799 | 0.266336 | 3.957074 | 7.474120 | 14.078789 | 6.085044 | 7.966018 | 3.819166 | 1.996811 | 1.307898 | 7.427520 | 2.295052 | 8.426273 | 8.250587 | 6.228793 | 3.911504 | 7.762716 | 5.641466 | 2.497482 | 3.558041 | 13.174833 | 0.153239 | 4.199007 | 0.542281 | 2.765664 | 0.617136 | 8.552785 | 12.674441 | 0.713795 | 1.868773 | 9.181642 | 4.953360 | 8.630791 | 0.185295 | 1.974172 | 0.857720 | 1.890661 | 7.598867 | 0.171439 | 4.368647 | 3.834687 | 1.080271 | 1.592758 | 4.472500 | 0.983776 | 2.622971 | 1.652755 | 13.256813 | 8.767417 | 4.175211 | 12.105735 | 1.708636 | 5.145416 | 6.140683 | 2.728711 | 0.317715 | 0.779032 | 3.150284 | 3.243374 | 4.146347 | 0.832933 | 0.456736 | 1.454610 | 0.375246 | 6.170636 | 7.616104 | 10.373414 | 8.889635 | 4.564611 | 7.683951 | 4.907185 | 6.732620 | 5.685169 | 2.941028 | 0.923951 | 3.897157 | 2.514621 | 7.444975 | 0.198559 | 10.374152 | 2.463189 | 3.971911 | 3.015322 | 2.013893 | 4.972334 | 5.775461 | 0.951806 | 5.576516 | 7.867884 | 4.133414 | 10.876184 | 0.217936 | 1.426064 | 5.268894 | 5.457937 | 5.039303 | 0.370668 | 7.827428 | 3.086443 | 0.365750 | 4.417876 | 5.378492 | 8.678024 | 5.987419 | 7.141816 | 2.884821 | 7.557001 | 2.626556 | 8.570314 | 2.803890 | 5.225554 | 1.369546 | 8.961936 | 4.464461 | 9.316889 | 4.724641 | 3.206635 | 11.562352 | 3.929227 | 0.976123 | 4.551239 | 3.025189 | 1.479966 | 3.995599 | 3.140652 | 1.429678 | 5.446346 | 0.920904 | 3.008717 | 10.398589 | . min 0.188700 | -15.043400 | 2.355200 | -0.022400 | 5.484400 | -27.767000 | 2.216400 | 5.713700 | -9.956000 | 4.243300 | -22.672400 | -25.811800 | 13.424500 | -4.741300 | 0.670300 | 13.203400 | 0.314300 | -28.90690 | -11.324200 | -12.699400 | -2.634600 | -9.940600 | -5.164000 | 1.390600 | -0.731300 | 12.749600 | -24.536100 | -6.040900 | 2.842500 | -4.421500 | -34.054800 | 1.309200 | -8.209000 | 1.691100 | 9.776400 | -16.923800 | -10.466800 | -0.885100 | -5.368300 | -14.083700 | -34.952400 | -8.569600 | 8.983700 | 10.644400 | -9.867700 | -83.692100 | 1.512000 | -46.554200 | -25.114800 | -7.439000 | 10.199600 | -14.366900 | -23.347300 | 3.411400 | -25.595500 | -3.275100 | 3.170300 | 3.549400 | -14.979700 | 6.052800 | -1.518700 | -48.336900 | -6.389900 | -12.600400 | 1.486700 | -15.242900 | 1.940000 | -16.187300 | 4.996000 | -15.538300 | -17.027500 | -0.171700 | -12.728300 | -0.249000 | -37.347000 | -0.798800 | -21.382400 | 7.990400 | -0.762000 | 10.551700 | -16.704500 | 8.204700 | -30.314600 | -24.709100 | -24.563900 | 7.164700 | -17.967600 | -7.364900 | 2.011200 | -8.169300 | -57.385300 | 6.538800 | -0.074200 | 8.613600 | 2.817100 | -2.313700 | -10.657900 | -24.502700 | -0.596500 | -7.364000 | -40.949000 | -0.644800 | -7.925600 | 0.967500 | 3.852600 | 1.200800 | 1.064800 | -6.542200 | 13.737900 | 5.078200 | -9.396400 | 2.825500 | -2.379600 | -4.928300 | 0.077200 | -6.875000 | -2.891200 | -37.607300 | -38.295700 | -9.315500 | -19.541500 | 6.436000 | -14.097200 | -11.795700 | -6.352800 | 11.525500 | 11.274200 | -8.657300 | -11.514100 | 2.417400 | 9.754600 | -0.910300 | 3.014500 | 5.491500 | -25.002000 | -28.480000 | -9.753500 | -9.634100 | -16.697400 | -21.274300 | -15.459500 | -16.213500 | -7.310600 | 3.254000 | 5.637700 | -8.207100 | 1.056100 | -29.788300 | 3.324000 | -33.527000 | 10.001800 | -2.902700 | -2.309600 | 10.895800 | -7.835700 | -22.410000 | 10.415400 | -24.955700 | -4.451500 | -3.921200 | -8.925700 | 4.910600 | 0.106200 | -6.093700 | -21.514000 | 3.667300 | 1.813100 | -37.176400 | -5.405700 | 4.291500 | -15.593200 | -20.393600 | -11.796600 | -21.342800 | -2.485400 | 2.951200 | -29.838400 | 5.025300 | -29.118500 | -7.767400 | -20.610600 | 5.346000 | -28.092800 | -5.476800 | -17.011400 | -22.467000 | -2.303800 | -47.306400 | 4.429100 | -2.511500 | -14.093300 | -2.407000 | -3.340900 | -11.413100 | 9.382800 | -4.911900 | -13.944200 | 6.169600 | 6.584000 | -39.457800 | . 25% 8.442975 | -4.700125 | 8.735600 | 5.230500 | 9.891075 | -11.201400 | 4.772600 | 13.933900 | -2.303900 | 6.623800 | -3.626000 | -7.522000 | 13.891000 | 5.073375 | 5.769500 | 14.262400 | 7.454400 | -10.49790 | 9.237700 | 6.322300 | 8.589600 | 11.511500 | 2.178300 | 2.633300 | 7.610750 | 13.456200 | -8.265500 | -2.299000 | 4.986275 | 3.166200 | -13.781900 | 8.880600 | -2.518200 | 11.440500 | 11.033200 | 0.162400 | 0.016900 | 4.120700 | 7.601375 | -2.170300 | -12.878300 | 4.495000 | 10.718300 | 11.344000 | 5.315700 | -28.673200 | 9.220500 | -20.591000 | 6.396500 | 10.662225 | 12.274200 | 7.210600 | -6.021650 | 5.443900 | -5.132300 | 8.130400 | 14.060000 | 5.684900 | 0.195000 | 8.307700 | 8.927900 | -20.948500 | -0.564800 | -1.569400 | 5.298000 | -1.722400 | 4.963300 | 6.694775 | 5.014000 | -6.364100 | 15.328700 | 0.473700 | -2.201500 | 14.129400 | 9.545100 | 12.486500 | 0.588100 | 16.022600 | 3.804600 | 13.379575 | 0.721275 | 13.222100 | -9.897300 | -5.064400 | -7.165400 | 15.352500 | 0.421900 | 7.259875 | 6.949100 | 1.156800 | -26.461300 | 6.869900 | 9.670100 | 10.198200 | 8.818350 | -0.525800 | 7.747800 | 8.951900 | 1.269600 | -2.119700 | -13.250800 | 9.604400 | 16.129275 | 1.428400 | 10.114875 | 3.641700 | 7.285400 | 12.221100 | 14.098700 | 15.084800 | 2.773900 | 5.504375 | 2.088075 | 4.794500 | 2.396475 | 0.385500 | 1.159800 | 6.316600 | -11.520900 | -0.131100 | 15.761400 | 10.007300 | -2.543300 | 2.728400 | 2.380175 | 12.243600 | 12.605500 | -1.508125 | -3.597500 | 12.476500 | 11.621700 | 0.210500 | 6.731400 | 6.545800 | -9.628800 | -10.058525 | 14.972100 | 10.702150 | -1.988350 | 2.439000 | -0.127800 | -2.201625 | 7.908800 | 10.325300 | 7.963500 | 1.922100 | 8.678500 | -8.718200 | 3.855100 | -1.893100 | 14.961000 | 7.087400 | 5.540675 | 15.220575 | 3.319600 | -6.221800 | 12.479600 | -8.965800 | 12.058350 | 7.228600 | 15.567800 | 5.473000 | 4.308175 | 7.067775 | -7.051200 | 15.751000 | 2.696500 | -9.712000 | 2.729800 | 5.375200 | -3.250000 | -4.678450 | 13.722200 | -4.998900 | 15.126500 | 9.382575 | -8.408100 | 9.793700 | -7.337925 | 0.605200 | -6.604425 | 9.081000 | -6.154625 | 5.432025 | 5.631700 | -7.334000 | 6.705300 | -19.136225 | 12.492600 | 0.019400 | -0.095000 | 5.166500 | 0.882975 | 0.587600 | 15.634775 | -1.160700 | -1.948600 | 8.260075 | 13.847275 | -11.124000 | . 50% 10.513800 | -1.590500 | 10.560700 | 6.822350 | 11.099750 | -4.834100 | 5.391600 | 16.422700 | 0.372000 | 7.632000 | 0.491850 | -3.314950 | 14.024600 | 8.617400 | 7.496950 | 14.572700 | 9.228900 | -5.69820 | 15.203200 | 12.484250 | 13.218650 | 17.211300 | 4.269000 | 3.008000 | 10.344300 | 13.661200 | -4.125800 | -1.127800 | 5.529900 | 4.953100 | -7.409000 | 10.385350 | -0.535200 | 14.561400 | 11.435100 | 3.947500 | 2.219250 | 5.909800 | 10.563750 | 0.700850 | -6.615750 | 9.137500 | 11.243800 | 11.563800 | 9.433200 | -12.577800 | 11.276100 | -12.438800 | 14.511750 | 16.691500 | 12.755000 | 13.393000 | -2.500300 | 6.036500 | 1.296500 | 12.650600 | 16.627100 | 6.264300 | 3.188000 | 8.898600 | 12.096050 | -11.957200 | 0.817400 | 0.693500 | 6.375450 | 1.025400 | 5.772500 | 11.846850 | 5.019100 | -3.335600 | 24.426900 | 0.668900 | 0.656900 | 19.311700 | 19.571150 | 16.877200 | 6.301700 | 18.988850 | 5.442500 | 14.399200 | 6.046850 | 14.865200 | -3.225700 | 1.013350 | -2.493900 | 18.327800 | 6.040800 | 11.338500 | 8.639600 | 3.664300 | -16.188350 | 6.987800 | 12.677050 | 10.581900 | 10.959100 | -0.104700 | 14.422700 | 18.564200 | 1.768900 | -0.769500 | -6.550400 | 13.356950 | 22.304000 | 1.566700 | 11.509200 | 4.224300 | 8.605350 | 17.605100 | 14.227300 | 18.280600 | 5.376950 | 6.340400 | 3.395500 | 8.144100 | 3.085300 | 2.226400 | 2.445700 | 15.850600 | -5.201200 | 3.045500 | 24.366500 | 11.231100 | 1.190200 | 7.158100 | 4.329550 | 12.485600 | 13.171700 | 0.916400 | -1.102900 | 15.392150 | 12.266000 | 0.559500 | 7.812050 | 6.809100 | -4.667800 | -4.148100 | 22.975200 | 17.379900 | 1.243750 | 8.035900 | 3.554150 | 2.981700 | 11.859350 | 12.345200 | 8.652600 | 4.922950 | 10.406750 | -3.126700 | 3.996300 | 5.211200 | 16.738300 | 10.153400 | 7.672500 | 16.639450 | 6.985600 | -1.995000 | 13.184600 | -4.897700 | 17.662300 | 10.255000 | 23.734400 | 5.636600 | 5.359800 | 10.820600 | -2.618500 | 19.290300 | 2.961100 | -4.080550 | 4.749300 | 5.632800 | 0.008000 | -0.782800 | 19.723750 | -0.564750 | 20.287200 | 11.668400 | -2.515400 | 11.707650 | -0.868300 | 2.496500 | -2.671600 | 10.027200 | 0.675100 | 8.602300 | 12.493350 | -3.927300 | 8.912850 | -10.166800 | 15.211000 | 0.759700 | 3.162400 | 7.379000 | 1.892600 | 3.428500 | 17.977600 | -0.162000 | 2.403600 | 8.892800 | 15.943400 | -2.725950 | . 75% 12.739600 | 1.343400 | 12.495025 | 8.327600 | 12.253400 | 0.942575 | 6.005800 | 19.094550 | 2.930025 | 8.584825 | 4.362400 | 0.832525 | 14.162900 | 12.270900 | 9.271125 | 14.875600 | 11.035500 | -0.81160 | 21.014500 | 18.441950 | 17.914200 | 23.031600 | 6.278200 | 3.405700 | 13.467500 | 13.862800 | -0.000700 | 0.026200 | 6.092200 | 6.793425 | -1.464000 | 11.890900 | 1.460225 | 18.084425 | 11.843100 | 7.513375 | 4.485600 | 7.556750 | 13.615200 | 3.654800 | -0.899000 | 13.724125 | 11.752900 | 11.802400 | 13.069425 | 3.094500 | 13.292825 | -4.304400 | 22.970150 | 22.519425 | 13.237500 | 19.343250 | 0.958925 | 6.546925 | 7.415400 | 17.101150 | 19.266025 | 6.850900 | 6.212600 | 9.565900 | 15.108750 | -3.228375 | 2.243300 | 3.023450 | 7.486400 | 3.720800 | 6.586025 | 17.007125 | 5.024000 | -0.501075 | 33.598525 | 0.865600 | 3.517900 | 25.216200 | 29.471025 | 21.478875 | 11.901925 | 22.045300 | 6.868200 | 15.388800 | 11.432875 | 16.334300 | 3.086950 | 7.441350 | 2.022700 | 21.408675 | 11.171350 | 15.431200 | 10.612150 | 6.144700 | -6.207550 | 7.102500 | 15.844900 | 10.944800 | 13.067800 | 0.323625 | 20.890400 | 28.182100 | 2.257700 | 0.539425 | 0.042400 | 17.235150 | 28.720150 | 1.706100 | 12.908300 | 4.821700 | 9.926125 | 23.374000 | 14.363500 | 21.861700 | 8.094200 | 7.077325 | 4.575100 | 11.636025 | 3.815800 | 4.095625 | 3.652600 | 25.718700 | 0.914700 | 6.124100 | 33.061100 | 12.615025 | 5.067600 | 11.665600 | 6.184200 | 12.716725 | 13.817400 | 3.282500 | 1.363400 | 18.469000 | 12.873700 | 0.904400 | 8.914600 | 7.072400 | -0.142700 | 1.075900 | 31.068500 | 24.426300 | 4.420325 | 13.246525 | 7.057325 | 8.187300 | 16.083900 | 14.460925 | 9.312600 | 7.691900 | 12.127900 | 2.057600 | 4.131500 | 12.641350 | 18.678325 | 13.079775 | 9.833200 | 18.252250 | 10.775900 | 1.892750 | 13.925600 | -0.995600 | 23.871200 | 13.135000 | 32.495275 | 5.794100 | 6.367900 | 14.645800 | 1.330300 | 23.040250 | 3.241600 | 1.313125 | 7.004400 | 5.898900 | 3.070100 | 2.982900 | 25.849600 | 3.652625 | 25.720000 | 13.748500 | 2.737700 | 13.902500 | 5.423900 | 4.384725 | 1.024600 | 11.002000 | 7.474700 | 12.126700 | 19.437600 | -0.626300 | 11.227100 | -1.438800 | 18.322925 | 1.495400 | 6.336475 | 9.531100 | 2.956000 | 6.174200 | 20.391725 | 0.837900 | 6.519800 | 9.595900 | 18.045200 | 4.935400 | . max 22.323400 | 9.385100 | 18.714100 | 13.142000 | 16.037100 | 17.253700 | 8.302500 | 28.292800 | 9.665500 | 11.003600 | 20.214500 | 16.771300 | 14.682000 | 21.605100 | 14.723100 | 15.798000 | 17.368700 | 19.15090 | 38.929000 | 35.432300 | 32.075800 | 47.417900 | 14.042600 | 5.024600 | 23.839600 | 14.596400 | 13.456400 | 3.371300 | 8.459900 | 12.953200 | 14.391500 | 19.471900 | 6.949600 | 29.247500 | 13.225100 | 22.318300 | 13.094100 | 12.014900 | 27.142700 | 14.167300 | 17.071700 | 26.725500 | 13.389500 | 12.564700 | 29.136300 | 58.307800 | 21.167500 | 20.496900 | 52.270300 | 42.040600 | 15.205500 | 40.689000 | 17.268300 | 8.083900 | 29.590300 | 29.802300 | 27.931800 | 9.179300 | 21.438000 | 11.791600 | 24.394800 | 26.399400 | 7.383200 | 11.048900 | 11.117200 | 15.488400 | 9.897200 | 41.514700 | 5.046500 | 7.388400 | 63.982700 | 1.516500 | 15.613300 | 43.798100 | 84.684100 | 37.947800 | 34.349100 | 30.967800 | 11.571200 | 18.127800 | 28.180700 | 23.437300 | 21.294900 | 25.775800 | 17.403000 | 30.890100 | 33.844500 | 32.776100 | 17.268600 | 16.032100 | 22.337500 | 7.501400 | 28.443500 | 12.461000 | 18.975000 | 1.926200 | 42.425700 | 57.646200 | 4.572800 | 5.014400 | 24.073200 | 29.156500 | 51.984500 | 2.239700 | 19.473600 | 7.256800 | 15.274300 | 46.566600 | 14.743000 | 32.173400 | 18.093300 | 9.728200 | 7.925400 | 21.974800 | 6.265000 | 11.361700 | 7.439200 | 63.754500 | 24.648900 | 15.008400 | 68.466100 | 17.071800 | 17.320600 | 27.682800 | 14.198300 | 13.600400 | 15.466600 | 10.161500 | 8.930200 | 30.778100 | 14.845900 | 2.019900 | 12.554900 | 8.188700 | 11.694900 | 19.725700 | 54.014500 | 44.296600 | 18.818700 | 37.730400 | 19.888700 | 22.293600 | 32.977300 | 22.561600 | 11.629600 | 16.432600 | 19.908400 | 22.011800 | 4.822800 | 46.864400 | 24.665100 | 22.148800 | 18.372700 | 23.768900 | 22.483800 | 16.821900 | 16.346800 | 13.189700 | 46.157700 | 26.495900 | 64.291100 | 6.343700 | 10.194200 | 27.150300 | 11.885500 | 37.026700 | 4.216200 | 20.524400 | 14.983600 | 6.936400 | 16.846500 | 17.269200 | 53.426500 | 19.237600 | 42.758200 | 19.892200 | 19.677300 | 20.007800 | 27.956800 | 14.067500 | 13.991000 | 14.055900 | 28.255300 | 25.568500 | 44.363400 | 12.488600 | 21.699900 | 23.569900 | 28.885200 | 3.780300 | 20.359000 | 16.716500 | 8.005000 | 17.632600 | 27.947800 | 4.545400 | 15.920700 | 12.275800 | 26.538400 | 27.907400 | . Observations . A quick look at the basic statistics and comparing the training and test dataset doesn&#39;t reveal too much. | Both the train and test datasets look quite similar in the mean, std and median values. | We can confirm this by looking at the frequency distribution of the data along all features in both the datasets. | . Frequency Distribution Plots . 1. Comparing train and test . We can visualize how each feature is distributed in both the train and test datasets and compare if there are any differences. The train dataset should ideally be representative of the test dataset. . %%time fig, ax = plt.subplots(20, 10, figsize = (25, 50), constrained_layout = True) for i, col in tqdm(enumerate(X.columns, start = 1)): plt.subplot(20, 10, i) sns.kdeplot(X[col]) sns.kdeplot(test_df[col]) plt.xlabel(col) plt.ylabel(&quot;&quot;) plt.legend([&quot;train&quot;, &quot;test&quot;]) . 200it [07:08, 2.14s/it] . Wall time: 7min 10s . Observations . All variables are distributed similarly among the train and test datasets. | The train dataset is representative of the test dataset. | . 2. Comparing target classes . We can also see if the data is distributed differently between the two target classes. . %%time # divide the dataset with respect to target class. t0 = X.loc[y == 0] t1 = X.loc[y == 1] # plot fig, ax = plt.subplots(20, 10, figsize = (25, 50), constrained_layout = True) for i, col in tqdm(enumerate(X.columns, start = 1)): plt.subplot(20, 10, i) sns.kdeplot(t0[col]) sns.kdeplot(t1[col]) plt.xlabel(col) plt.ylabel(&quot;&quot;) plt.legend([0, 1]) . 200it [04:28, 1.34s/it] . Wall time: 4min 31s . Observations . There do appear to some differences in the distributions of features betweeen both target clases. The ML algorithms we&#39;ll train will try to learn from these differences and also find more patterns which will help them classify and differentiate between the two target classes better. . Target class . We&#39;ll look at the distribution of the target class now. . # distribution target_vc = y.value_counts()/len(y) target_vc . 0 0.89951 1 0.10049 Name: target, dtype: float64 . sns.barplot(x = target_vc.index, y = target_vc) . &lt;AxesSubplot:ylabel=&#39;target&#39;&gt; . Observations . The target class is imbalanced. | It is distributed with about 9:1 ratio. | For training some ML models, it would be better to oversample the data. | . Null values . # null values count X.isnull().sum().sort_values(ascending = False) . var_0 0 var_137 0 var_127 0 var_128 0 var_129 0 .. var_69 0 var_70 0 var_71 0 var_72 0 var_199 0 Length: 200, dtype: int64 . Observations . There are no null values in the data, and thus it doesn&#39;t need any handling/preparation. | . Feature transformation . # Scaling and Oversampling scaler = MinMaxScaler() sm = SMOTE(random_state = seed) . Separate validation data . This validation data will help us in evaluating the performance of trained ML models. . # make train_test_split X_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify = y, random_state = seed) . Train Baseline model and evaluate results . We&#39;ll train a dummy classifier to establish a baseline score. This will give us a scoer value, which our future models should at least beat. It helps to identify errors in training. . # define base model and its pipeline base_model = DummyClassifier() base_pipe = make_imb_pipeline(scaler, sm, base_model) . # fit the base pipline base_pipe.fit(X_train, y_train) . Pipeline(steps=[(&#39;minmaxscaler&#39;, MinMaxScaler()), (&#39;smote&#39;, SMOTE(random_state=42)), (&#39;dummyclassifier&#39;, DummyClassifier())]) . # function to evaluate the model def evaluate_model(model_pipe, plot_graph = False): preds = model_pipe.predict(X_valid) try: preds_prob = model_pipe.predict_proba(X_valid)[:, 1] except: preds_prob = model_pipe.decision_function(X_valid) res = { &quot;Accuracy Score&quot;: accuracy_score(y_valid, preds), &quot;Precision Score&quot;: precision_score(y_valid, preds, zero_division = 0), &quot;Recall Score&quot;: recall_score(y_valid, preds), &quot;ROC_AUC Score&quot;: roc_auc_score(y_valid, preds_prob), &quot;f1 Score&quot;: f1_score(y_valid, preds) } print(res) if plot_graph: plt.figure(1) ConfusionMatrixDisplay.from_predictions(y_valid, preds) plt.title(&quot;Confusion Matrix&quot;) plt.figure(2) RocCurveDisplay.from_predictions(y_valid, preds_prob) plt.title(&quot;Roc Curve&quot;) return res . base_scores = evaluate_model(base_pipe, True) . {&#39;Accuracy Score&#39;: 0.89952, &#39;Precision Score&#39;: 0.0, &#39;Recall Score&#39;: 0.0, &#39;ROC_AUC Score&#39;: 0.5, &#39;f1 Score&#39;: 0.0} . &lt;Figure size 432x288 with 0 Axes&gt; . As expected, dummy classifier achieved accuracy of 0.89 because the class is imbalanced and the classifier predicted the majority class for all predictions. And as expected, it has no discrimative ability. This is reflected in its ROC_AUC score of 0.5. The other ML Algorithms that we&#39;ll train should at least beat this score. . Make submission . # predicions dummy_preds = base_pipe.predict(test_df) . # add predictions to submission file submission_df[&quot;target&quot;] = dummy_preds submission_df.head() . ID_code target . 0 test_0 | 0 | . 1 test_1 | 0 | . 2 test_2 | 0 | . 3 test_3 | 0 | . 4 test_4 | 0 | . # save submission file submission_df.to_csv(&quot;baseline_model_preds.csv&quot;, index = None) . On Kaggle, this submission gives the score of 0.5, as expected. . Model Seletion . We&#39;ll now train multiple models for classification and compare their performance. We&#39;ll also compare the effects of scaling and oversampling on performance. From these trained models, we can then choose one model and then try to improve the scores through feature engineering and tuning hyperpararmeters on that ML model. . # Define the models models = {&quot;LogisticRegression&quot;: LogisticRegression(random_state = seed), &quot;RidgeClassification&quot;: RidgeClassifier(random_state = seed), &quot;GaussianNB&quot;: GaussianNB(), &quot;RandomForestClassifier&quot;: RandomForestClassifier(n_estimators = 100, max_depth = 7, n_jobs = -1, random_state = seed), &quot;LGBMClassifier&quot;: lgb.LGBMClassifier(max_depth = 7, learning_rate = 0.05, n_estimators = 300, random_state = seed)} . # models with no scaling and oversampling model_scores = {} for model_name, model in models.items(): model.fit(X_train, y_train) print(model_name, &quot; n&quot;) model_scores[model_name] = evaluate_model(model) print(&quot; n- n n&quot;) . LogisticRegression {&#39;Accuracy Score&#39;: 0.91098, &#39;Precision Score&#39;: 0.6684303350970018, &#39;Recall Score&#39;: 0.22631369426751594, &#39;ROC_AUC Score&#39;: 0.8465539016420931, &#39;f1 Score&#39;: 0.3381412639405205} - RidgeClassification {&#39;Accuracy Score&#39;: 0.90196, &#39;Precision Score&#39;: 0.9621212121212122, &#39;Recall Score&#39;: 0.025278662420382167, &#39;ROC_AUC Score&#39;: 0.8589495475081402, &#39;f1 Score&#39;: 0.04926299456943367} - GaussianNB {&#39;Accuracy Score&#39;: 0.91994, &#39;Precision Score&#39;: 0.7046092184368737, &#39;Recall Score&#39;: 0.3499203821656051, &#39;ROC_AUC Score&#39;: 0.8872843205689885, &#39;f1 Score&#39;: 0.4676153743848916} - RandomForestClassifier {&#39;Accuracy Score&#39;: 0.89952, &#39;Precision Score&#39;: 0.0, &#39;Recall Score&#39;: 0.0, &#39;ROC_AUC Score&#39;: 0.7883268347329475, &#39;f1 Score&#39;: 0.0} - LGBMClassifier {&#39;Accuracy Score&#39;: 0.9112, &#39;Precision Score&#39;: 0.8387470997679815, &#39;Recall Score&#39;: 0.1439092356687898, &#39;ROC_AUC Score&#39;: 0.8771059621748726, &#39;f1 Score&#39;: 0.2456676860346585} - . # models with only scaling model_scores = {} for model_name, model in models.items(): model_pipe = make_imb_pipeline(scaler, model) model_pipe.fit(X_train, y_train) print(model_name, &quot; n&quot;) model_scores[model_name] = evaluate_model(model_pipe) print(&quot; n- n n&quot;) . LogisticRegression {&#39;Accuracy Score&#39;: 0.91322, &#39;Precision Score&#39;: 0.674655787863335, &#39;Recall Score&#39;: 0.2633359872611465, &#39;ROC_AUC Score&#39;: 0.859026871125322, &#39;f1 Score&#39;: 0.3788117394416607} - RidgeClassification {&#39;Accuracy Score&#39;: 0.90194, &#39;Precision Score&#39;: 0.9618320610687023, &#39;Recall Score&#39;: 0.025079617834394906, &#39;ROC_AUC Score&#39;: 0.8589494811245403, &#39;f1 Score&#39;: 0.04888457807953443} - GaussianNB {&#39;Accuracy Score&#39;: 0.91994, &#39;Precision Score&#39;: 0.7046092184368737, &#39;Recall Score&#39;: 0.3499203821656051, &#39;ROC_AUC Score&#39;: 0.8872830327271504, &#39;f1 Score&#39;: 0.4676153743848916} - RandomForestClassifier {&#39;Accuracy Score&#39;: 0.89952, &#39;Precision Score&#39;: 0.0, &#39;Recall Score&#39;: 0.0, &#39;ROC_AUC Score&#39;: 0.7883291271799312, &#39;f1 Score&#39;: 0.0} - LGBMClassifier {&#39;Accuracy Score&#39;: 0.9114, &#39;Precision Score&#39;: 0.8453488372093023, &#39;Recall Score&#39;: 0.14470541401273884, &#39;ROC_AUC Score&#39;: 0.8771115560995588, &#39;f1 Score&#39;: 0.2471108089734874} - . # models with data scaled and oversampled model_scores = {} for model_name, model in models.items(): model_pipe = make_imb_pipeline(scaler, sm, model) model_pipe.fit(X_train, y_train) print(model_name, &quot; n&quot;) model_scores[model_name] = evaluate_model(model_pipe) print(&quot; n- n n&quot;) . LogisticRegression {&#39;Accuracy Score&#39;: 0.7887, &#39;Precision Score&#39;: 0.2902566431978197, &#39;Recall Score&#39;: 0.7631369426751592, &#39;ROC_AUC Score&#39;: 0.8575722869606891, &#39;f1 Score&#39;: 0.4205561344814348} - RidgeClassification {&#39;Accuracy Score&#39;: 0.78284, &#39;Precision Score&#39;: 0.28573527251358893, &#39;Recall Score&#39;: 0.7742834394904459, &#39;ROC_AUC Score&#39;: 0.8576126393382912, &#39;f1 Score&#39;: 0.41742676252816824} - GaussianNB {&#39;Accuracy Score&#39;: 0.8685, &#39;Precision Score&#39;: 0.1988349514563107, &#39;Recall Score&#39;: 0.10191082802547771, &#39;ROC_AUC Score&#39;: 0.6057410599524276, &#39;f1 Score&#39;: 0.1347545729701277} - RandomForestClassifier {&#39;Accuracy Score&#39;: 0.73772, &#39;Precision Score&#39;: 0.1695801339650384, &#39;Recall Score&#39;: 0.41321656050955413, &#39;ROC_AUC Score&#39;: 0.6527922154731639, &#39;f1 Score&#39;: 0.2404726051198888} - LGBMClassifier {&#39;Accuracy Score&#39;: 0.85454, &#39;Precision Score&#39;: 0.2912567291627993, &#39;Recall Score&#39;: 0.31230095541401276, &#39;ROC_AUC Score&#39;: 0.7348220492896991, &#39;f1 Score&#39;: 0.30141196811065224} - . Observations . Oversampling doesn&#39;t seem to help in performance and is actually hurting the performance in most models. Thus it would be better to avoid it. Scaling does help in Logistic Regression a little, but it has no effect on other models.GaussianNB and LGBClassifier perform the best without any scaling and oversampling. Although GaussianNB performs the best here, LGBClassifier is only a little behind. A quick look at the other kernels on Kaggle and the opportunity to improve LGBClassifier with more boosting rounds suggests that we should opt this model for further improvement on scores. . Feature Engineering . This wonderful kernel showed that there is synthetic data in the test dataset and half of it is used to evaluate the submission file. It also gives the indices of test dataset which are using for the public LB and the private LB. The important fact that is realized in this kernel is that the count of unique values for every feature is somehow important. This exact knowledge will be used for feature engineering. . The data from that kernel has already been added to the project data directory and will now be imported. . # file paths for dirname, _, filenames in os.walk(os.getcwd()): for filename in filenames: print(os.path.join(dirname, filename)) . C: Users ncits Downloads Santader_Transactions_Predictions baseline_model_preds.csv C: Users ncits Downloads Santader_Transactions_Predictions final_submssion.csv C: Users ncits Downloads Santader_Transactions_Predictions private_LB.npy C: Users ncits Downloads Santader_Transactions_Predictions public_LB.npy C: Users ncits Downloads Santader_Transactions_Predictions sample_submission.csv C: Users ncits Downloads Santader_Transactions_Predictions synthetic_samples_indexes.npy C: Users ncits Downloads Santader_Transactions_Predictions test.csv C: Users ncits Downloads Santader_Transactions_Predictions train.csv C: Users ncits Downloads Santader_Transactions_Predictions .ipynb_checkpoints Untitled-checkpoint.ipynb . synthetic_samples_indices = np.load(r&quot;C: Users ncits Downloads Santader_Transactions_Predictions synthetic_samples_indexes.npy&quot;) public_lb = np.load(r&quot;C: Users ncits Downloads Santader_Transactions_Predictions public_LB.npy&quot;) private_lb = np.load(r&quot;C: Users ncits Downloads Santader_Transactions_Predictions private_LB.npy&quot;) . # merge the train dataset with the real data from the public LB and the private LB into a new dataset full = pd.concat([X, pd.concat([test_df.iloc[public_lb], test_df.iloc[private_lb]])]) full . var_0 var_1 var_2 var_3 var_4 var_5 var_6 var_7 var_8 var_9 var_10 var_11 var_12 var_13 var_14 var_15 var_16 var_17 var_18 var_19 var_20 var_21 var_22 var_23 var_24 var_25 var_26 var_27 var_28 var_29 var_30 var_31 var_32 var_33 var_34 var_35 var_36 var_37 var_38 var_39 var_40 var_41 var_42 var_43 var_44 var_45 var_46 var_47 var_48 var_49 var_50 var_51 var_52 var_53 var_54 var_55 var_56 var_57 var_58 var_59 var_60 var_61 var_62 var_63 var_64 var_65 var_66 var_67 var_68 var_69 var_70 var_71 var_72 var_73 var_74 var_75 var_76 var_77 var_78 var_79 var_80 var_81 var_82 var_83 var_84 var_85 var_86 var_87 var_88 var_89 var_90 var_91 var_92 var_93 var_94 var_95 var_96 var_97 var_98 var_99 var_100 var_101 var_102 var_103 var_104 var_105 var_106 var_107 var_108 var_109 var_110 var_111 var_112 var_113 var_114 var_115 var_116 var_117 var_118 var_119 var_120 var_121 var_122 var_123 var_124 var_125 var_126 var_127 var_128 var_129 var_130 var_131 var_132 var_133 var_134 var_135 var_136 var_137 var_138 var_139 var_140 var_141 var_142 var_143 var_144 var_145 var_146 var_147 var_148 var_149 var_150 var_151 var_152 var_153 var_154 var_155 var_156 var_157 var_158 var_159 var_160 var_161 var_162 var_163 var_164 var_165 var_166 var_167 var_168 var_169 var_170 var_171 var_172 var_173 var_174 var_175 var_176 var_177 var_178 var_179 var_180 var_181 var_182 var_183 var_184 var_185 var_186 var_187 var_188 var_189 var_190 var_191 var_192 var_193 var_194 var_195 var_196 var_197 var_198 var_199 . ID_code . train_0 8.9255 | -6.7863 | 11.9081 | 5.0930 | 11.4607 | -9.2834 | 5.1187 | 18.6266 | -4.9200 | 5.7470 | 2.9252 | 3.1821 | 14.0137 | 0.5745 | 8.7989 | 14.5691 | 5.7487 | -7.2393 | 4.2840 | 30.7133 | 10.5350 | 16.2191 | 2.5791 | 2.4716 | 14.3831 | 13.4325 | -5.1488 | -0.4073 | 4.9306 | 5.9965 | -0.3085 | 12.9041 | -3.8766 | 16.8911 | 11.1920 | 10.5785 | 0.6764 | 7.8871 | 4.6667 | 3.8743 | -5.2387 | 7.3746 | 11.5767 | 12.0446 | 11.6418 | -7.0170 | 5.9226 | -14.2136 | 16.0283 | 5.3253 | 12.9194 | 29.0460 | -0.6940 | 5.1736 | -0.7474 | 14.8322 | 11.2668 | 5.3822 | 2.0183 | 10.1166 | 16.1828 | 4.9590 | 2.0771 | -0.2154 | 8.6748 | 9.5319 | 5.8056 | 22.4321 | 5.0109 | -4.7010 | 21.6374 | 0.5663 | 5.1999 | 8.8600 | 43.1127 | 18.3816 | -2.3440 | 23.4104 | 6.5199 | 12.1983 | 13.6468 | 13.8372 | 1.3675 | 2.9423 | -4.5213 | 21.4669 | 9.3225 | 16.4597 | 7.9984 | -1.7069 | -21.4494 | 6.7806 | 11.0924 | 9.9913 | 14.8421 | 0.1812 | 8.9642 | 16.2572 | 2.1743 | -3.4132 | 9.4763 | 13.3102 | 26.5376 | 1.4403 | 14.7100 | 6.0454 | 9.5426 | 17.1554 | 14.1104 | 24.3627 | 2.0323 | 6.7602 | 3.9141 | -0.4851 | 2.5240 | 1.5093 | 2.5516 | 15.5752 | -13.4221 | 7.2739 | 16.0094 | 9.7268 | 0.8897 | 0.7754 | 4.2218 | 12.0039 | 13.8571 | -0.7338 | -1.9245 | 15.4462 | 12.8287 | 0.3587 | 9.6508 | 6.5674 | 5.1726 | 3.1345 | 29.4547 | 31.4045 | 2.8279 | 15.6599 | 8.3307 | -5.6011 | 19.0614 | 11.2663 | 8.6989 | 8.3694 | 11.5659 | -16.4727 | 4.0288 | 17.9244 | 18.5177 | 10.7800 | 9.0056 | 16.6964 | 10.4838 | 1.6573 | 12.1749 | -13.1324 | 17.6054 | 11.5423 | 15.4576 | 5.3133 | 3.6159 | 5.0384 | 6.6760 | 12.6644 | 2.7004 | -0.6975 | 9.5981 | 5.4879 | -4.7645 | -8.4254 | 20.8773 | 3.1531 | 18.5618 | 7.7423 | -10.1245 | 13.7241 | -3.5189 | 1.7202 | -8.4051 | 9.0164 | 3.0657 | 14.3691 | 25.8398 | 5.8764 | 11.8411 | -19.7159 | 17.5743 | 0.5857 | 4.4354 | 3.9642 | 3.1364 | 1.6910 | 18.5227 | -2.3978 | 7.8784 | 8.5635 | 12.7803 | -1.0914 | . train_1 11.5006 | -4.1473 | 13.8588 | 5.3890 | 12.3622 | 7.0433 | 5.6208 | 16.5338 | 3.1468 | 8.0851 | -0.4032 | 8.0585 | 14.0239 | 8.4135 | 5.4345 | 13.7003 | 13.8275 | -15.5849 | 7.8000 | 28.5708 | 3.4287 | 2.7407 | 8.5524 | 3.3716 | 6.9779 | 13.8910 | -11.7684 | -2.5586 | 5.0464 | 0.5481 | -9.2987 | 7.8755 | 1.2859 | 19.3710 | 11.3702 | 0.7399 | 2.7995 | 5.8434 | 10.8160 | 3.6783 | -11.1147 | 1.8730 | 9.8775 | 11.7842 | 1.2444 | -47.3797 | 7.3718 | 0.1948 | 34.4014 | 25.7037 | 11.8343 | 13.2256 | -4.1083 | 6.6885 | -8.0946 | 18.5995 | 19.3219 | 7.0118 | 1.9210 | 8.8682 | 8.0109 | -7.2417 | 1.7944 | -1.3147 | 8.1042 | 1.5365 | 5.4007 | 7.9344 | 5.0220 | 2.2302 | 40.5632 | 0.5134 | 3.1701 | 20.1068 | 7.7841 | 7.0529 | 3.2709 | 23.4822 | 5.5075 | 13.7814 | 2.5462 | 18.1782 | 0.3683 | -4.8210 | -5.4850 | 13.7867 | -13.5901 | 11.0993 | 7.9022 | 12.2301 | 0.4768 | 6.8852 | 8.0905 | 10.9631 | 11.7569 | -1.2722 | 24.7876 | 26.6881 | 1.8944 | 0.6939 | -13.6950 | 8.4068 | 35.4734 | 1.7093 | 15.1866 | 2.6227 | 7.3412 | 32.0888 | 13.9550 | 13.0858 | 6.6203 | 7.1051 | 5.3523 | 8.5426 | 3.6159 | 4.1569 | 3.0454 | 7.8522 | -11.5100 | 7.5109 | 31.5899 | 9.5018 | 8.2736 | 10.1633 | 0.1225 | 12.5942 | 14.5697 | 2.4354 | 0.8194 | 16.5346 | 12.4205 | -0.1780 | 5.7582 | 7.0513 | 1.9568 | -8.9921 | 9.7797 | 18.1577 | -1.9721 | 16.1622 | 3.6937 | 6.6803 | -0.3243 | 12.2806 | 8.6086 | 11.0738 | 8.9231 | 11.7700 | 4.2578 | -4.4223 | 20.6294 | 14.8743 | 9.4317 | 16.7242 | -0.5687 | 0.1898 | 12.2419 | -9.6953 | 22.3949 | 10.6261 | 29.4846 | 5.8683 | 3.8208 | 15.8348 | -5.0121 | 15.1345 | 3.2003 | 9.3192 | 3.8821 | 5.7999 | 5.5378 | 5.0988 | 22.0330 | 5.5134 | 30.2645 | 10.4968 | -7.2352 | 16.5721 | -7.3477 | 11.0752 | -5.5937 | 9.4878 | -14.9100 | 9.4245 | 22.5441 | -4.8622 | 7.6543 | -15.9319 | 13.3175 | -0.3566 | 7.6421 | 7.7214 | 2.5837 | 10.9516 | 15.4305 | 2.0339 | 8.1267 | 8.7889 | 18.3560 | 1.9518 | . train_2 8.6093 | -2.7457 | 12.0805 | 7.8928 | 10.5825 | -9.0837 | 6.9427 | 14.6155 | -4.9193 | 5.9525 | -0.3249 | -11.2648 | 14.1929 | 7.3124 | 7.5244 | 14.6472 | 7.6782 | -1.7395 | 4.7011 | 20.4775 | 17.7559 | 18.1377 | 1.2145 | 3.5137 | 5.6777 | 13.2177 | -7.9940 | -2.9029 | 5.8463 | 6.1439 | -11.1025 | 12.4858 | -2.2871 | 19.0422 | 11.0449 | 4.1087 | 4.6974 | 6.9346 | 10.8917 | 0.9003 | -13.5174 | 2.2439 | 11.5283 | 12.0406 | 4.1006 | -7.9078 | 11.1405 | -5.7864 | 20.7477 | 6.8874 | 12.9143 | 19.5856 | 0.7268 | 6.4059 | 9.3124 | 6.2846 | 15.6372 | 5.8200 | 1.1000 | 9.1854 | 12.5963 | -10.3734 | 0.8748 | 5.8042 | 3.7163 | -1.1016 | 7.3667 | 9.8565 | 5.0228 | -5.7828 | 2.3612 | 0.8520 | 6.3577 | 12.1719 | 19.7312 | 19.4465 | 4.5048 | 23.2378 | 6.3191 | 12.8046 | 7.4729 | 15.7811 | 13.3529 | 10.1852 | 5.4604 | 19.0773 | -4.4577 | 9.5413 | 11.9052 | 2.1447 | -22.4038 | 7.0883 | 14.1613 | 10.5080 | 14.2621 | 0.2647 | 20.4031 | 17.0360 | 1.6981 | -0.0269 | -0.3939 | 12.6317 | 14.8863 | 1.3854 | 15.0284 | 3.9995 | 5.3683 | 8.6273 | 14.1963 | 20.3882 | 3.2304 | 5.7033 | 4.5255 | 2.1929 | 3.1290 | 2.9044 | 1.1696 | 28.7632 | -17.2738 | 2.1056 | 21.1613 | 8.9573 | 2.7768 | -2.1746 | 3.6932 | 12.4653 | 14.1978 | -2.5511 | -0.9479 | 17.1092 | 11.5419 | 0.0975 | 8.8186 | 6.6231 | 3.9358 | -11.7218 | 24.5437 | 15.5827 | 3.8212 | 8.6674 | 7.3834 | -2.4438 | 10.2158 | 7.4844 | 9.1104 | 4.3649 | 11.4934 | 1.7624 | 4.0714 | -1.2681 | 14.3330 | 8.0088 | 4.4015 | 14.1479 | -5.1747 | 0.5778 | 14.5362 | -1.7624 | 33.8820 | 11.6041 | 13.2070 | 5.8442 | 4.7086 | 5.7141 | -1.0410 | 20.5092 | 3.2790 | -5.5952 | 7.3176 | 5.7690 | -7.0927 | -3.9116 | 7.2569 | -5.8234 | 25.6820 | 10.9202 | -0.3104 | 8.8438 | -9.7009 | 2.4013 | -4.2935 | 9.3908 | -13.2648 | 3.1545 | 23.0866 | -5.3000 | 5.3745 | -6.2660 | 10.1934 | -0.8417 | 2.9057 | 9.7905 | 1.6704 | 1.6858 | 21.6042 | 3.1417 | -6.5213 | 8.2675 | 14.7222 | 0.3965 | . train_3 11.0604 | -2.1518 | 8.9522 | 7.1957 | 12.5846 | -1.8361 | 5.8428 | 14.9250 | -5.8609 | 8.2450 | 2.3061 | 2.8102 | 13.8463 | 11.9704 | 6.4569 | 14.8372 | 10.7430 | -0.4299 | 15.9426 | 13.7257 | 20.3010 | 12.5579 | 6.8202 | 2.7229 | 12.1354 | 13.7367 | 0.8135 | -0.9059 | 5.9070 | 2.8407 | -15.2398 | 10.4407 | -2.5731 | 6.1796 | 10.6093 | -5.9158 | 8.1723 | 2.8521 | 9.1738 | 0.6665 | -3.8294 | -1.0370 | 11.7770 | 11.2834 | 8.0485 | -24.6840 | 12.7404 | -35.1659 | 0.7613 | 8.3838 | 12.6832 | 9.5503 | 1.7895 | 5.2091 | 8.0913 | 12.3972 | 14.4698 | 6.5850 | 3.3164 | 9.4638 | 15.7820 | -25.0222 | 3.4418 | -4.3923 | 8.6464 | 6.3072 | 5.6221 | 23.6143 | 5.0220 | -3.9989 | 4.0462 | 0.2500 | 1.2516 | 24.4187 | 4.5290 | 15.4235 | 11.6875 | 23.6273 | 4.0806 | 15.2733 | 0.7839 | 10.5404 | 1.6212 | -5.2896 | 1.6027 | 17.9762 | -2.3174 | 15.6298 | 4.5474 | 7.5509 | -7.5866 | 7.0364 | 14.4027 | 10.7795 | 7.2887 | -1.0930 | 11.3596 | 18.1486 | 2.8344 | 1.9480 | -19.8592 | 22.5316 | 18.6129 | 1.3512 | 9.3291 | 4.2835 | 10.3907 | 7.0874 | 14.3256 | 14.4135 | 4.2827 | 6.9750 | 1.6480 | 11.6896 | 2.5762 | -2.5459 | 5.3446 | 38.1015 | 3.5732 | 5.0988 | 30.5644 | 11.3025 | 3.9618 | -8.2464 | 2.7038 | 12.3441 | 12.5431 | -1.3683 | 3.5974 | 13.9761 | 14.3003 | 1.0486 | 8.9500 | 7.1954 | -1.1984 | 1.9586 | 27.5609 | 24.6065 | -2.8233 | 8.9821 | 3.8873 | 15.9638 | 10.0142 | 7.8388 | 9.9718 | 2.9253 | 10.4994 | 4.1622 | 3.7613 | 2.3701 | 18.0984 | 17.1765 | 7.6508 | 18.2452 | 17.0336 | -10.9370 | 12.0500 | -1.2155 | 19.9750 | 12.3892 | 31.8833 | 5.9684 | 7.2084 | 3.8899 | -11.0882 | 17.2502 | 2.5881 | -2.7018 | 0.5641 | 5.3430 | -7.1541 | -6.1920 | 18.2366 | 11.7134 | 14.7483 | 8.1013 | 11.8771 | 13.9552 | -10.4701 | 5.6961 | -3.7546 | 8.4117 | 1.8986 | 7.2601 | -0.4639 | -0.0498 | 7.9336 | -12.8279 | 12.4124 | 1.8489 | 4.4666 | 4.7433 | 0.7178 | 1.4214 | 23.0347 | -1.2706 | -2.9275 | 10.2922 | 17.9697 | -8.9996 | . train_4 9.8369 | -1.4834 | 12.8746 | 6.6375 | 12.2772 | 2.4486 | 5.9405 | 19.2514 | 6.2654 | 7.6784 | -9.4458 | -12.1419 | 13.8481 | 7.8895 | 7.7894 | 15.0553 | 8.4871 | -3.0680 | 6.5263 | 11.3152 | 21.4246 | 18.9608 | 10.1102 | 2.7142 | 14.2080 | 13.5433 | 3.1736 | -3.3423 | 5.9015 | 7.9352 | -3.1582 | 9.4668 | -0.0083 | 19.3239 | 12.4057 | 0.6329 | 2.7922 | 5.8184 | 19.3038 | 1.4450 | -5.5963 | 14.0685 | 11.9171 | 11.5111 | 6.9087 | -65.4863 | 13.8657 | 0.0444 | -0.1346 | 14.4268 | 13.3273 | 10.4857 | -1.4367 | 5.7555 | -8.5414 | 14.1482 | 16.9840 | 6.1812 | 1.9548 | 9.2048 | 8.6591 | -27.7439 | -0.4952 | -1.7839 | 5.2670 | -4.3205 | 6.9860 | 1.6184 | 5.0301 | -3.2431 | 40.1236 | 0.7737 | -0.7264 | 4.5886 | -4.5346 | 23.3521 | 1.0273 | 19.1600 | 7.1734 | 14.3937 | 2.9598 | 13.3317 | -9.2587 | -6.7075 | 7.8984 | 14.5265 | 7.0799 | 20.1670 | 8.0053 | 3.7954 | -39.7997 | 7.0065 | 9.3627 | 10.4316 | 14.0553 | 0.0213 | 14.7246 | 35.2988 | 1.6844 | 0.6715 | -22.9264 | 12.3562 | 17.3410 | 1.6940 | 7.1179 | 5.1934 | 8.8230 | 10.6617 | 14.0837 | 28.2749 | -0.1937 | 5.9654 | 1.0719 | 7.9923 | 2.9138 | -3.6135 | 1.4684 | 25.6795 | 13.8224 | 4.7478 | 41.1037 | 12.7140 | 5.2964 | 9.7289 | 3.9370 | 12.1316 | 12.5815 | 7.0642 | 5.6518 | 10.9346 | 11.4266 | 0.9442 | 7.7532 | 6.6173 | -6.8304 | 6.4730 | 17.1728 | 25.8128 | 2.6791 | 13.9547 | 6.6289 | -4.3965 | 11.7159 | 16.1080 | 7.6874 | 9.1570 | 11.5670 | -12.7047 | 3.7574 | 9.9110 | 20.1461 | 1.2995 | 5.8493 | 19.8234 | 4.7022 | 10.6101 | 13.0021 | -12.6068 | 27.0846 | 8.0913 | 33.5107 | 5.6953 | 5.4663 | 18.2201 | 6.5769 | 21.2607 | 3.2304 | -1.7759 | 3.1283 | 5.5518 | 1.4493 | -2.6627 | 19.8056 | 2.3705 | 18.4685 | 16.3309 | -3.3456 | 13.5261 | 1.7189 | 5.1743 | -7.6938 | 9.7685 | 4.8910 | 12.2198 | 11.8503 | -7.8931 | 6.4209 | 5.9270 | 16.0201 | -0.2829 | -1.4905 | 9.5214 | -0.1508 | 9.1942 | 13.2876 | -1.5121 | 3.9267 | 9.5031 | 17.9974 | -8.8104 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . test_131051 6.4676 | -0.6751 | 8.2936 | 6.4750 | 10.3517 | 1.7992 | 6.0640 | 16.0825 | -2.4526 | 6.3020 | 11.3281 | -1.3814 | 14.1652 | 2.9898 | 4.5672 | 14.2940 | 7.1272 | -3.8645 | 19.1195 | 17.6041 | 21.6629 | 26.6998 | 6.2519 | 2.7735 | 7.2963 | 13.6628 | -11.9718 | 0.6629 | 6.2859 | 0.3822 | 4.2549 | 13.7082 | -2.8315 | 6.6608 | 12.4849 | -0.6445 | 3.9421 | 7.5283 | 12.9724 | 7.4984 | -13.1367 | 2.2516 | 12.5235 | 11.4311 | 15.6269 | -33.5031 | 7.7235 | -8.7433 | -10.0378 | 11.6555 | 12.0142 | -0.2932 | -0.2155 | 5.8820 | 11.5303 | 16.3143 | 21.9504 | 5.1339 | -2.4913 | 7.6735 | 14.7846 | -5.5707 | 0.4015 | 2.8647 | 7.3000 | 3.2443 | 7.0295 | 9.9881 | 5.0173 | -6.7424 | 2.3657 | 0.9425 | -0.6697 | 31.0763 | 21.4923 | 15.9743 | 7.0236 | 15.9122 | 4.4354 | 16.0806 | 6.6441 | 13.3110 | -13.1598 | -11.8077 | -18.8852 | 20.8884 | 12.2497 | 3.8526 | 8.3548 | 11.8627 | -15.7791 | 7.0773 | 13.2672 | 10.2773 | 8.6421 | -0.3864 | 7.7504 | 41.8491 | 1.6827 | -0.5183 | -11.5218 | 8.2739 | 23.3924 | 1.7795 | 14.9711 | 3.8828 | 7.2746 | 16.1618 | 14.0643 | 13.4608 | 8.1026 | 4.9293 | 5.8725 | 3.7971 | 1.6581 | 5.8520 | 2.0255 | 18.3802 | -23.5947 | 10.8909 | 26.5699 | 10.3014 | 1.7596 | 10.3124 | 5.2988 | 12.5212 | 12.2784 | -2.9135 | -4.8077 | 20.8228 | 10.8098 | 0.9398 | 5.1387 | 7.4697 | -1.8767 | -12.3929 | 17.2235 | 14.2443 | 1.0082 | 15.1984 | 0.8586 | 3.6226 | 11.7467 | 9.0578 | 9.3819 | 5.7547 | 12.3301 | 0.7659 | 3.6361 | 12.6739 | 13.8519 | 13.2884 | 11.6808 | 17.5711 | 7.6217 | -8.5340 | 12.5772 | -2.8817 | 22.1513 | 10.6107 | 22.6822 | 5.8186 | 7.7799 | 18.1606 | -5.5226 | 23.7968 | 3.6984 | -0.3227 | 2.4371 | 5.1285 | -2.3154 | -6.9206 | 17.3889 | 3.8711 | 12.0771 | 10.9589 | 5.6342 | 12.7486 | -1.7424 | 2.3116 | 7.4584 | 9.3447 | 19.5723 | 10.1140 | 20.1822 | 0.1209 | 11.4629 | -28.2552 | 19.4171 | 2.2255 | 0.1002 | 9.9519 | -0.6884 | 6.6460 | 19.1234 | -1.5598 | -2.8097 | 9.0289 | 18.2563 | -12.4072 | . test_131059 8.4813 | 3.1147 | 9.9969 | 9.2470 | 11.1361 | -2.1919 | 5.2232 | 17.4854 | -2.5667 | 6.6899 | 5.9598 | -5.2289 | 13.9081 | 13.7772 | 11.6033 | 14.4411 | 10.3478 | -6.3069 | 10.6149 | 8.0140 | 20.8811 | -2.7980 | 8.8075 | 3.5666 | 4.8129 | 13.6839 | 7.9705 | 0.4641 | 6.5098 | 5.6265 | -25.2886 | 10.8785 | 1.8876 | 19.0652 | 11.1831 | 2.8044 | -0.2012 | 6.2079 | 8.3930 | 3.1757 | 0.0542 | 9.9176 | 10.0010 | 11.4281 | 7.2591 | -12.4100 | 7.1068 | 0.7974 | 33.3185 | 25.5068 | 13.4549 | 23.6881 | -6.8979 | 5.6977 | 0.4225 | 19.1785 | 13.8189 | 5.4389 | 5.4396 | 7.9882 | 8.5549 | 9.1621 | 3.5599 | 6.1650 | 6.9362 | 2.3054 | 7.2683 | 17.9180 | 5.0174 | -6.6177 | 36.3947 | 0.3163 | 7.7810 | 16.3636 | 36.9809 | 25.6018 | 22.0670 | 15.4416 | 3.7327 | 14.9617 | 12.6117 | 14.7653 | -4.2003 | -6.5108 | 3.0251 | 17.5000 | 8.5830 | 6.8111 | 12.0136 | -1.8740 | -7.4069 | 7.1612 | 14.1059 | 11.0104 | 11.8350 | 0.5402 | 9.8477 | 47.3763 | 1.8608 | -2.6691 | -19.3323 | 18.5075 | 17.9046 | 1.4808 | 13.8692 | 3.4859 | 8.4436 | 11.6112 | 14.1922 | 21.5739 | 4.3684 | 9.0379 | 2.4136 | 6.3450 | 3.6251 | 7.9110 | 0.6643 | 14.5977 | -9.3238 | -1.7052 | 50.6796 | 12.9420 | 8.0891 | 3.0561 | 4.4203 | 12.6712 | 13.2752 | -2.5462 | 3.1256 | 16.4356 | 12.4657 | 0.8914 | 7.7370 | 7.1875 | -3.1910 | 4.3278 | 24.5657 | 30.5515 | -0.1621 | 19.0629 | 5.2945 | -0.0317 | 21.8289 | 13.0997 | 9.3507 | 0.7396 | 9.9363 | -5.6303 | 3.8166 | 11.4600 | 18.5876 | 2.1761 | 8.0577 | 14.3899 | 8.5478 | -3.8608 | 13.7062 | -8.1591 | 10.1942 | 9.6796 | 36.9286 | 5.3861 | 5.0618 | 6.1363 | -6.7802 | 22.8567 | 2.9525 | -15.8005 | 8.5602 | 4.9867 | 4.7021 | -3.8686 | 29.5134 | 2.8487 | 26.8952 | 11.5513 | 0.7991 | 13.0859 | -9.5414 | 0.5115 | 9.8042 | 9.6781 | 10.6630 | 9.1215 | 30.7039 | -1.1086 | 12.7785 | 12.5316 | 16.9558 | 0.3449 | 4.5032 | 6.8670 | 4.0856 | 3.8657 | 21.4049 | 0.4630 | 0.6766 | 8.6395 | 19.1666 | 11.9240 | . test_131066 17.1476 | -1.7580 | 9.0408 | 4.9697 | 12.3497 | -2.6610 | 4.9144 | 21.3001 | -2.0329 | 7.9769 | -7.9172 | -0.0365 | 14.2177 | 5.3723 | 8.2095 | 14.5701 | 5.2749 | -7.8550 | 20.2733 | 26.9390 | 14.8837 | 18.9319 | 1.3159 | 3.1540 | 6.6168 | 13.8299 | 0.3746 | -5.3681 | 5.7036 | 5.4731 | 6.9659 | 12.4996 | -0.4385 | 17.0244 | 12.2047 | 1.3484 | 0.7108 | 4.6492 | 8.7400 | -3.4173 | -19.0955 | 10.1491 | 12.2569 | 12.0181 | 7.4443 | -42.2688 | 12.7955 | -18.4056 | 18.2175 | 24.3498 | 13.0738 | 19.9123 | -0.5735 | 7.1556 | 17.8137 | 7.4439 | 15.1618 | 6.1296 | -1.8050 | 10.0322 | 10.5506 | -10.3003 | -2.7427 | 2.1599 | 4.6109 | 9.6146 | 5.4939 | 8.8724 | 5.0152 | -6.7305 | 16.2822 | 0.6807 | -4.4602 | 25.0129 | 18.9161 | 29.0850 | 7.0172 | 27.8048 | 2.3660 | 12.1447 | -3.5334 | 17.7416 | -0.4245 | 4.9783 | -11.8972 | 23.5959 | -4.1598 | 17.7982 | 9.4893 | 6.8621 | -38.1650 | 7.1488 | 9.5792 | 11.1319 | 12.1864 | -0.2319 | 15.4551 | 21.1336 | 1.0619 | 2.8736 | -21.9540 | 12.7619 | 27.5728 | 1.5587 | 13.0486 | 5.4359 | 8.8955 | 11.9490 | 14.3019 | 22.0889 | 8.1021 | 6.4279 | 1.3237 | 6.8962 | 3.7049 | -1.9316 | 3.3246 | 7.2147 | -6.7410 | -0.4083 | 17.4429 | 10.7978 | 5.1751 | -0.2867 | 2.5337 | 12.4948 | 14.1547 | 2.1254 | -4.6822 | 12.8528 | 10.9902 | 0.8533 | 6.8729 | 6.2429 | 3.5496 | 2.8385 | 37.5117 | 23.7626 | -8.1436 | 16.6858 | 3.3072 | 1.8837 | 4.0947 | 7.7933 | 9.9988 | 8.1783 | 13.3116 | 2.5664 | 3.8657 | 10.5833 | 14.1726 | 8.2687 | 7.2453 | 12.3911 | 10.7258 | 8.3014 | 13.7621 | -4.5784 | 21.7287 | 11.5826 | 11.6605 | 5.9014 | 4.6668 | 15.6401 | -12.0932 | 20.2850 | 2.6463 | -3.4234 | 1.2506 | 5.4782 | -2.9302 | -1.2197 | 22.2231 | -8.4721 | 34.9281 | 14.6835 | -5.5410 | 12.5028 | 9.0345 | 3.7606 | -2.9783 | 10.1719 | -11.5395 | 7.9889 | -2.7402 | -5.8177 | 7.1102 | 10.4245 | 18.1095 | 0.0710 | -4.7865 | 9.7313 | -0.2040 | 4.4193 | 18.6293 | 0.4939 | 2.9853 | 9.1823 | 22.3054 | -3.9768 | . test_131068 11.8592 | 5.9623 | 9.6131 | 7.8257 | 11.3450 | -13.0483 | 4.7687 | 19.7234 | 7.0627 | 7.0057 | 6.8098 | -5.1289 | 14.1632 | 11.5822 | 8.1099 | 14.8988 | 7.9353 | -15.0157 | 23.5460 | 19.0837 | 21.6578 | 18.5514 | 2.0605 | 2.2013 | 10.7035 | 13.5751 | -0.1525 | -0.7117 | 4.9148 | 4.5541 | 1.5445 | 13.5480 | -5.0413 | 9.6490 | 11.5276 | 11.0541 | 8.5084 | 6.8674 | 4.5066 | 3.6886 | 10.1572 | 3.8873 | 11.3149 | 11.7900 | 6.6631 | -44.7317 | 12.4323 | -33.6344 | 27.1009 | 7.5273 | 12.4311 | 1.8985 | 1.6427 | 7.1692 | -2.7789 | 15.7664 | 20.6419 | 6.4920 | 7.6287 | 9.6064 | 11.1338 | -5.2222 | 2.3550 | -3.0622 | 4.4360 | -5.0402 | 6.4659 | 3.9092 | 5.0348 | -11.0843 | 20.5578 | 0.3970 | -0.1217 | 6.6432 | 14.8794 | 15.7465 | 1.6692 | 17.1907 | 3.8452 | 13.8303 | 16.1960 | 13.9851 | -12.2722 | -10.1260 | 2.7476 | 17.1415 | 3.2802 | 14.9158 | 8.7803 | -0.9468 | 1.5280 | 6.7659 | 12.0910 | 9.8853 | 13.3212 | -1.0293 | 15.0316 | 5.4395 | 1.3333 | 1.5205 | 3.0958 | 10.2154 | 22.0886 | 1.7895 | 10.7978 | 5.4645 | 9.6934 | 0.8809 | 13.9915 | 21.0866 | 6.3316 | 6.8531 | 7.1170 | 5.2512 | 4.8697 | 1.9485 | 0.9979 | 39.2366 | -9.6428 | 6.8303 | 31.4263 | 9.3174 | -6.7042 | 0.5318 | 5.3938 | 12.5598 | 12.4643 | 3.4577 | -0.6334 | 15.0960 | 11.6978 | -0.6141 | 7.3610 | 7.2742 | 5.2248 | -3.5881 | 24.9923 | 19.2004 | 4.4551 | -6.0112 | -1.7854 | 0.2647 | 4.8186 | 14.3978 | 9.3242 | -0.1742 | 9.2957 | 0.8374 | 3.8922 | 2.1836 | 15.7742 | 4.9734 | 6.0724 | 19.0119 | 7.1925 | -6.5685 | 12.4716 | 2.9276 | 16.1010 | 11.4127 | 28.4897 | 5.7982 | 4.9428 | 23.4808 | -10.2529 | 20.2112 | 3.1077 | -3.0883 | 1.0345 | 5.3482 | -3.9167 | -12.3275 | 18.5421 | -0.2225 | 23.6320 | 6.1771 | 2.7007 | 12.5825 | 3.7566 | 3.6299 | -6.0186 | 9.0342 | 1.0227 | 10.3582 | -4.8846 | -1.4530 | 3.4663 | -19.1525 | 11.8059 | -0.6135 | -2.5308 | 7.4326 | 2.5299 | 9.1893 | 14.7174 | 1.6083 | 4.2321 | 8.0658 | 20.5306 | 5.5140 | . test_131071 8.2034 | -8.0700 | 12.9146 | 4.6771 | 9.5412 | -4.4721 | 5.6222 | 23.3273 | 3.7714 | 8.0598 | 2.3445 | 10.6460 | 14.1794 | 4.4355 | 8.5268 | 14.9357 | 8.1839 | -1.7573 | 30.2598 | 24.1162 | 12.5149 | 5.5738 | -1.4916 | 2.8164 | 6.4644 | 13.2357 | -5.4919 | -0.0643 | 5.6453 | 6.0481 | -19.3015 | 9.3845 | -3.8450 | 16.0137 | 11.4814 | 2.5956 | 1.8732 | 2.0693 | 7.0976 | -0.4648 | -8.1865 | 13.1152 | 11.6222 | 11.7830 | 0.0182 | -12.7629 | 14.4351 | -27.3811 | 15.7809 | 21.6270 | 12.1818 | 8.7560 | 3.7991 | 5.9564 | 11.6130 | 15.4324 | 15.7169 | 5.8218 | -1.0566 | 8.9456 | 15.4067 | -5.2110 | -1.3703 | 0.8467 | 4.5174 | 0.4181 | 4.2484 | 9.8887 | 5.0260 | -8.2359 | 16.3639 | 0.5370 | 1.8536 | 21.8880 | 31.4273 | 16.6845 | 6.4504 | 23.0138 | 8.1258 | 15.4581 | 3.1832 | 17.3645 | 9.8441 | -2.0266 | 2.9415 | 21.7955 | 9.4422 | 5.6769 | 6.7804 | 3.3566 | -6.9003 | 6.8944 | 9.3717 | 10.7856 | 11.2807 | -0.9129 | 21.5711 | 9.7624 | 0.6004 | -0.6183 | -5.6391 | 11.3413 | 34.6067 | 1.6308 | 10.9678 | 4.7667 | 10.9063 | 23.0681 | 14.2528 | 18.8450 | 9.0313 | 5.4689 | 4.9279 | 7.3150 | 2.7818 | -3.3044 | 3.4508 | 32.9176 | -11.1527 | -0.1159 | 47.3747 | 9.3028 | 0.9779 | 0.6206 | 7.6684 | 12.2409 | 13.7193 | 2.8372 | 1.7559 | 11.4343 | 12.3175 | 0.1347 | 8.3950 | 6.6922 | -14.3920 | -10.0356 | 7.7070 | 9.1436 | -6.6935 | 21.7355 | 3.1190 | -6.6898 | -0.4675 | 13.3535 | 8.3959 | -1.5484 | 11.5772 | -0.6959 | 3.9411 | 22.8115 | 14.7320 | 11.2564 | 10.6410 | 14.6840 | 7.3109 | -1.4962 | 11.1819 | -2.1142 | 16.5170 | 14.5130 | 9.5167 | 5.4935 | 5.8381 | -0.1552 | -1.1440 | 10.2765 | 3.5344 | -6.4611 | 4.7353 | 5.4278 | -2.2865 | 8.5304 | 15.4835 | -5.5625 | 24.5046 | 12.0405 | -3.5315 | 16.9220 | 8.5335 | 4.8674 | 4.2000 | 11.6058 | -3.2128 | 11.5689 | 5.7378 | -4.3803 | 5.8254 | -14.9304 | 10.3127 | 0.1279 | 11.5927 | 7.1993 | -1.0688 | 7.4700 | 14.2476 | 0.0007 | 10.8189 | 7.5399 | 12.7860 | -1.6035 | . 300000 rows × 200 columns . We will add a new column for each feature in the dataset. This new column will include information of the value counts of the values in each of those feature columns. This extra information should improve our score. . # add new columns for feature in X.columns: count_vals = full[feature].value_counts() X[&quot;new_&quot; + feature] = count_vals.loc[X[feature]].values test_df[&quot;new_&quot; + feature] = count_vals.loc[test_df[feature]].values . # check the new shape of both train data and test data X.shape, test_df.shape . ((200000, 400), (200000, 400)) . # make new train_test_split X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.25, stratify = y, random_state = seed) . Model Training . To train the LGBClassifier, this time we&#39;ll not use the Scikit Learn&#39;s API. Rather, we&#39;ll use the lightgbm API which will give us better control and flexibility on the hyperparameters. Also, high score on GaussianNB suggests that the features are pretty independent of each other (except the new columns which depend on the column which they are based on). Therefore, we can train the LGB model two features at a time, one being the original feature and the second being the value count feature that we have added through feature engineering. This stops the model from studying interelationships between all the 400 features, which can save a huge amount of training time. . # set training hyperparameters core_count = psutil.cpu_count(logical=False) param = {&#39;bagging_fraction&#39;: 0.8, &#39;bagging_freq&#39;: 2, &#39;lambda_l1&#39;: 0.7, &#39;lambda_l2&#39;: 2, &#39;learning_rate&#39;: 0.01, &#39;max_depth&#39;: 5, &#39;min_data_in_leaf&#39;: 22, &#39;min_gain_to_split&#39;: 0.07, &#39;min_sum_hessian_in_leaf&#39;: 15, &#39;num_leaves&#39;: 20, &#39;feature_fraction&#39;: 1, &#39;save_binary&#39;: True, &#39;seed&#39;: seed, &#39;feature_fraction_seed&#39;: seed, &#39;bagging_seed&#39;: seed, &#39;drop_seed&#39;: seed, &#39;data_random_seed&#39;: seed, &#39;objective&#39;: &#39;binary&#39;, &#39;boosting_type&#39;: &#39;gbdt&#39;, &#39;verbosity&#39;: -1, &#39;metric&#39;: &#39;auc&#39;, &#39;is_unbalance&#39;: True, &#39;boost_from_average&#39;: &#39;false&#39;, &#39;num_threads&#39;: core_count} . # prediction matrices valid_sub_preds = np.zeros([X_valid.shape[0], 200]) test_sub_preds = np.zeros([test_df.shape[0], 200]) # run training col by col for col in tqdm(range(200)): feature = X.columns[col] feature_set = [feature, &quot;new_&quot; + feature] # make lgbm datasets train_l = lgb.Dataset(X_train[feature_set], y_train) valid_l = lgb.Dataset(X_valid[feature_set], y_valid) # train model lgb_clf = lgb.train(param, train_l, num_boost_round = 50, valid_sets = [train_l, valid_l], verbose_eval = -1) # make predictions valid_sub_preds[:, col] = lgb_clf.predict(X_valid[feature_set], num_iteration = lgb_clf.best_iteration) test_sub_preds[:, col] = lgb_clf.predict(test_df[feature_set], num_iteration = lgb_clf.best_iteration) . 100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [01:35&lt;00:00, 2.09it/s] . Evaluate Performance . # validation set predictions val_full_preds = valid_sub_preds.sum(axis = 1) / 200 . def evaluate_perf(preds_prob): preds = (preds_prob &gt; 0.5).astype(int) res = { &quot;Accuracy Score&quot;: accuracy_score(y_valid, preds), &quot;Precision Score&quot;: precision_score(y_valid, preds, zero_division = 0), &quot;Recall Score&quot;: recall_score(y_valid, preds), &quot;ROC_AUC Score&quot;: roc_auc_score(y_valid, preds_prob), &quot;f1 Score&quot;: f1_score(y_valid, preds) } return res evaluate_perf(val_full_preds) . {&#39;Accuracy Score&#39;: 0.89192, &#39;Precision Score&#39;: 0.4756971092350985, &#39;Recall Score&#39;: 0.7402468152866242, &#39;ROC_AUC Score&#39;: 0.9152776960521903, &#39;f1 Score&#39;: 0.5791932720760006} . ROC_AUC Score has improved a lot considering that it was already high. Accuracy also looks good. This improvement can be attributed to the feature engineering we did. Now we can make a submssion. . Make Submission . # Make predictions on test dataset test_full_preds = test_sub_preds.sum(axis = 1) / 200 test_full_preds . array([0.49980011, 0.49991953, 0.50039014, ..., 0.49759414, 0.4996321 , 0.49941331]) . # save predictions and export it to csv file submission_df[&quot;target&quot;] = test_full_preds submission_df.to_csv(&quot;lgbm_first_training.csv&quot;, index = None) . Submitting this file on Kaglle gets us a score of 0.91093 which makes us stand in the top 3% of the leaderboard. Our standing can improve further with hyperparameter tuning and some other tweaks. . Add feature weights . Because each independent feature contributes differently to the variation in target class, we can add the feauture weights in the final computation the final predictions. The feature weights can ba calculated with the roc_auc score each feature gets and calculating the deviation from mean for each roc_auc score. . # calculate feature weights weights = [] for col in range(200): feature_roc_score = roc_auc_score(y_valid, valid_sub_preds[:, col]) if feature_roc_score &gt; 0.5: weights.append(feature_roc_score) else: weights.append(0) weights[:30] . [0.543336515143533, 0.5473523268496205, 0.5464708831971531, 0.5074337837752676, 0.5024420645540325, 0.5266766700555937, 0.5584090641866745, 0.50110258512608, 0.5156600970092755, 0.5397526747988171, 0.5029713874646804, 0.5129343952478831, 0.5598962449116529, 0.5563167593310913, 0.5052252058316452, 0.5078600970411395, 0.5009789965653303, 0, 0.5380105522839358, 0.5083492822144917, 0.5205073101974274, 0.5587881233933398, 0.5545546287991954, 0.5214425223530398, 0.5334865630565602, 0.5012005053615289, 0.5557101769740748, 0.5005401301607142, 0.5289215974457432, 0] . # transform weights to usable form weights = np.array(weights) weights = 1 + ((weights - weights.mean()) / weights.mean()) weights[:30] . array([1.12208421, 1.13037756, 1.12855722, 1.04793884, 1.03763007, 1.08767874, 1.15321164, 1.03486381, 1.0649276 , 1.11468296, 1.03872321, 1.05929855, 1.15628292, 1.14889067, 1.04337774, 1.04881925, 1.03460858, 0. , 1.11108517, 1.0498295 , 1.07493794, 1.15399446, 1.14525156, 1.07686931, 1.10174235, 1.03506603, 1.14763797, 1.03370224, 1.0923149 , 0. ]) . Evaluate Performance . weighted_valid_preds = (valid_sub_preds * weights).sum(axis = 1) / 200 evaluate_perf(weighted_valid_preds) . {&#39;Accuracy Score&#39;: 0.8914, &#39;Precision Score&#39;: 0.47449748743718595, &#39;Recall Score&#39;: 0.7517914012738853, &#39;ROC_AUC Score&#39;: 0.9167512969054126, &#39;f1 Score&#39;: 0.5817929759704252} . The scores have improved a little after adding weights, and we can make a new submission now. . Make submission . weighted_preds = (test_sub_preds * weights).sum(axis = 1) / 200 weighted_preds . array([0.49969991, 0.49995959, 0.50052471, ..., 0.49724971, 0.49961701, 0.49939437]) . # save predictions and export it to csv file submission_df[&quot;target&quot;] = weighted_preds submission_df.to_csv(&quot;lgbm_weighted_preds.csv&quot;, index = None) . Our new submission private ROC_AUC score is 0.91353. This makes our new private LB standing in the top 150, which means we are in the top 2% of the LB. Now, it is time to tune the model&#39;s hyperparameters and then we can make a final submission. . Hyperparameter Tuning . We can run Bayesian Optimization for hyperparameter tuning. It&#39;ll include the following steps: . Create a sperate train test split of ratio 1:1. | Create a black box function for Bayesian Optimization to execute. | Set parameter boundaries | Find the best hyperparameters by running maximise on Bayesian Optimization | . Because finding the best hyperparameters takes a long time, running the algorithm multiple times is neither feasable not environmental friendly. Thus, I&#39;ll use the hyperparameters from the past runs. . The inspiration for the tuning was taken from here. . # separate data for tuning X_train_tuning, X_valid_tuning, y_train_tuning, y_valid_tuning = train_test_split(X, y, stratify = y, test_size = 0.5, random_state = seed) . # black box function for Bayesian Optimization def LGB_bayesian( bagging_fraction, bagging_freq, # int lambda_l1, lambda_l2, learning_rate, max_depth, # int min_data_in_leaf, # int min_gain_to_split, min_sum_hessian_in_leaf, num_leaves, # int feature_fraction, num_boost_rounds): # LightGBM expects these parameters to be integer. So we make them integer bagging_freq = int(bagging_freq) num_leaves = int(num_leaves) min_data_in_leaf = int(min_data_in_leaf) max_depth = int(max_depth) num_boost_rounds = int(num_boost_rounds) # parameters param = {&#39;bagging_fraction&#39;: bagging_fraction, &#39;bagging_freq&#39;: bagging_freq, &#39;lambda_l1&#39;: lambda_l1, &#39;lambda_l2&#39;: lambda_l2, &#39;learning_rate&#39;: learning_rate, &#39;max_depth&#39;: max_depth, &#39;min_data_in_leaf&#39;: min_data_in_leaf, &#39;min_gain_to_split&#39;: min_gain_to_split, &#39;min_sum_hessian_in_leaf&#39;: min_sum_hessian_in_leaf, &#39;num_leaves&#39;: num_leaves, &#39;feature_fraction&#39;: feature_fraction, &#39;save_binary&#39;: True, &#39;seed&#39;: seed, &#39;feature_fraction_seed&#39;: seed, &#39;bagging_seed&#39;: seed, &#39;drop_seed&#39;: seed, &#39;data_random_seed&#39;: seed, &#39;objective&#39;: &#39;binary&#39;, &#39;boosting_type&#39;: &#39;gbdt&#39;, &#39;verbosity&#39;: -1, &#39;metric&#39;: &#39;auc&#39;, &#39;is_unbalance&#39;: True, &#39;boost_from_average&#39;: &#39;false&#39;, &#39;num_threads&#39;: core_count} # prediction matrix valid_sub_preds_tuning = np.zeros([X_valid_tuning.shape[0], 200]) # run training col by col for col in range(200): feature = X.columns[col] feature_set = [feature, &quot;new_&quot; + feature] # make lgbm datasets train_l_tuning = lgb.Dataset(X_train_tuning[feature_set], y_train_tuning) valid_l_tuning = lgb.Dataset(X_valid_tuning[feature_set], y_valid_tuning) # train model lgb_clf = lgb.train(param, train_l_tuning, num_boost_round = num_boost_rounds, valid_sets = [train_l_tuning, valid_l_tuning], verbose_eval = -1) # make predictions valid_sub_preds_tuning[:, col] = lgb_clf.predict(X_valid_tuning[feature_set], num_iteration = lgb_clf.best_iteration) # calculate feature weights weights = [] for col in range(200): feature_roc_score = roc_auc_score(y_valid, valid_sub_preds[:, col]) if feature_roc_score &gt; 0.5: weights.append(feature_roc_score) else: weights.append(0) # validation predictions weights = np.array(weights) weights_mean = weights.mean() weights = 1 + ((weights - weights_mean) / weights_mean) valid_full_preds_tuning = (valid_sub_preds_tuning * weights).sum(axis = 1) # score score = roc_auc_score(y_valid, valid_full_preds_tuning) return score . # parameter bounds bounds_LGB = { &#39;bagging_fraction&#39;: (0.5, 1), &#39;bagging_freq&#39;: (1, 4), &#39;lambda_l1&#39;: (0, 3.0), &#39;lambda_l2&#39;: (0, 3.0), &#39;learning_rate&#39;: (0.005, 0.3), &#39;max_depth&#39;:(3,8), &#39;min_data_in_leaf&#39;: (5, 20), &#39;min_gain_to_split&#39;: (0, 1), &#39;min_sum_hessian_in_leaf&#39;: (0.01, 20), &#39;num_leaves&#39;: (5, 20), &#39;feature_fraction&#39;: (0.05, 1), &#39;num_boost_rounds&#39;: (30, 130) } . LG_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state = seed) LG_BO.space.keys . [&#39;bagging_fraction&#39;, &#39;bagging_freq&#39;, &#39;feature_fraction&#39;, &#39;lambda_l1&#39;, &#39;lambda_l2&#39;, &#39;learning_rate&#39;, &#39;max_depth&#39;, &#39;min_data_in_leaf&#39;, &#39;min_gain_to_split&#39;, &#39;min_sum_hessian_in_leaf&#39;, &#39;num_boost_rounds&#39;, &#39;num_leaves&#39;] . # LG_BO.maximize(init_points=5, n_iter=120, acq=&#39;ucb&#39;, xi=0.0, alpha=1e-6) . Running the above cell after uncommenting the code will find the best hyperparameters. We&#39;ll use the results from the past run. . # tuned hyperparameters iterations = 126 param = {&#39;bagging_fraction&#39;: 0.7693, &#39;bagging_freq&#39;: 2, &#39;lambda_l1&#39;: 0.7199, &#39;lambda_l2&#39;: 1.992, &#39;learning_rate&#39;: 0.009455, &#39;max_depth&#39;: 3, &#39;min_data_in_leaf&#39;: 22, &#39;min_gain_to_split&#39;: 0.06549, &#39;min_sum_hessian_in_leaf&#39;: 18.55, &#39;num_leaves&#39;: 20, &#39;feature_fraction&#39;: 1} . Final Training and Submission . Now, that we have found the best hyperparameters for the model, we can train the model on the full training data, and then make a submission from its predictions. . # All hyperparameters iterations = 126 param = {&#39;bagging_fraction&#39;: 0.7693, &#39;bagging_freq&#39;: 2, &#39;lambda_l1&#39;: 0.7199, &#39;lambda_l2&#39;: 1.992, &#39;learning_rate&#39;: 0.009455, &#39;max_depth&#39;: 3, &#39;min_data_in_leaf&#39;: 22, &#39;min_gain_to_split&#39;: 0.06549, &#39;min_sum_hessian_in_leaf&#39;: 18.55, &#39;num_leaves&#39;: 20, &#39;feature_fraction&#39;: 1, &#39;save_binary&#39;: True, &#39;seed&#39;: seed, &#39;feature_fraction_seed&#39;: seed, &#39;bagging_seed&#39;: seed, &#39;drop_seed&#39;: seed, &#39;data_random_seed&#39;: seed, &#39;objective&#39;: &#39;binary&#39;, &#39;boosting_type&#39;: &#39;gbdt&#39;, &#39;verbosity&#39;: -1, &#39;metric&#39;: &#39;auc&#39;, &#39;is_unbalance&#39;: True, &#39;boost_from_average&#39;: &#39;false&#39;, &#39;num_threads&#39;: core_count} . # Model Training folds = StratifiedKFold(n_splits = 4) columns = X.columns col_count = 200 train_sub_preds = np.zeros([len(X), col_count]) test_sub_preds = np.zeros([len(test_df), col_count]) for col_idx in tqdm(range(col_count)): feature = columns[col_idx] feature_set = [feature, &#39;new_&#39; + feature] temp_preds = np.zeros(len(test_df)) for train_idx, valid_idx in folds.split(X, y): train_data = lgb.Dataset(X.iloc[train_idx][feature_set], y[train_idx]) valid_data = lgb.Dataset(X.iloc[valid_idx][feature_set], y[valid_idx]) clf = lgb.train(param, train_data, num_boost_round = iterations, valid_sets = [train_data, valid_data], verbose_eval=-1) train_sub_preds[valid_idx, col_idx] = clf.predict(X.iloc[valid_idx][feature_set], num_iteration=clf.best_iteration) temp_preds += clf.predict(test_df[feature_set], num_iteration=clf.best_iteration) / folds.n_splits test_sub_preds[:, col_idx] = temp_preds . 100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [15:51&lt;00:00, 4.76s/it] . # calculate feature weights weights = [] for col in range(200): feature_roc_score = roc_auc_score(y, train_sub_preds[:, col]) if feature_roc_score &gt; 0.5: weights.append(feature_roc_score) else: weights.append(0) # final predictions weights = np.array(weights) weights_mean = weights.mean() weights = 1 + ((weights - weights_mean) / weights_mean) train_full_preds = (train_sub_preds * weights).sum(axis = 1) / 200 test_full_preds = (test_sub_preds * weights).sum(axis = 1) / 200 . # avg roc_score on validation data roc_auc_score(y, train_full_preds) . 0.9210397907630518 . Make Submission . submission_df[&quot;target&quot;] = test_full_preds submission_df.to_csv(&quot;final_submssion.csv&quot;, index = None) . This submssion gives us a public score of 0.92231 and a private score of 0.92069. With this score we stand at about 60 rank out of 8712 current submissions. Thus we are in the top 1% with our final submission. . Summary and Conclusion . In this project, we participated in the Santander Customer Transaction Predition Competition on Kaggle and tried to achieve as high standing as possible in the public and private leaderboards. Our goal was to predict whether a customer would make a specific transaction of not. After doing basic data analysis, we compared multiple models and selected lightgbm which had the potential to improve the score most. We also applied some feature engineering, which helped in improving the auc roc score. Additional methods like using feaure weights to compute predictions, and hyperparameter tuning further improved the ML training and predictions. . Throughout all these steps, we made multiple submission files and submitted them on Kaggle, each improving the standing on the previous one. In the final submission, we achieved a private LB score of 0.92069 which placed us at rank 60 out of 8712 participants. . Thanks for reading. .",
            "url": "https://ncitshubham.github.io/blogs/2022/02/06/santander-customer-transaction.html",
            "relUrl": "/2022/02/06/santander-customer-transaction.html",
            "date": " • Feb 6, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "EDA and Data Visualization of Zomato Bangalore Restuarants Dataset",
            "content": "Introduction . Zomato is an Indian multinational restaurant aggregator and food delivery company. In this project, we&#39;re going to study and analyze the Zomato Dataset shared by Himanshu Podder on Kaggle. This dataset contains information on restaurants in the city of Bengaluru, India. We can use this dataset to get an idea of different factors affecting the restaurants in different parts of the city and also answer questions like which type of food is most popular in the city, how does the location of the restuarant affects its rating on the Zomato platform, and what relation does the rating of the restaurant and the number of cuisines it offers has? . We will use the approach of Exploratory Data Analysis (EDA) for studying this data, which is used to analyse datasets to summarize their main characteristics, often using statistical graphics and other data visualization methods. EDA can help in seeing what the data can tell us beyond the formal modeling or hypothesis testing task.3 . The dataset can be used to answer a lot of questions but for this project, we are going to limit ourselves to 5-10 questions and then try to answer these questions. . Setup . Import the required libraries and get the file path. . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # Data Visualization import seaborn as sns # Data Visualization import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . /kaggle/input/zomato-bangalore-restaurants/zomato.csv . file_path = &quot;/kaggle/input/zomato-bangalore-restaurants/zomato.csv&quot; !ls -lh {file_path} . -rw-r--r-- 1 nobody nogroup 548M Feb 1 14:14 /kaggle/input/zomato-bangalore-restaurants/zomato.csv . The file size of the dataset is 548MB and it is safe to import the whole dataset at once. . # Read the csv file into a pandas DataFrame df = pd.read_csv(file_path, thousands = &#39;,&#39;) . Data Preparation and Cleaning . Premilinary Analysis . Evaluate the structure of the dataset . df.head() . url address name online_order book_table rate votes phone location rest_type dish_liked cuisines approx_cost(for two people) reviews_list menu_item listed_in(type) listed_in(city) . 0 https://www.zomato.com/bangalore/jalsa-banasha... | 942, 21st Main Road, 2nd Stage, Banashankari, ... | Jalsa | Yes | Yes | 4.1/5 | 775 | 080 42297555 r n+91 9743772233 | Banashankari | Casual Dining | Pasta, Lunch Buffet, Masala Papad, Paneer Laja... | North Indian, Mughlai, Chinese | 800.0 | [(&#39;Rated 4.0&#39;, &#39;RATED n A beautiful place to ... | [] | Buffet | Banashankari | . 1 https://www.zomato.com/bangalore/spice-elephan... | 2nd Floor, 80 Feet Road, Near Big Bazaar, 6th ... | Spice Elephant | Yes | No | 4.1/5 | 787 | 080 41714161 | Banashankari | Casual Dining | Momos, Lunch Buffet, Chocolate Nirvana, Thai G... | Chinese, North Indian, Thai | 800.0 | [(&#39;Rated 4.0&#39;, &#39;RATED n Had been here for din... | [] | Buffet | Banashankari | . 2 https://www.zomato.com/SanchurroBangalore?cont... | 1112, Next to KIMS Medical College, 17th Cross... | San Churro Cafe | Yes | No | 3.8/5 | 918 | +91 9663487993 | Banashankari | Cafe, Casual Dining | Churros, Cannelloni, Minestrone Soup, Hot Choc... | Cafe, Mexican, Italian | 800.0 | [(&#39;Rated 3.0&#39;, &quot;RATED n Ambience is not that ... | [] | Buffet | Banashankari | . 3 https://www.zomato.com/bangalore/addhuri-udupi... | 1st Floor, Annakuteera, 3rd Stage, Banashankar... | Addhuri Udupi Bhojana | No | No | 3.7/5 | 88 | +91 9620009302 | Banashankari | Quick Bites | Masala Dosa | South Indian, North Indian | 300.0 | [(&#39;Rated 4.0&#39;, &quot;RATED n Great food and proper... | [] | Buffet | Banashankari | . 4 https://www.zomato.com/bangalore/grand-village... | 10, 3rd Floor, Lakshmi Associates, Gandhi Baza... | Grand Village | No | No | 3.8/5 | 166 | +91 8026612447 r n+91 9901210005 | Basavanagudi | Casual Dining | Panipuri, Gol Gappe | North Indian, Rajasthani | 600.0 | [(&#39;Rated 4.0&#39;, &#39;RATED n Very good restaurant ... | [] | Buffet | Banashankari | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 51717 entries, 0 to 51716 Data columns (total 17 columns): # Column Non-Null Count Dtype -- -- 0 url 51717 non-null object 1 address 51717 non-null object 2 name 51717 non-null object 3 online_order 51717 non-null object 4 book_table 51717 non-null object 5 rate 43942 non-null object 6 votes 51717 non-null int64 7 phone 50509 non-null object 8 location 51696 non-null object 9 rest_type 51490 non-null object 10 dish_liked 23639 non-null object 11 cuisines 51672 non-null object 12 approx_cost(for two people) 51371 non-null float64 13 reviews_list 51717 non-null object 14 menu_item 51717 non-null object 15 listed_in(type) 51717 non-null object 16 listed_in(city) 51717 non-null object dtypes: float64(1), int64(1), object(15) memory usage: 6.7+ MB . Change the structure . We&#39;ll drop the columns which don&#39;t inform us much about the restuarants. . df.drop([&quot;url&quot;, &quot;name&quot;, &quot;phone&quot;, &quot;reviews_list&quot;, &quot;address&quot;, &quot;menu_item&quot;], axis = 1, inplace = True) df.columns . Index([&#39;online_order&#39;, &#39;book_table&#39;, &#39;rate&#39;, &#39;votes&#39;, &#39;location&#39;, &#39;rest_type&#39;, &#39;dish_liked&#39;, &#39;cuisines&#39;, &#39;approx_cost(for two people)&#39;, &#39;listed_in(type)&#39;, &#39;listed_in(city)&#39;], dtype=&#39;object&#39;) . We&#39;ll also rename some of the columns. . df.rename(mapper = {&quot;listed_in(type)&quot;: &quot;type&quot;, &quot;approx_cost(for two people)&quot;: &quot;cost_for_two_people&quot;, &quot;rate&quot;: &quot;rating&quot;}, axis = 1, inplace = True) df.columns . Index([&#39;online_order&#39;, &#39;book_table&#39;, &#39;rating&#39;, &#39;votes&#39;, &#39;location&#39;, &#39;rest_type&#39;, &#39;dish_liked&#39;, &#39;cuisines&#39;, &#39;cost_for_two_people&#39;, &#39;type&#39;, &#39;listed_in(city)&#39;], dtype=&#39;object&#39;) . Correcting datatypes . Now we&#39;ll look at the dtypes of the DataFrame and see if that needs any change. . df.dtypes . online_order object book_table object rating object votes int64 location object rest_type object dish_liked object cuisines object cost_for_two_people float64 type object listed_in(city) object dtype: object . We need to change the rating column to numeric dtype. . # All distinct values in the `rating` column df.rating.unique() . array([&#39;4.1/5&#39;, &#39;3.8/5&#39;, &#39;3.7/5&#39;, &#39;3.6/5&#39;, &#39;4.6/5&#39;, &#39;4.0/5&#39;, &#39;4.2/5&#39;, &#39;3.9/5&#39;, &#39;3.1/5&#39;, &#39;3.0/5&#39;, &#39;3.2/5&#39;, &#39;3.3/5&#39;, &#39;2.8/5&#39;, &#39;4.4/5&#39;, &#39;4.3/5&#39;, &#39;NEW&#39;, &#39;2.9/5&#39;, &#39;3.5/5&#39;, nan, &#39;2.6/5&#39;, &#39;3.8 /5&#39;, &#39;3.4/5&#39;, &#39;4.5/5&#39;, &#39;2.5/5&#39;, &#39;2.7/5&#39;, &#39;4.7/5&#39;, &#39;2.4/5&#39;, &#39;2.2/5&#39;, &#39;2.3/5&#39;, &#39;3.4 /5&#39;, &#39;-&#39;, &#39;3.6 /5&#39;, &#39;4.8/5&#39;, &#39;3.9 /5&#39;, &#39;4.2 /5&#39;, &#39;4.0 /5&#39;, &#39;4.1 /5&#39;, &#39;3.7 /5&#39;, &#39;3.1 /5&#39;, &#39;2.9 /5&#39;, &#39;3.3 /5&#39;, &#39;2.8 /5&#39;, &#39;3.5 /5&#39;, &#39;2.7 /5&#39;, &#39;2.5 /5&#39;, &#39;3.2 /5&#39;, &#39;2.6 /5&#39;, &#39;4.5 /5&#39;, &#39;4.3 /5&#39;, &#39;4.4 /5&#39;, &#39;4.9/5&#39;, &#39;2.1/5&#39;, &#39;2.0/5&#39;, &#39;1.8/5&#39;, &#39;4.6 /5&#39;, &#39;4.9 /5&#39;, &#39;3.0 /5&#39;, &#39;4.8 /5&#39;, &#39;2.3 /5&#39;, &#39;4.7 /5&#39;, &#39;2.4 /5&#39;, &#39;2.1 /5&#39;, &#39;2.2 /5&#39;, &#39;2.0 /5&#39;, &#39;1.8 /5&#39;], dtype=object) . # Remove the non-desired values from the rating column df = df.loc[df.rating != &quot;NEW&quot;] df = df.loc[df.rating != &quot;-&quot;] # Select the first 3 characters and convert the column to numeric df.rating = pd.to_numeric(df.rating.str[:3]) . df.rating.head() . 0 4.1 1 4.1 2 3.8 3 3.7 4 3.8 Name: rating, dtype: float64 . Deal with null values . # Number of null values df.isnull().sum().sort_values(ascending = False) . dish_liked 25948 rating 7775 cost_for_two_people 341 rest_type 225 cuisines 45 location 21 online_order 0 book_table 0 votes 0 type 0 listed_in(city) 0 dtype: int64 . It appears that in all the columns with null values, absence of values neither indicates the value of zero nor informs us on something useful. Thus, it&#39;s better to drop the rows with null values. In the dish_liked column, because the null values account for about half of the data, it&#39;s better to drop the whole column. . df.dropna(subset = [&quot;location&quot;, &quot;rating&quot;, &quot;rest_type&quot;, &quot;cuisines&quot;, &quot;cost_for_two_people&quot;], inplace = True) df.drop([&quot;dish_liked&quot;], axis = 1, inplace = True) df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 41263 entries, 0 to 51716 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 online_order 41263 non-null object 1 book_table 41263 non-null object 2 rating 41263 non-null float64 3 votes 41263 non-null int64 4 location 41263 non-null object 5 rest_type 41263 non-null object 6 cuisines 41263 non-null object 7 cost_for_two_people 41263 non-null float64 8 type 41263 non-null object 9 listed_in(city) 41263 non-null object dtypes: float64(2), int64(1), object(7) memory usage: 3.5+ MB . df.head() . online_order book_table rating votes location rest_type cuisines cost_for_two_people type listed_in(city) . 0 Yes | Yes | 4.1 | 775 | Banashankari | Casual Dining | North Indian, Mughlai, Chinese | 800.0 | Buffet | Banashankari | . 1 Yes | No | 4.1 | 787 | Banashankari | Casual Dining | Chinese, North Indian, Thai | 800.0 | Buffet | Banashankari | . 2 Yes | No | 3.8 | 918 | Banashankari | Cafe, Casual Dining | Cafe, Mexican, Italian | 800.0 | Buffet | Banashankari | . 3 No | No | 3.7 | 88 | Banashankari | Quick Bites | South Indian, North Indian | 300.0 | Buffet | Banashankari | . 4 No | No | 3.8 | 166 | Basavanagudi | Casual Dining | North Indian, Rajasthani | 600.0 | Buffet | Banashankari | . Extras . We&#39;ll also convert the values in rest_type and cuisines columns to lists. . df.cuisines = df.cuisines.str.split(&quot;,&quot;) df.rest_type = df.rest_type.str.split(&quot;,&quot;) . df.head() . online_order book_table rating votes location rest_type cuisines cost_for_two_people type listed_in(city) . 0 Yes | Yes | 4.1 | 775 | Banashankari | [Casual Dining] | [North Indian, Mughlai, Chinese] | 800.0 | Buffet | Banashankari | . 1 Yes | No | 4.1 | 787 | Banashankari | [Casual Dining] | [Chinese, North Indian, Thai] | 800.0 | Buffet | Banashankari | . 2 Yes | No | 3.8 | 918 | Banashankari | [Cafe, Casual Dining] | [Cafe, Mexican, Italian] | 800.0 | Buffet | Banashankari | . 3 No | No | 3.7 | 88 | Banashankari | [Quick Bites] | [South Indian, North Indian] | 300.0 | Buffet | Banashankari | . 4 No | No | 3.8 | 166 | Basavanagudi | [Casual Dining] | [North Indian, Rajasthani] | 600.0 | Buffet | Banashankari | . Exploratory Analysis and Visualization . With these columns in mind, we will try to answer the following questions from the dataset: . What locations are most popular for restaurants in Benagluru? | Which locations have the best rated restaurants? | What relation does the rating and number of votes that a restaurant receives have? What about table booking and online order facility? | Is a restaurant which offers online order facility rated better? | Are restaurants offering expensive food rated better? Does a table booking facility make a difference? | Does the number of cuisines that a restaurant provides have a relation to the rating it recieves? | How do Casual Dining and Fine Dining restaurants differ in their rating? | How do Casual Dining and Fine Dining restaurants differ in their cost_for_two_people? | What are the number of different types of restaurants? | Q1. What locations are most popular for restaurants in Benagluru? . popular_locations = df.location.value_counts().head(15) popular_locations . BTM 3879 Koramangala 5th Block 2297 HSR 1993 Indiranagar 1800 JP Nagar 1710 Jayanagar 1634 Whitefield 1568 Marathahalli 1410 Bannerghatta Road 1226 Koramangala 7th Block 1055 Koramangala 6th Block 1054 Brigade Road 1052 Bellandur 997 Sarjapur Road 854 Koramangala 1st Block 852 Name: location, dtype: int64 . plt.figure(figsize = (10, 8)) sns.barplot(x = popular_locations, y = popular_locations.index) . &lt;AxesSubplot:xlabel=&#39;location&#39;&gt; . The 5 most popular locations for restaurants are BTM, Koramangala 5th Block, HSR, Indiranagar, and JP Nagar, with BTM boasting of nearly 4000 eateries. . Q2. Which locations have the best rated restaurants? . (We&#39;ll select only those locations which have a minimum of 50 restaurants.) . # groupby location and get the count of each location along with the average rating of restaurants in that location. location_rating = df.groupby(by = [&quot;location&quot;])[&quot;rating&quot;].agg([&quot;count&quot;, &quot;mean&quot;]) location_rating.head() . count mean . location . BTM 3879 | 3.571410 | . Banashankari 744 | 3.649866 | . Banaswadi 468 | 3.496368 | . Bannerghatta Road 1226 | 3.509869 | . Basavanagudi 595 | 3.671092 | . # select the locations with 50 minimumn eateries and then sort them by their rating. rated_locations = location_rating.loc[location_rating[&quot;count&quot;] &gt;= 50].sort_values(by = &quot;mean&quot;, ascending = False) # select the top 20 locations. top20_rated_locations = rated_locations[:20] top20_rated_locations.head() . count mean . location . Lavelle Road 481 | 4.141788 | . Koramangala 3rd Block 191 | 4.020419 | . St. Marks Road 343 | 4.017201 | . Koramangala 5th Block 2297 | 4.006661 | . Church Street 546 | 3.992125 | . # plot the observations plt.figure(figsize = (15, 5)) sns.barplot(x = top20_rated_locations.index, y = top20_rated_locations[&quot;mean&quot;]) plt.xticks(rotation = 45) plt.ylim(3.5, 4.3) plt.show() . These are the top 20 locations in Bengaluru based on the eatries&#39; ratings. The average rating in these locations range from 4.14 and 3.8. . Q3. What relation does the rating and number of votes that a restaurant receives have? What about table booking and online order facility? . # Relationship between `rating`, `votes` and `book_table` plt.figure(figsize = (8, 5)) sns.scatterplot(x = &quot;rating&quot;, y = &quot;votes&quot;, hue = &quot;book_table&quot;, data = df, s = 40) . &lt;AxesSubplot:xlabel=&#39;rating&#39;, ylabel=&#39;votes&#39;&gt; . The number of votes look to be directly correlated with the rating of the restaurant, and they look to increase exponentially with the rating after a critical point. This is to be expected because better restaurants would atrract more customers and thus more votes. . Also, the restaurants which don&#39;t provide booking facility are clustered in low ratings and less number of votes. In other words, more popular restaurants with high ratings are more likely to offer table booking facility, which can also be seen the following graph. . sns.boxplot(x = &quot;book_table&quot;, y = &quot;rating&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;book_table&#39;, ylabel=&#39;rating&#39;&gt; . Now, we can look at the scatterplot for rating, votes and online_order . # Relationship between `rating`, `votes` and `online_order` plt.figure(figsize = (8, 5)) sns.scatterplot(x = &quot;rating&quot;, y = &quot;votes&quot;, hue = &quot;online_order&quot;, data = df, s = 40) . &lt;AxesSubplot:xlabel=&#39;rating&#39;, ylabel=&#39;votes&#39;&gt; . The data points for online order facility are scattered in the graph, and thus the data doesn&#39;t reveal any relationship between these variables. But, it may be worth exploring its relationship with the votes and the rating individually, which we&#39;ll do in the following sections. . Q4. Is a restaurant which offers online order facility rated better? . sns.violinplot(x = &quot;online_order&quot;, y = &quot;rating&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;online_order&#39;, ylabel=&#39;rating&#39;&gt; . As discussed in the previous question, the rating doesn&#39;t seem to be any different between the restaurants which offer online order facility and which don&#39;t. . Q5. Are restaurants offering expensive food rated better? Does a table booking facility make a difference? . plt.figure(figsize = (8, 5)) sns.scatterplot(x = &quot;rating&quot;, y = &quot;cost_for_two_people&quot;, data = df, hue = &#39;book_table&#39;) . &lt;AxesSubplot:xlabel=&#39;rating&#39;, ylabel=&#39;cost_for_two_people&#39;&gt; . Although there are hints of an exponential relationship between the cost for two people and the rating of a restaurant, most of the data is clustered in low cost, and thus because of lack of data points for expensive restaurants, the data is inconclusive for this relations. But we can study book table facility individuallly with the cost for two people, where restaurants which offer book_table facility seem to be more expensive than restaurants which don&#39;t. . sns.boxplot(x = &quot;book_table&quot;, y = &quot;cost_for_two_people&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;book_table&#39;, ylabel=&#39;cost_for_two_people&#39;&gt; . This graph also supports the idea that table booking is correlated with the cost for two people. . Q6. Does the number of cuisines that a restaurant provides have a relation to the rating it recieves? . The cuisines column shows the all the cuisines that a restaurant offers. We can add column to the DataFrame to store the number of cuisines that a restaurant offers. . df[&quot;no_of_cuisines&quot;] = df.cuisines.str.len() df[&quot;no_of_cuisines&quot;].head() . 0 3 1 3 2 3 3 2 4 2 Name: no_of_cuisines, dtype: int64 . plt.figure(figsize = (8, 5)) sns.stripplot(x = &quot;no_of_cuisines&quot;, y = &quot;rating&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;no_of_cuisines&#39;, ylabel=&#39;rating&#39;&gt; . The rating seems to become more concentrated towards mean as the no. of cuisines that a restaurant offers goes up. But, this could also be a artifact of low no. of restaurants offering higher no. of cuisines. In general, the mean of rating also seems to go up with the increase in no. of cuisines, but the graph is inconclusive. We&#39;ll explore this more in the boxplot. . sns.boxplot(x = &quot;no_of_cuisines&quot;, y = &quot;rating&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;no_of_cuisines&#39;, ylabel=&#39;rating&#39;&gt; . This graph reflects the relationship better and does support the idea that restaurants offering more no of cuisines are usually rated better. . Q7. How do Casual Dining and Fine Dining restaurants differ in their cost_for_two_people? . #Look at the dataframe df.head() . online_order book_table rating votes location rest_type cuisines cost_for_two_people type listed_in(city) no_of_cuisines . 0 Yes | Yes | 4.1 | 775 | Banashankari | [Casual Dining] | [North Indian, Mughlai, Chinese] | 800.0 | Buffet | Banashankari | 3 | . 1 Yes | No | 4.1 | 787 | Banashankari | [Casual Dining] | [Chinese, North Indian, Thai] | 800.0 | Buffet | Banashankari | 3 | . 2 Yes | No | 3.8 | 918 | Banashankari | [Cafe, Casual Dining] | [Cafe, Mexican, Italian] | 800.0 | Buffet | Banashankari | 3 | . 3 No | No | 3.7 | 88 | Banashankari | [Quick Bites] | [South Indian, North Indian] | 300.0 | Buffet | Banashankari | 2 | . 4 No | No | 3.8 | 166 | Basavanagudi | [Casual Dining] | [North Indian, Rajasthani] | 600.0 | Buffet | Banashankari | 2 | . We&#39;ll need to explode the rest_type column to extract information. . # extract information from `rest_type` column rest_type_exploded = df.explode(column = &quot;rest_type&quot;) rest_type_exploded[&quot;rest_type&quot;] = rest_type_exploded[&quot;rest_type&quot;].str.strip() rest_type_exploded.head() . online_order book_table rating votes location rest_type cuisines cost_for_two_people type listed_in(city) no_of_cuisines . 0 Yes | Yes | 4.1 | 775 | Banashankari | Casual Dining | [North Indian, Mughlai, Chinese] | 800.0 | Buffet | Banashankari | 3 | . 1 Yes | No | 4.1 | 787 | Banashankari | Casual Dining | [Chinese, North Indian, Thai] | 800.0 | Buffet | Banashankari | 3 | . 2 Yes | No | 3.8 | 918 | Banashankari | Cafe | [Cafe, Mexican, Italian] | 800.0 | Buffet | Banashankari | 3 | . 2 Yes | No | 3.8 | 918 | Banashankari | Casual Dining | [Cafe, Mexican, Italian] | 800.0 | Buffet | Banashankari | 3 | . 3 No | No | 3.7 | 88 | Banashankari | Quick Bites | [South Indian, North Indian] | 300.0 | Buffet | Banashankari | 2 | . # separate data for casual dining restaurants and fine dining restaurants. fine_dining_rest = rest_type_exploded.loc[rest_type_exploded.rest_type == &quot;Fine Dining&quot;] casual_dining_rest = rest_type_exploded.loc[rest_type_exploded.rest_type == &quot;Casual Dining&quot;] . # plot the data sns.kdeplot(fine_dining_rest.cost_for_two_people, fill = True) sns.kdeplot(casual_dining_rest.cost_for_two_people, fill = True) plt.legend([&quot;Fine Dining&quot;, &quot;Casual Dining&quot;]) plt.show() . As expected, fine dining restaurants are much more expensive than casual dining restaurants. . Q8. How do Casual Dining and Fine Dining restaurants differ in their rating? . sns.kdeplot(fine_dining_rest.rating, fill = True) sns.kdeplot(casual_dining_rest.rating, fill = True) plt.legend([&quot;Fine Dining&quot;, &quot;Casual Dining&quot;]) plt.show() . Fine dining restaurants are usually rated better and their ratings show much less variance than the ratings of casual dining restaurants. . Q9. What are the five most common types of restaurants? . # Five most common types of restaurants most_common_types_of_restaurants = rest_type_exploded.rest_type.value_counts() most_common_types_of_restaurants.head() . Quick Bites 15144 Casual Dining 12188 Cafe 4604 Delivery 2946 Dessert Parlor 2695 Name: rest_type, dtype: int64 . sns.barplot(x = most_common_types_of_restaurants.head(), y = most_common_types_of_restaurants.index[:5]) plt.show() . Q10. What are the top 5 rated restaurants in type and no_of_cuisines combined? . In other words, which type and no_of_cuisines combinations gather the highest ratings? . # group and extract data for different `types` and `no of cuisines` rating_data = df.groupby(by = [&quot;type&quot;, &quot;no_of_cuisines&quot;])[&quot;rating&quot;].agg(&quot;mean&quot;) rating_data.head() . type no_of_cuisines Buffet 1 3.995402 2 3.903509 3 3.911111 4 4.063212 5 4.217284 Name: rating, dtype: float64 . Now, we&#39;ll make a 2D datarame out of this multiindexed pandas Series. . rating_data_df = rating_data.unstack() rating_data_df.head() . no_of_cuisines 1 2 3 4 5 6 7 8 . type . Buffet 3.995402 | 3.903509 | 3.911111 | 4.063212 | 4.217284 | 3.926667 | 3.500000 | 4.062500 | . Cafes 3.643697 | 3.760366 | 3.941109 | 3.938434 | 4.015942 | 4.090698 | 4.156757 | 4.084615 | . Delivery 3.593535 | 3.629772 | 3.654311 | 3.739603 | 3.817625 | 3.901439 | 3.999333 | 3.801149 | . Desserts 3.698520 | 3.772140 | 3.755556 | 3.934932 | 4.125610 | 4.084211 | 3.920000 | 4.060000 | . Dine-out 3.613442 | 3.603628 | 3.687592 | 3.825194 | 3.903406 | 4.006091 | 4.035433 | 3.982051 | . # plot the data plt.figure(figsize = (9, 7)) fig = sns.heatmap(data = rating_data_df, annot = True, cmap = &quot;rocket_r&quot;) fig.set(xlabel = &quot;No. of cuisines&quot;, ylabel = &quot;Type&quot;) plt.show() . From the plot, Pubs and bars which offer more than 3 cuisines are all rated high. Similarly, Drinks &amp; nightlife restaurants with multiple cuisines are also rated really high. . We can get the top 5 combinations from the rating_data Series. . # top 5 combinations for `type` and `no_of_cuisines` rating_data.sort_values(ascending = False).head() . type no_of_cuisines Drinks &amp; nightlife 7 4.464706 Pubs and bars 7 4.455556 8 4.400000 Drinks &amp; nightlife 6 4.264865 8 4.250000 Name: rating, dtype: float64 . Q11. What is the relationship between the type and cost_for_two_people? . plt.figure(figsize = (8, 6)) sns.boxplot(x = &quot;type&quot;, y = &quot;cost_for_two_people&quot;, data = df) plt.xticks(rotation = 45) plt.show() . First thing to note is that there are quite a few outliers in the data, almost all of them offering much more expensive food from the rest of the distribution. Also buffet, drinks &amp; nighlife, and pubs are much more expensive than eateries of the type desserts and delivery. . Summary and Conclusion . Many questions could be asked and explored from the Zomato Dataset, but here we tried to answer 11 of them. We studied all the restaurants in Bengaluru, who have registered on Zomato, and tried to explore multiple variables&#39; relationship with the restaurants&#39; ratings. We also studied what factors go along with the food cost for two people in these restaurants. . There are two important things to note here before making any conclusions. First, all the analysis we did might apply only to restaurants registered on Zomato and other similar online platforms, and might differ significantly if we explore the food industry offline. Second really important thing is all the relationships that we studied are correlational in nature. This project thus does not establish causal relationships, although it might suggest some and can be taken as an inspiration to conduct actual experimental studies to explore the variables discussed here. Keeping in mind this, we can look at what we actually did establish in this EDA of Zomato Dataset. . The most popular places for restaurants in Benagluru are BTM, Koramangala 5th Block, HSR, Indiranagar, and JP Nagar, with BTM boasting of nearly 4000 eateries. | The top 5 locations according to avg rating of restaurants are Lavelle Road, Koramangala 3rd Block, St. Marks Road, Koramangala 5th Block and Church Street. | Restaurants with higher ratings have generally received more votes than the restaurants with lower rating and they are more likely to offer table booking facility. | Also, restaurants offering table booking facility are also generally more expensive. | Restaurants offering more no. of cuisines are also on average rated better. | Fine dining restaurants are much more expensive than casual dining restaurants and they are also usually rated better with much less variance in the ratings. | Quick Bites and Casual Dining restaurants but are the most popular types of restaurant in Bengaluru on Zomato. | Pubs and bars which offer more than 3 cuisines are all rated high. Similarly, Drinks &amp; nightlife restaurants with multiple cuisines are also rated really highly. | Buffet, drinks &amp; nighlife, and pubs are much more expensive than eateries of the type desserts and delivery. | For the other questions we asked, the data was more or less inconclusive. We may need more extensive data to answer those questions. | . Apart from these inferences, many more interesting relationships can be studied and should be explored from this data. . Thanks for reading. .",
            "url": "https://ncitshubham.github.io/blogs/2021/11/23/eda-and-data-vis-of-zomato-dataset.html",
            "relUrl": "/2021/11/23/eda-and-data-vis-of-zomato-dataset.html",
            "date": " • Nov 23, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Scraping Data Science Jobs on in.indeed.com",
            "content": "Introduction . Indeed.com is a worldwide employment website for job listings. Globally, it hosts millions of job listings on thousands of jobs. In this project, we are interested in the &#39;Data Science&#39; related job listings on https://in.indeed.com/ . Thus, we&#39;ll scrape the website for this information and save that to a csv file for future use. In order to do this, we&#39;ll use the following tools: . Python as the programming language | Requests library for downloading the webpage contents | BeautifulSoup library for finding and accessing the relevant information from the downloaded webpage. | Numpy library for handling missing values. | Pandas library for saving the accessed information to a csv file. | . Here are the steps we&#39;ll follow: . We&#39;ll scrape first 30 pages from https://in.indeed.com/jobs?q=data%20science | We&#39;ll get a list of all 15 jobs on each page. | For each job, we&#39;ll grab the Job Title, Salary, Location, Company Name, and Company Rating. | We&#39;ll save all of the information in a csv file in the following format: . Title,Salary,Location,Company Name,Company Rating Data science,&quot;₹35,000 - ₹45,000 a month&quot;,&quot;Mumbai, Maharashtra&quot;,V Digitech Sevices Data Science ( 2 - 8 Yrs) - Remote,&quot;₹8,00,000 - ₹20,00,000 a year&quot;,Remote,ProGrad Data Science Internship,&quot;₹10,000 a month&quot;,&quot;Gurgaon, Haryana&quot;,Zigram . | . Initial Setup . Import the required libraries . import requests from bs4 import BeautifulSoup import pandas as pd from numpy import nan . Set up base URL and the user-agent. . # The base_url is grabbed after searching &#39;Data Science&#39; on in.indeed.com # The start value in the base_url will increment by 10 to access each following page. base_url = &quot;https://in.indeed.com/jobs?q=data%20science&amp;start={}&quot; header = {&quot;User-Agent&quot;: &quot;Mozilla/5.0&quot;} . Create a dictionary to save all information. . jobs = {&quot;Job Title&quot;: [], &quot;Salary&quot;: [], &quot;Location&quot;: [], &quot;Company Name&quot;: [], &quot;Company Rating&quot;: []} . Scrape the search result webpage. . Download webpage and create a BeautifulSoup object . def get_soup(url): &#39;&#39;&#39; This function will download the webpage for the url supplied as argument and return the BeautifulSoup object for the webpage which can be used to grab required information for the webpage. &#39;&#39;&#39; response = requests.get(url, &quot;html.parser&quot;, headers = header) if response.status_code != 200: raise Exception(&#39;Failed to load page {}&#39;.format(url)) soup = BeautifulSoup(response.text) return soup . Example for get_soup . soup = get_soup(base_url.format(10)) type(soup) . bs4.BeautifulSoup . Create a transform function . Now, we&#39;ll create a transform function to grab the list of jobs from the result webpage. . To do this, we&#39;ll pick td tags with the class: resultContent . . def transform(soup): # find all the job listings on the webpage. jobs_tags = soup.find_all(&quot;td&quot;, class_ = &quot;resultContent&quot;) # for each job, call helper functions to grab information about the job # and save that to jobs dictionary. for job in jobs_tags: jobs[&quot;Job Title&quot;].append(get_job_title(job)) jobs[&quot;Salary&quot;].append(get_job_salary(job)) jobs[&quot;Location&quot;].append(get_company_location(job)) jobs[&quot;Company Name&quot;].append(get_company_name(job)) jobs[&quot;Company Rating&quot;].append(get_company_rating(job)) . Example for finding the job tags . job_tags = soup.find_all(&quot;td&quot;, class_ = &quot;resultContent&quot;) print(len(job_tags)) . 15 . print(job_tags[0].text) . newData Analyst/ScientistSopra Steria3.5Noida, Uttar Pradesh . Create helper functions . Create helper functions to grab job information for each job and store that in the jobs dictionary. . First grab Job Title. . def get_job_title(job): &#39;&#39;&#39; Function to grab the job title. Because some job titles have a prefix new in their job titles, this function will automatically detect this prefix and return the title sans &#39;new&#39; in the job title. &#39;&#39;&#39; title = job.find(class_ = &quot;jobTitle&quot;).text if title[:3] == &quot;new&quot;: return title[3:] else: return title get_job_title(job_tags[0]) . &#39;Data Analyst/Scientist&#39; . Now, we&#39;ll grab the job salary, if the listing has one. . def get_job_salary(job): salary = job.find(&quot;div&quot;, class_ = &quot;salary-snippet&quot;) if salary: return salary.text else: return nan get_job_salary(job_tags[1]) . &#39;₹500 an hour&#39; . Similarly, we&#39;ll grab the company name, location, and its rating. . def get_company_name(job): &#39;&#39;&#39; Returns the company name for the supp&#39;&#39;&#39; return job.find(class_ = &quot;companyName&quot;).text def get_company_location(job): &#39;&#39;&#39; Returns the company location for the supplied job tag &#39;&#39;&#39; return job.find(class_ = &quot;companyLocation&quot;).text def get_company_rating(job): &#39;&#39;&#39; Returns the company rating for the supplied job tag &#39;&#39;&#39; rating = job.find(class_ = &quot;ratingNumber&quot;) if rating: return float(rating.text) else: return nan # Example print(get_company_name(job_tags[0]), get_company_location(job_tags[0]), get_company_rating(job_tags[0]), sep = &quot; n&quot;) . Sopra Steria Noida, Uttar Pradesh 3.5 . Putting it all together . We&#39;ll use a for loop to loop through 30 search result pages. Within this loop, we can apply the get_soup function to download these pages and the transform function to parse through all job listings from these pages and save the information in the jobs dictionary. We&#39;ll then use this dictionary to create a pandas DataFrame, which can then be saved to a csv file. . for page in range(0, 310, 10): print(f&quot;Scraping page {page}...&quot;) soup = get_soup(base_url.format(page)) transform(soup) . Scraping page 0... Scraping page 10... Scraping page 20... Scraping page 30... Scraping page 40... Scraping page 50... Scraping page 60... Scraping page 70... Scraping page 80... Scraping page 90... Scraping page 100... Scraping page 110... Scraping page 120... Scraping page 130... Scraping page 140... Scraping page 150... Scraping page 160... Scraping page 170... Scraping page 180... Scraping page 190... Scraping page 200... Scraping page 210... Scraping page 220... Scraping page 230... Scraping page 240... Scraping page 250... Scraping page 260... Scraping page 270... Scraping page 280... Scraping page 290... Scraping page 300... . # create a pandas DataFrame of the scraped data jobs_df = pd.DataFrame(jobs) jobs_df.head() . Job Title Salary Location Company Name Company Rating . 0 Technology Analyst: Data Science | Machine Lea... | NaN | Bengaluru, Karnataka | Infosys Limited | 3.9 | . 1 Analyst-Data Science | NaN | Gurgaon, Haryana+2 locations | Amex | NaN | . 2 Junior Data Scientist Data Science Chennai, India | NaN | Tamil Nadu | Applied Data Finance | NaN | . 3 Data Engineer – EPH | NaN | India | Kyndryl | 3.4 | . 4 Data Science Analyst | NaN | India | Helius Technologies | NaN | . # save data to a csv file jobs_df.to_csv(&quot;Data_Science_jobs_from_indeed.com.csv&quot;, index = None, encoding = &quot;utf-8&quot;) . Summary . This was a short project, where we looked into how job listings can be scraped from Indeed.com. We craped 30 pages of job listings with tags Data Science. This gave us a total of 450 job listings with the details like the job title, salary, company, location, etc. We then saved this scraped data into a csv file for future use. .",
            "url": "https://ncitshubham.github.io/blogs/2021/10/19/scraping-indeed.com-for-data-science-jobs.html",
            "relUrl": "/2021/10/19/scraping-indeed.com-for-data-science-jobs.html",
            "date": " • Oct 19, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "EDA of Stroke Dataset and Prediction of Strokes using Selected ML Algorithms",
            "content": "Introduction . According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths. . In this project, we&#39;ll try to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. To do this, we&#39;ll use the Stroke Prediction Dataset provided by fedesoriano on Kaggle. . Each row in the dataset provides relavant information about the patient like age, smoking status, gender, heart disease, bmi, work type and in the end whether the patient suffered a stroke. This last parameter will be our target, which we&#39;ll try to predict using information from the other columns. . The steps that we&#39;ll take: . Setup and import the dataset | Perform basic EDA and prepare the dataset for training | Train and evaluate a baseline model | Train multiple ML models and make predictions. | Evaluate and compare their performance. | . Setup . import numpy as np # Linear algebra import pandas as pd # Data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # Data Visualizatin import seaborn as sns # Data Visualization from imblearn.over_sampling import SMOTE # Oversampling imbalanced classes from sklearn.impute import SimpleImputer, MissingIndicator # Handle missing values from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate, GridSearchCV, StratifiedKFold # Model validation from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, RocCurveDisplay, PrecisionRecallDisplay, confusion_matrix, ConfusionMatrixDisplay # Metrics from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, FunctionTransformer # Data preparation from imblearn.pipeline import make_pipeline # Build pipeline from sklearn.compose import make_column_transformer # Combine data preparation steps # Models for prediciton from sklearn.dummy import DummyClassifier from sklearn.linear_model import LogisticRegression, RidgeClassifier from sklearn.svm import SVC from sklearn.naive_bayes import GaussianNB from sklearn.neighbors import KNeighborsClassifier from sklearn.ensemble import RandomForestClassifier from xgboost import XGBClassifier # ignore warnings import warnings warnings.filterwarnings(&quot;ignore&quot;) # get the file path import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . /kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv . file_path = &quot;/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv&quot; . # file size of the dataset !ls -lh {file_path} . -rw-r--r-- 1 nobody nogroup 310K Jan 29 2021 /kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv . # look at the first few rows of the dataseet !head {file_path} . id,gender,age,hypertension,heart_disease,ever_married,work_type,Residence_type,avg_glucose_level,bmi,smoking_status,stroke 9046,Male,67,0,1,Yes,Private,Urban,228.69,36.6,formerly smoked,1 51676,Female,61,0,0,Yes,Self-employed,Rural,202.21,N/A,never smoked,1 31112,Male,80,0,1,Yes,Private,Rural,105.92,32.5,never smoked,1 60182,Female,49,0,0,Yes,Private,Urban,171.23,34.4,smokes,1 1665,Female,79,1,0,Yes,Self-employed,Rural,174.12,24,never smoked,1 56669,Male,81,0,0,Yes,Private,Urban,186.21,29,formerly smoked,1 53882,Male,74,1,1,Yes,Private,Rural,70.09,27.4,never smoked,1 10434,Female,69,0,0,No,Private,Urban,94.39,22.8,never smoked,1 27419,Female,59,0,0,Yes,Private,Rural,76.15,N/A,Unknown,1 . Observations . File size is 310 KB, thus it is safe to import the whole dataset. | The delimiters are , | id is the index column | stroke is the prediction class in the last column | . # Load the dataset into pandas DataFrame df = pd.read_csv(file_path, index_col = [&quot;id&quot;]) df.head() . gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status stroke . id . 9046 Male | 67.0 | 0 | 1 | Yes | Private | Urban | 228.69 | 36.6 | formerly smoked | 1 | . 51676 Female | 61.0 | 0 | 0 | Yes | Self-employed | Rural | 202.21 | NaN | never smoked | 1 | . 31112 Male | 80.0 | 0 | 1 | Yes | Private | Rural | 105.92 | 32.5 | never smoked | 1 | . 60182 Female | 49.0 | 0 | 0 | Yes | Private | Urban | 171.23 | 34.4 | smokes | 1 | . 1665 Female | 79.0 | 1 | 0 | Yes | Self-employed | Rural | 174.12 | 24.0 | never smoked | 1 | . Basic EDA and Data Preparation . First, it is really important to separate the test data from the train data at this point, so that the transformers and models cannot learn from the test data itself. Before making a split, it is worth looking at the distribution of prediction class, to see if there is an imbalance and whether we will need to stratify the split. . # Distribution of prediction class stroke_val_count = df.stroke.value_counts() print(f&quot;Value Count in the prediction class - Stroke: n{stroke_val_count} n n&quot;) sns.barplot(x = stroke_val_count.index, y = stroke_val_count) plt.show() . Value Count in the prediction class - Stroke: 0 4861 1 249 Name: stroke, dtype: int64 . As we can see, the prediction class is highly imbalanced. Therefore, we&#39;ll need to stratify the split. Also, after making the split, it would be worth to generate artificial data in the training dataset to help the ML models distinguish better between the two categories of prediction class. . # Separate the prediction class from the training features X = df.copy() y = X.pop(&quot;stroke&quot;) # split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 42) . We&#39;ll perform EDA on the dataset and use the observations that we make for building a data preparation pipeline as the last step. . Basic Inspection . # Look at the dataset X_train.head() . gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status . id . 25283 Female | 48.0 | 0 | 0 | Yes | Private | Urban | 69.21 | 33.1 | never smoked | . 43734 Male | 15.0 | 0 | 0 | No | Private | Rural | 122.25 | 21.0 | never smoked | . 47113 Female | 67.0 | 0 | 0 | Yes | Self-employed | Rural | 110.42 | 24.9 | never smoked | . 56996 Male | 44.0 | 0 | 0 | Yes | Private | Urban | 65.41 | 24.8 | smokes | . 26325 Male | 14.0 | 0 | 0 | No | Govt_job | Urban | 82.34 | 31.6 | Unknown | . X_train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 4088 entries, 25283 to 31836 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 gender 4088 non-null object 1 age 4088 non-null float64 2 hypertension 4088 non-null int64 3 heart_disease 4088 non-null int64 4 ever_married 4088 non-null object 5 work_type 4088 non-null object 6 Residence_type 4088 non-null object 7 avg_glucose_level 4088 non-null float64 8 bmi 3918 non-null float64 9 smoking_status 4088 non-null object dtypes: float64(3), int64(2), object(5) memory usage: 351.3+ KB . Observations . There are 4088 entries in the train dataset. | There are total 10 features which we can use to predict the occurance of stroke. | There are some categorical features like gender, work_type, Residence_type of dtype object, which we&#39;ll need to be One Hot Encoded. | Numerical features will need to be scaled. | There are some missing values in the bmi column. | . Null values . # Null values X_train.isnull().sum() . gender 0 age 0 hypertension 0 heart_disease 0 ever_married 0 work_type 0 Residence_type 0 avg_glucose_level 0 bmi 170 smoking_status 0 dtype: int64 . # Distribution of null values in the dataset plt.figure(figsize = (10, 8)) sns.heatmap(X_train.isnull(), cmap = &quot;rocket&quot;, yticklabels = &quot;&quot;, cbar = None) . &lt;AxesSubplot:ylabel=&#39;id&#39;&gt; . null_bmi = y_train.loc[X_train.bmi.isna()] not_null_bmi = y_train.loc[X_train.bmi.notnull()] print(f&quot;&quot;&quot;Non null bmi values:- Stroke-no stroke ratio: {null_bmi.sum()/len(null_bmi)} Null bmi values:- Stroke-no stroke ratio: {not_null_bmi.sum()/len(not_null_bmi)} &quot;&quot;&quot;) . Non null bmi values:- Stroke-no stroke ratio: 0.21764705882352942 Null bmi values:- Stroke-no stroke ratio: 0.04134762633996937 . Observations . Although Null values look to be evenly distributed in the heatmap, the ratio for occurance of stroke is significatly different in the entries with null bmi values. Thus, instead of dropping the null values, it would be better to impute the null values with median bmi value, and also encode the presence of null values in a separate column. This may help in better prediction of stroke. . Categorical columns . First it is worth inspecting the categories and their distributions in all categorical columns. . # Distribution of Categorical Features cat_cols = [&#39;gender&#39;, &#39;hypertension&#39;, &#39;heart_disease&#39;, &#39;ever_married&#39;, &#39;work_type&#39;, &#39;Residence_type&#39;, &#39;smoking_status&#39;] print(&quot;Value Counts of all categorical columns... n&quot;) for i, col in enumerate(cat_cols): val_count = X_train[col].value_counts() print(f&quot;Values:- n{val_count} n n&quot;) plt.figure(i) sns.barplot(x = val_count.index, y = val_count) . Value Counts of all categorical columns... Values:- Female 2395 Male 1692 Other 1 Name: gender, dtype: int64 Values:- 0 3691 1 397 Name: hypertension, dtype: int64 Values:- 0 3867 1 221 Name: heart_disease, dtype: int64 Values:- Yes 2700 No 1388 Name: ever_married, dtype: int64 Values:- Private 2332 Self-employed 667 children 554 Govt_job 522 Never_worked 13 Name: work_type, dtype: int64 Values:- Urban 2069 Rural 2019 Name: Residence_type, dtype: int64 Values:- never smoked 1501 Unknown 1247 formerly smoked 714 smokes 626 Name: smoking_status, dtype: int64 . Observations . The categories in all the categorical features look ok, albeit most of the categories are unevenly distributed. Thus, we&#39;ll just one hot encode these columns. . Numerical Columns . # Look at the basic statistics num_cols = [&quot;age&quot;, &quot;avg_glucose_level&quot;, &quot;bmi&quot;] X_train[num_cols].describe() . age avg_glucose_level bmi . count 4088.000000 | 4088.000000 | 3918.000000 | . mean 43.353288 | 106.317167 | 28.922180 | . std 22.596816 | 45.259652 | 7.928378 | . min 0.080000 | 55.120000 | 10.300000 | . 25% 26.000000 | 77.312500 | 23.600000 | . 50% 45.000000 | 91.945000 | 28.000000 | . 75% 61.000000 | 114.197500 | 33.100000 | . max 82.000000 | 271.740000 | 97.600000 | . Observations . Age appears to be slightly negatively skewed | avg_glucose_level appears to be positively skewed | min age of 0.08 suggests that age is stored in fractions, which needs further inspection. | . # Distribution of Numerical (Continuous) Features for i, col in enumerate(num_cols): plt.figure(i) sns.violinplot(x = X_train[col]) plt.legend([f&quot;skewness: {X_train[col].skew():.2f} nkurtosis: {X_train[col].kurtosis():.2f}&quot;]) . Observations . age is very slightly negatively skewed with negative kurtosis. | avg_glucose_level and bmi are positively skewed with sharp peaks(positive kurtosis) | . Although some ML models assume normal distribution of data, they can work fine with data with small skewness and kurtosis values. Therefore, we&#39;ll just scale this data using MinMaxScaler. . # inspect`age` column print(sorted(X_train.age.unique())) . [0.08, 0.24, 0.32, 0.4, 0.48, 0.56, 0.64, 0.72, 0.8, 0.88, 1.0, 1.08, 1.16, 1.24, 1.32, 1.4, 1.48, 1.56, 1.64, 1.72, 1.8, 1.88, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0] . Observations . Only values smaller than 2 are stored as floats. Thus, we can change the whole column to int datatype to make this feature uniform. . Build Column Transformer . Now, we can use combine all the observations that we made to build a data preparation pipeline with a column transformer. We&#39;ll use the transformer to: . impute missing values | change age column dtype to int | add missing value indicator | onehotencode categorical columns | scale numerical features | . # Columns age_col = [&quot;age&quot;] num_cols_without_age = [&quot;avg_glucose_level&quot;, &quot;bmi&quot;] cat_cols = [&quot;gender&quot;, &quot;ever_married&quot;, &quot;work_type&quot;, &quot;Residence_type&quot;, &quot;smoking_status&quot;] # Scale the data scaler = MinMaxScaler() # handle missing values in `bmi` imputer = SimpleImputer(strategy = &quot;median&quot;) # change dtype of `age` def to_int(x): return pd.DataFrame(x).astype(int) int_tr = FunctionTransformer(to_int) . # build transformer for `age` separately age_transformer = make_pipeline(int_tr, scaler) # build transformer for numeric cols without age num_transformer = make_pipeline(imputer, scaler) # missing_indicator miss_ind = MissingIndicator() # build transformer for categorical cols cat_transformer = OneHotEncoder(drop = &quot;first&quot;, handle_unknown = &quot;ignore&quot;, sparse = False) # combine transformers to make a single column transformer ct = make_column_transformer((age_transformer, age_col), (num_transformer, num_cols_without_age), (miss_ind, num_cols_without_age), (cat_transformer, cat_cols), remainder = &quot;passthrough&quot;) . ct.fit_transform(X_train).shape . (4088, 17) . SMOTE . We&#39;ll use imblearn&#39;s SMOTE over-sampling to balance the data. We&#39;ll implement this within the final pipeline with the training model. . sm = SMOTE(random_state = 42) sm . SMOTE(random_state=42) . Train and Evaluate a Baseline Model . A baseline model which doesn&#39;t use any of the features to make predictions will give a baseline score, that the future ML models should at least beat. This score will help to identify errors in model training and evaluation if the models perform worse than the baseline score. . # Build imblearn pipeline with the DummyClassifier base_model = DummyClassifier(strategy = &#39;prior&#39;) base_pipe = make_pipeline(ct, sm, base_model) # specify KFold strategy cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42) . Baseline scores . # Scores on multiple metrics from the DummyClassifier cross_validate(base_pipe, X_train, y_train, cv = cv, n_jobs = -1, scoring = [&quot;accuracy&quot;, &quot;recall&quot;, &quot;roc_auc&quot;, &quot;f1&quot;]) . {&#39;fit_time&#39;: array([0.05653143, 0.05199432, 0.05150819, 0.05320215, 0.04633212]), &#39;score_time&#39;: array([0.04810262, 0.04459357, 0.04388809, 0.04212928, 0.03426027]), &#39;test_accuracy&#39;: array([0.95110024, 0.95110024, 0.95110024, 0.95226438, 0.95104039]), &#39;test_recall&#39;: array([0., 0., 0., 0., 0.]), &#39;test_roc_auc&#39;: array([0.5, 0.5, 0.5, 0.5, 0.5]), &#39;test_f1&#39;: array([0., 0., 0., 0., 0.])} . The model accuracy comes out to be really great at about 0.95. Although, it would generally be a good score to achieve. in this domain, a good accuracy score can be decieving. We can actually confirm this by looking at the test recall scores. All the test recall scores are 0, which means that the model failed to catch even a single True Positive. This is what we almost never want in the medical sphere. If the testing isn&#39;t prohibitively expensive or risky for the patient, which in this case it isn&#39;t, the test should aim for a high recall score and not a high accuracy score. That is why we&#39;ll judge our models using the roc_auc score primarily, which shows the relation between the True Positive Rate(TPR/recall) and False Positive Rate(FPR) of the model. For this baseline model, the roc_auc score comes out to be 0.5, which means the model isn&#39;t able to distinguish between the prediction classes at any of the thresholds, which we would expect from a dummy model. We can also look at the roc_auc_curve of the model. . # ROC curve of DummyClassifier base_preds_prob = cross_val_predict(base_pipe, X_train, y_train, cv = cv, n_jobs = -1, method = &quot;predict_proba&quot;)[:, 1] RocCurveDisplay.from_predictions(y_train, base_preds_prob) . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7fae7fadca90&gt; . Model Selection . In this section, we&#39;ll train multiple ML models typically used for binary classification and compare their performance using the scores from cross_validate . # Specify ML models models = {&quot;Logistic_Regression&quot;: LogisticRegression(random_state = 42), &quot;Ridge_Classification&quot;: RidgeClassifier(random_state = 42), &quot;SVC&quot;: SVC(random_state = 42), &quot;GaussianNB&quot;: GaussianNB(), &quot;KNClassifier&quot;: KNeighborsClassifier(n_neighbors = 5), &quot;RandomForestClassifier&quot;: RandomForestClassifier(max_depth = 10, n_jobs = -1, random_state = 42), &quot;XGBClassifier&quot;: XGBClassifier(n_estimators = 50, learning_rate = 0.03, n_jobs = -1, objective = &quot;binary:logistic&quot;, eval_metric = &quot;auc&quot;, tree_method = &quot;hist&quot;, random_state = 42)} . # Model comparison on multiple metrics scores = {} for model_name, model in models.items(): model_pipe = make_pipeline(ct, sm, model) cross_val = cross_validate(model_pipe, X_train, y_train, cv = cv, scoring = [&quot;accuracy&quot;, &quot;recall&quot;, &quot;precision&quot;, &quot;roc_auc&quot;, &quot;f1&quot;]) del cross_val[&quot;fit_time&quot;] del cross_val[&quot;score_time&quot;] print(model_name + &quot; : n&quot;) for score_name, score_vals in cross_val.items(): score_mean = score_vals.mean() score_std = score_vals.std() cross_val[score_name] = score_mean print(f&quot;{score_name}: Mean: {(score_mean * 100):.2f} % Std: {(score_std * 100):.2f} n&quot;) print(&quot; n&quot;, &quot;-&quot; * 50, &quot; n&quot;) scores[model_name] = cross_val . Logistic_Regression : test_accuracy: Mean: 75.42 % Std: 1.51 test_recall: Mean: 78.91 % Std: 2.90 test_precision: Mean: 14.06 % Std: 0.93 test_roc_auc: Mean: 84.92 % Std: 0.96 test_f1: Mean: 23.86 % Std: 1.40 -- Ridge_Classification : test_accuracy: Mean: 73.95 % Std: 1.57 test_recall: Mean: 79.41 % Std: 3.60 test_precision: Mean: 13.41 % Std: 0.94 test_roc_auc: Mean: 85.04 % Std: 1.38 test_f1: Mean: 22.93 % Std: 1.47 -- SVC : test_accuracy: Mean: 81.97 % Std: 1.27 test_recall: Mean: 45.26 % Std: 10.97 test_precision: Mean: 12.36 % Std: 1.46 test_roc_auc: Mean: 76.44 % Std: 3.34 test_f1: Mean: 19.36 % Std: 2.79 -- GaussianNB : test_accuracy: Mean: 25.37 % Std: 4.74 test_recall: Mean: 99.50 % Std: 1.00 test_precision: Mean: 6.12 % Std: 0.43 test_roc_auc: Mean: 82.68 % Std: 1.54 test_f1: Mean: 11.53 % Std: 0.76 -- KNClassifier : test_accuracy: Mean: 83.76 % Std: 1.21 test_recall: Mean: 36.19 % Std: 3.84 test_precision: Mean: 11.84 % Std: 0.47 test_roc_auc: Mean: 65.28 % Std: 0.76 test_f1: Mean: 17.80 % Std: 0.71 -- RandomForestClassifier : test_accuracy: Mean: 83.41 % Std: 1.46 test_recall: Mean: 40.69 % Std: 5.24 test_precision: Mean: 12.65 % Std: 0.85 test_roc_auc: Mean: 80.27 % Std: 0.91 test_f1: Mean: 19.25 % Std: 1.29 -- XGBClassifier : test_accuracy: Mean: 77.28 % Std: 1.95 test_recall: Mean: 63.27 % Std: 6.57 test_precision: Mean: 12.89 % Std: 1.56 test_roc_auc: Mean: 80.95 % Std: 2.15 test_f1: Mean: 21.39 % Std: 2.45 -- . scores_df = pd.DataFrame.from_dict(scores, orient = &#39;index&#39;) scores_df.sort_values(by = &quot;test_roc_auc&quot;, ascending = False) . test_accuracy test_recall test_precision test_roc_auc test_f1 . Ridge_Classification 0.739481 | 0.794103 | 0.134058 | 0.850447 | 0.229299 | . Logistic_Regression 0.754158 | 0.789103 | 0.140608 | 0.849226 | 0.238578 | . GaussianNB 0.253661 | 0.995000 | 0.061211 | 0.826782 | 0.115301 | . XGBClassifier 0.772752 | 0.632692 | 0.128876 | 0.809516 | 0.213944 | . RandomForestClassifier 0.834148 | 0.406923 | 0.126488 | 0.802683 | 0.192495 | . SVC 0.819717 | 0.452564 | 0.123650 | 0.764393 | 0.193636 | . KNClassifier 0.837574 | 0.361923 | 0.118366 | 0.652839 | 0.177978 | . From the scores we get, linear models although lacking in accuracy porformed way much better than the others in recall, auc_roc, and f1 metrics. Their roc_auc score of above 80s but low precision score suggests that they can do well with threshold tuning. The tree models like RandomForestClassifier and XGBClassifier also achieved satisfactory performance with high accuracy and moderate roc_auc scores. KNClassifier and SVC performed the worst and may require some hyperparameter tuning. But time would be better spent tuning the linear models to get even better performance than to tune the other ML models. . Model Tuning . We&#39;ll train LogisticRegression and RidgeClassifier ML models with GridSearchCV to find the best model and its parameters. We can then train this model on the full train dataset and then make predictions on the test dataset in the next section. . # Initialize models and specify param_grid for GridSearchCV models_and_params = {&quot;LogisticRegression&quot;: [LogisticRegression(random_state = 42), {&quot;logisticregression__class_weight&quot;: [{0:1, 1: 1}, {0:1, 1:3}]}], &quot;RidgeClassification&quot;: [RidgeClassifier(random_state = 42), {&quot;ridgeclassifier__alpha&quot;: [1, 2, 3], &quot;ridgeclassifier__class_weight&quot;: [{0:1, 1: 1}, {0:1, 1:3}]}]} . # Run GridSearchCV and store its results tuning_scores = [] for model_name in models_and_params: model, params = models_and_params[model_name] model_pipe = make_pipeline(ct, sm, model) grid_cv = GridSearchCV(model_pipe, params, scoring = [&quot;accuracy&quot;, &quot;recall&quot;, &quot;precision&quot;, &quot;roc_auc&quot;, &quot;f1&quot;], n_jobs = -1, cv = cv, refit = False) grid_cv.fit(X_train, y_train) tuning_scores.append(grid_cv) . # Results for LogisticRegression logistic_regression_grid_result = pd.DataFrame.from_dict(tuning_scores[0].cv_results_) logistic_regression_grid_result = logistic_regression_grid_result[[ &quot;param_logisticregression__class_weight&quot;, &quot;mean_test_accuracy&quot;, &quot;mean_test_precision&quot;, &quot;mean_test_recall&quot;, &quot;mean_test_roc_auc&quot;, &quot;mean_test_f1&quot;]] logistic_regression_grid_result.sort_values(by = &quot;mean_test_roc_auc&quot;, ascending = False) . param_logisticregression__class_weight mean_test_accuracy mean_test_precision mean_test_recall mean_test_roc_auc mean_test_f1 . 0 {0: 1, 1: 1} | 0.754158 | 0.140608 | 0.789103 | 0.849226 | 0.238578 | . 1 {0: 1, 1: 3} | 0.610319 | 0.104987 | 0.929744 | 0.847900 | 0.188645 | . # Results for RidgeClassifier ridge_classifier_grid_result = pd.DataFrame.from_dict(tuning_scores[1].cv_results_) ridge_classifier_grid_result = ridge_classifier_grid_result[[ &quot;param_ridgeclassifier__alpha&quot;, &quot;param_ridgeclassifier__class_weight&quot;, &quot;mean_test_accuracy&quot;, &quot;mean_test_precision&quot;, &quot;mean_test_recall&quot;, &quot;mean_test_roc_auc&quot;, &quot;mean_test_f1&quot;]] ridge_classifier_grid_result.sort_values(by = &quot;mean_test_roc_auc&quot;, ascending = False) . param_ridgeclassifier__alpha param_ridgeclassifier__class_weight mean_test_accuracy mean_test_precision mean_test_recall mean_test_roc_auc mean_test_f1 . 4 3 | {0: 1, 1: 1} | 0.738258 | 0.133504 | 0.794103 | 0.850666 | 0.228482 | . 2 2 | {0: 1, 1: 1} | 0.739237 | 0.133963 | 0.794103 | 0.850492 | 0.229157 | . 0 1 | {0: 1, 1: 1} | 0.739481 | 0.134058 | 0.794103 | 0.850447 | 0.229299 | . 5 3 | {0: 1, 1: 3} | 0.582188 | 0.101112 | 0.959872 | 0.844802 | 0.182931 | . 3 2 | {0: 1, 1: 3} | 0.582432 | 0.101167 | 0.959872 | 0.844769 | 0.183022 | . 1 1 | {0: 1, 1: 3} | 0.583411 | 0.100952 | 0.954872 | 0.844712 | 0.182581 | . From these grid search, we have found that both the logistic regression and the ridge regression models are really close in performance on their roc_auc score which is the primary metric, and that the increasing alpha values in ridge classifier does help in improving the scores. Changing class weights improves test recall a lot but because of tradeoff between test recall and test precision, test precision takes a sligh hit, along with a large hit on test accuracy. Now, we have to make a call on what parameter is more important for us between test precision and test recall. with the roc_auc, the primary metric still high, we choose to prefer better test precision and test accuracy. . From this, we have identified the ML model - RidgeClassifier with the default class weghts 1:1 and alpha value of 3. Now, we will train this final model and make predictions on the test dataset and then evaluate those predictions. . Final Model and Predictions . We can now train a RidgeClassifier with the default parameters with a imblearn Pipeline and then make predictions on the test dataset. . # Model Training ridge = RidgeClassifier(random_state = 42, alpha = 3) ridge_pipe = make_pipeline(ct, sm, ridge) ridge_pipe.fit(X_train, y_train) . Pipeline(steps=[(&#39;columntransformer&#39;, ColumnTransformer(remainder=&#39;passthrough&#39;, transformers=[(&#39;pipeline-1&#39;, Pipeline(steps=[(&#39;functiontransformer&#39;, FunctionTransformer(func=&lt;function to_int at 0x7fae7f9bcf80&gt;)), (&#39;minmaxscaler&#39;, MinMaxScaler())]), [&#39;age&#39;]), (&#39;pipeline-2&#39;, Pipeline(steps=[(&#39;simpleimputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;minmaxscaler&#39;, MinM... [&#39;avg_glucose_level&#39;, &#39;bmi&#39;]), (&#39;missingindicator&#39;, MissingIndicator(), [&#39;avg_glucose_level&#39;, &#39;bmi&#39;]), (&#39;onehotencoder&#39;, OneHotEncoder(drop=&#39;first&#39;, handle_unknown=&#39;ignore&#39;, sparse=False), [&#39;gender&#39;, &#39;ever_married&#39;, &#39;work_type&#39;, &#39;Residence_type&#39;, &#39;smoking_status&#39;])])), (&#39;smote&#39;, SMOTE(random_state=42)), (&#39;ridgeclassifier&#39;, RidgeClassifier(alpha=3, random_state=42))]) . # Predictions on test dataset preds = ridge_pipe.predict(X_test) ConfusionMatrixDisplay.from_predictions(y_test, preds) . &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fae7f031b90&gt; . # Metrics for test predictions RocCurveDisplay.from_estimator(ridge_pipe, X_test, y_test, name = &quot;RidgeClassifier&quot;) print(&quot;Accuracy Score:&quot;, accuracy_score(y_test, preds)) print(&quot;Precision Score:&quot;, precision_score(y_test, preds)) print(&quot;Recall Score:&quot;, recall_score(y_test, preds)) . Accuracy Score: 0.7465753424657534 Precision Score: 0.1384083044982699 Recall Score: 0.8 . The classifier has predicted 723 True Negatives and 40 True Positives. There are 259 misclassifications too out of which 249 are False Positives. Considering that the predictions class was highly imbalanced to begin with, the model has worked really well in making correct classifications. Its AUC_ROC score on the test dataset is 0.85, similar to what we found on the train dataset. With more data, we can improve this score further. . Summary and Conclusion . In this project, we used the Stroke Dataset available on Kaggle to predict whether a patient would suffer from a stroke. First, we prepared the data for training and test by splitting it using train_test_split. Then we explored the data and understood where it needed some cleaning and preparation. With this knowledge, we developed imblearn&#39;s Pipelines to clean the data. We then explored multiple ML models and studied their performance through multiple metrics, primarily focusing on roc_auc scores. This choice of metrics was made with the knowledge that in the medicinal domain, the correct knowledge of True Positives is much more valuable than the wrong knowledge on False Positives. We chose 2 linear models from this step for further model tuning and selection. RidgeClassifier turned out to be performing the best with its default parameters. We then trained this model and made predictions on the test data that we got from the split earlier. On the predictions, we achieved respectable scores on recall - 0.8 and on precision - 0.14. .",
            "url": "https://ncitshubham.github.io/blogs/2021/09/21/stroke-prediction.html",
            "relUrl": "/2021/09/21/stroke-prediction.html",
            "date": " • Sep 21, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "EDA and Prediction of Real Estate Sale Price using Select ML Algorithms on Kaggle",
            "content": "Introduction . In this notebook, we&#39;ll work on the Ames Housing Dataset available on Kaggle as an educational competition. With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, the competition challenges us to predict the final price of each home. The competition dataset was compiled by Dean De Cock and it is an incredible alternative as a modernized and expanded version of the often cited Boston Housing dataset. . Our work will be to make predictions on the SalePrice of the houses in the dataset. We will train ML algorithms on the train dataset given by the competition and then make submissions on the predictions of SalePrice of houses in the test dataset. Our submissions will be evaluated by rmsle, and we&#39;ll try to improve on this metric with each of our submission. . Setup . First, we&#39;ll import the required libraries and get the file paths . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # data visualization import seaborn as sns # data visualization from sklearn.preprocessing import OneHotEncoder, Normalizer, RobustScaler # data preparation from sklearn.impute import SimpleImputer # missing value handling from sklearn.model_selection import KFold, cross_val_score # model selection from sklearn.metrics import mean_squared_error # metrics from scipy.stats import norm, skew # statistics import psutil # get cpu core count from bayes_opt import BayesianOptimization # hyperparameter tuning # pipelines from sklearn.compose import make_column_transformer from sklearn.pipeline import make_pipeline # ML models from sklearn.dummy import DummyRegressor from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV from sklearn.ensemble import RandomForestRegressor import lightgbm as lgb import xgboost as xgb # ignore warnings import warnings warnings.filterwarnings(&quot;ignore&quot;) # get file paths import os data_dir = os.getcwd() for dirname, _, filenames in os.walk(data_dir): for filename in filenames: if filename[-4:] == &#39;.csv&#39;: print(os.path.join(dirname, filename)) . C: Users ncits Downloads Ames Housing sample_submission.csv C: Users ncits Downloads Ames Housing test.csv C: Users ncits Downloads Ames Housing train.csv . All data files are under 1MB and safe to import wholly. . # import files train = pd.read_csv(data_dir + &quot;/train.csv&quot;, index_col = [&quot;Id&quot;]) test = pd.read_csv(data_dir + &quot;/test.csv&quot;, index_col = [&quot;Id&quot;]) submission_df = pd.read_csv(data_dir + &quot;/sample_submission.csv&quot;) . train.head() . MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice . Id . 1 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | Inside | ... | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | 208500 | . 2 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | FR2 | ... | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | 181500 | . 3 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | Inside | ... | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | 223500 | . 4 70 | RL | 60.0 | 9550 | Pave | NaN | IR1 | Lvl | AllPub | Corner | ... | 0 | NaN | NaN | NaN | 0 | 2 | 2006 | WD | Abnorml | 140000 | . 5 60 | RL | 84.0 | 14260 | Pave | NaN | IR1 | Lvl | AllPub | FR2 | ... | 0 | NaN | NaN | NaN | 0 | 12 | 2008 | WD | Normal | 250000 | . 5 rows × 80 columns . Observations . data contains both numeric and categorical features. | SalePrice is the target column | . # random state seed seed = 42 . # Separate target from features X_train = train.copy() y_train = X_train.pop(&quot;SalePrice&quot;) . EDA and Data Preparation . Now, we&#39;ll perform some basic data analysis and we&#39;ll use the insights we&#39;ll get to prepare the data for ML training. . Preliminary Analysis . pd.set_option(&quot;display.max_columns&quot;, None) X_train.head(15) . MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle OverallQual OverallCond YearBuilt YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF Heating HeatingQC CentralAir Electrical 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr KitchenQual TotRmsAbvGrd Functional Fireplaces FireplaceQu GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond PavedDrive WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition . Id . 1 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2003 | 2003 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 196.0 | Gd | TA | PConc | Gd | TA | No | GLQ | 706 | Unf | 0 | 150 | 856 | GasA | Ex | Y | SBrkr | 856 | 854 | 0 | 1710 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 8 | Typ | 0 | NaN | Attchd | 2003.0 | RFn | 2 | 548 | TA | TA | Y | 0 | 61 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | . 2 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | FR2 | Gtl | Veenker | Feedr | Norm | 1Fam | 1Story | 6 | 8 | 1976 | 1976 | Gable | CompShg | MetalSd | MetalSd | None | 0.0 | TA | TA | CBlock | Gd | TA | Gd | ALQ | 978 | Unf | 0 | 284 | 1262 | GasA | Ex | Y | SBrkr | 1262 | 0 | 0 | 1262 | 0 | 1 | 2 | 0 | 3 | 1 | TA | 6 | Typ | 1 | TA | Attchd | 1976.0 | RFn | 2 | 460 | TA | TA | Y | 298 | 0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | . 3 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2001 | 2002 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 162.0 | Gd | TA | PConc | Gd | TA | Mn | GLQ | 486 | Unf | 0 | 434 | 920 | GasA | Ex | Y | SBrkr | 920 | 866 | 0 | 1786 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 6 | Typ | 1 | TA | Attchd | 2001.0 | RFn | 2 | 608 | TA | TA | Y | 0 | 42 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | . 4 70 | RL | 60.0 | 9550 | Pave | NaN | IR1 | Lvl | AllPub | Corner | Gtl | Crawfor | Norm | Norm | 1Fam | 2Story | 7 | 5 | 1915 | 1970 | Gable | CompShg | Wd Sdng | Wd Shng | None | 0.0 | TA | TA | BrkTil | TA | Gd | No | ALQ | 216 | Unf | 0 | 540 | 756 | GasA | Gd | Y | SBrkr | 961 | 756 | 0 | 1717 | 1 | 0 | 1 | 0 | 3 | 1 | Gd | 7 | Typ | 1 | Gd | Detchd | 1998.0 | Unf | 3 | 642 | TA | TA | Y | 0 | 35 | 272 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2006 | WD | Abnorml | . 5 60 | RL | 84.0 | 14260 | Pave | NaN | IR1 | Lvl | AllPub | FR2 | Gtl | NoRidge | Norm | Norm | 1Fam | 2Story | 8 | 5 | 2000 | 2000 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 350.0 | Gd | TA | PConc | Gd | TA | Av | GLQ | 655 | Unf | 0 | 490 | 1145 | GasA | Ex | Y | SBrkr | 1145 | 1053 | 0 | 2198 | 1 | 0 | 2 | 1 | 4 | 1 | Gd | 9 | Typ | 1 | TA | Attchd | 2000.0 | RFn | 3 | 836 | TA | TA | Y | 192 | 84 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 12 | 2008 | WD | Normal | . 6 50 | RL | 85.0 | 14115 | Pave | NaN | IR1 | Lvl | AllPub | Inside | Gtl | Mitchel | Norm | Norm | 1Fam | 1.5Fin | 5 | 5 | 1993 | 1995 | Gable | CompShg | VinylSd | VinylSd | None | 0.0 | TA | TA | Wood | Gd | TA | No | GLQ | 732 | Unf | 0 | 64 | 796 | GasA | Ex | Y | SBrkr | 796 | 566 | 0 | 1362 | 1 | 0 | 1 | 1 | 1 | 1 | TA | 5 | Typ | 0 | NaN | Attchd | 1993.0 | Unf | 2 | 480 | TA | TA | Y | 40 | 30 | 0 | 320 | 0 | 0 | NaN | MnPrv | Shed | 700 | 10 | 2009 | WD | Normal | . 7 20 | RL | 75.0 | 10084 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | Somerst | Norm | Norm | 1Fam | 1Story | 8 | 5 | 2004 | 2005 | Gable | CompShg | VinylSd | VinylSd | Stone | 186.0 | Gd | TA | PConc | Ex | TA | Av | GLQ | 1369 | Unf | 0 | 317 | 1686 | GasA | Ex | Y | SBrkr | 1694 | 0 | 0 | 1694 | 1 | 0 | 2 | 0 | 3 | 1 | Gd | 7 | Typ | 1 | Gd | Attchd | 2004.0 | RFn | 2 | 636 | TA | TA | Y | 255 | 57 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 8 | 2007 | WD | Normal | . 8 60 | RL | NaN | 10382 | Pave | NaN | IR1 | Lvl | AllPub | Corner | Gtl | NWAmes | PosN | Norm | 1Fam | 2Story | 7 | 6 | 1973 | 1973 | Gable | CompShg | HdBoard | HdBoard | Stone | 240.0 | TA | TA | CBlock | Gd | TA | Mn | ALQ | 859 | BLQ | 32 | 216 | 1107 | GasA | Ex | Y | SBrkr | 1107 | 983 | 0 | 2090 | 1 | 0 | 2 | 1 | 3 | 1 | TA | 7 | Typ | 2 | TA | Attchd | 1973.0 | RFn | 2 | 484 | TA | TA | Y | 235 | 204 | 228 | 0 | 0 | 0 | NaN | NaN | Shed | 350 | 11 | 2009 | WD | Normal | . 9 50 | RM | 51.0 | 6120 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | OldTown | Artery | Norm | 1Fam | 1.5Fin | 7 | 5 | 1931 | 1950 | Gable | CompShg | BrkFace | Wd Shng | None | 0.0 | TA | TA | BrkTil | TA | TA | No | Unf | 0 | Unf | 0 | 952 | 952 | GasA | Gd | Y | FuseF | 1022 | 752 | 0 | 1774 | 0 | 0 | 2 | 0 | 2 | 2 | TA | 8 | Min1 | 2 | TA | Detchd | 1931.0 | Unf | 2 | 468 | Fa | TA | Y | 90 | 0 | 205 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 4 | 2008 | WD | Abnorml | . 10 190 | RL | 50.0 | 7420 | Pave | NaN | Reg | Lvl | AllPub | Corner | Gtl | BrkSide | Artery | Artery | 2fmCon | 1.5Unf | 5 | 6 | 1939 | 1950 | Gable | CompShg | MetalSd | MetalSd | None | 0.0 | TA | TA | BrkTil | TA | TA | No | GLQ | 851 | Unf | 0 | 140 | 991 | GasA | Ex | Y | SBrkr | 1077 | 0 | 0 | 1077 | 1 | 0 | 1 | 0 | 2 | 2 | TA | 5 | Typ | 2 | TA | Attchd | 1939.0 | RFn | 1 | 205 | Gd | TA | Y | 0 | 4 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 1 | 2008 | WD | Normal | . 11 20 | RL | 70.0 | 11200 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | Sawyer | Norm | Norm | 1Fam | 1Story | 5 | 5 | 1965 | 1965 | Hip | CompShg | HdBoard | HdBoard | None | 0.0 | TA | TA | CBlock | TA | TA | No | Rec | 906 | Unf | 0 | 134 | 1040 | GasA | Ex | Y | SBrkr | 1040 | 0 | 0 | 1040 | 1 | 0 | 1 | 0 | 3 | 1 | TA | 5 | Typ | 0 | NaN | Detchd | 1965.0 | Unf | 1 | 384 | TA | TA | Y | 0 | 0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | . 12 60 | RL | 85.0 | 11924 | Pave | NaN | IR1 | Lvl | AllPub | Inside | Gtl | NridgHt | Norm | Norm | 1Fam | 2Story | 9 | 5 | 2005 | 2006 | Hip | CompShg | WdShing | Wd Shng | Stone | 286.0 | Ex | TA | PConc | Ex | TA | No | GLQ | 998 | Unf | 0 | 177 | 1175 | GasA | Ex | Y | SBrkr | 1182 | 1142 | 0 | 2324 | 1 | 0 | 3 | 0 | 4 | 1 | Ex | 11 | Typ | 2 | Gd | BuiltIn | 2005.0 | Fin | 3 | 736 | TA | TA | Y | 147 | 21 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 7 | 2006 | New | Partial | . 13 20 | RL | NaN | 12968 | Pave | NaN | IR2 | Lvl | AllPub | Inside | Gtl | Sawyer | Norm | Norm | 1Fam | 1Story | 5 | 6 | 1962 | 1962 | Hip | CompShg | HdBoard | Plywood | None | 0.0 | TA | TA | CBlock | TA | TA | No | ALQ | 737 | Unf | 0 | 175 | 912 | GasA | TA | Y | SBrkr | 912 | 0 | 0 | 912 | 1 | 0 | 1 | 0 | 2 | 1 | TA | 4 | Typ | 0 | NaN | Detchd | 1962.0 | Unf | 1 | 352 | TA | TA | Y | 140 | 0 | 0 | 0 | 176 | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | . 14 20 | RL | 91.0 | 10652 | Pave | NaN | IR1 | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 1Story | 7 | 5 | 2006 | 2007 | Gable | CompShg | VinylSd | VinylSd | Stone | 306.0 | Gd | TA | PConc | Gd | TA | Av | Unf | 0 | Unf | 0 | 1494 | 1494 | GasA | Ex | Y | SBrkr | 1494 | 0 | 0 | 1494 | 0 | 0 | 2 | 0 | 3 | 1 | Gd | 7 | Typ | 1 | Gd | Attchd | 2006.0 | RFn | 3 | 840 | TA | TA | Y | 160 | 33 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 8 | 2007 | New | Partial | . 15 20 | RL | NaN | 10920 | Pave | NaN | IR1 | Lvl | AllPub | Corner | Gtl | NAmes | Norm | Norm | 1Fam | 1Story | 6 | 5 | 1960 | 1960 | Hip | CompShg | MetalSd | MetalSd | BrkFace | 212.0 | TA | TA | CBlock | TA | TA | No | BLQ | 733 | Unf | 0 | 520 | 1253 | GasA | TA | Y | SBrkr | 1253 | 0 | 0 | 1253 | 1 | 0 | 1 | 1 | 2 | 1 | TA | 5 | Typ | 1 | Fa | Attchd | 1960.0 | RFn | 1 | 352 | TA | TA | Y | 0 | 213 | 176 | 0 | 0 | 0 | NaN | GdWo | NaN | 0 | 5 | 2008 | WD | Normal | . X_train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1460 entries, 1 to 1460 Data columns (total 79 columns): # Column Non-Null Count Dtype -- -- 0 MSSubClass 1460 non-null object 1 MSZoning 1460 non-null object 2 LotFrontage 1201 non-null float64 3 LotArea 1460 non-null int64 4 Street 1460 non-null object 5 Alley 91 non-null object 6 LotShape 1460 non-null object 7 LandContour 1460 non-null object 8 Utilities 1460 non-null object 9 LotConfig 1460 non-null object 10 LandSlope 1460 non-null object 11 Neighborhood 1460 non-null object 12 Condition1 1460 non-null object 13 Condition2 1460 non-null object 14 BldgType 1460 non-null object 15 HouseStyle 1460 non-null object 16 OverallQual 1460 non-null int64 17 OverallCond 1460 non-null int64 18 YearBuilt 1460 non-null int64 19 YearRemodAdd 1460 non-null int64 20 RoofStyle 1460 non-null object 21 RoofMatl 1460 non-null object 22 Exterior1st 1460 non-null object 23 Exterior2nd 1460 non-null object 24 MasVnrType 1452 non-null object 25 MasVnrArea 1452 non-null float64 26 ExterQual 1460 non-null object 27 ExterCond 1460 non-null object 28 Foundation 1460 non-null object 29 BsmtQual 1423 non-null object 30 BsmtCond 1423 non-null object 31 BsmtExposure 1422 non-null object 32 BsmtFinType1 1423 non-null object 33 BsmtFinSF1 1460 non-null int64 34 BsmtFinType2 1422 non-null object 35 BsmtFinSF2 1460 non-null int64 36 BsmtUnfSF 1460 non-null int64 37 TotalBsmtSF 1460 non-null int64 38 Heating 1460 non-null object 39 HeatingQC 1460 non-null object 40 CentralAir 1460 non-null object 41 Electrical 1459 non-null object 42 1stFlrSF 1460 non-null int64 43 2ndFlrSF 1460 non-null int64 44 LowQualFinSF 1460 non-null int64 45 GrLivArea 1460 non-null int64 46 BsmtFullBath 1460 non-null int64 47 BsmtHalfBath 1460 non-null int64 48 FullBath 1460 non-null int64 49 HalfBath 1460 non-null int64 50 BedroomAbvGr 1460 non-null int64 51 KitchenAbvGr 1460 non-null int64 52 KitchenQual 1460 non-null object 53 TotRmsAbvGrd 1460 non-null int64 54 Functional 1460 non-null object 55 Fireplaces 1460 non-null int64 56 FireplaceQu 770 non-null object 57 GarageType 1379 non-null object 58 GarageYrBlt 1379 non-null float64 59 GarageFinish 1379 non-null object 60 GarageCars 1460 non-null int64 61 GarageArea 1460 non-null int64 62 GarageQual 1379 non-null object 63 GarageCond 1379 non-null object 64 PavedDrive 1460 non-null object 65 WoodDeckSF 1460 non-null int64 66 OpenPorchSF 1460 non-null int64 67 EnclosedPorch 1460 non-null int64 68 3SsnPorch 1460 non-null int64 69 ScreenPorch 1460 non-null int64 70 PoolArea 1460 non-null int64 71 PoolQC 7 non-null object 72 Fence 281 non-null object 73 MiscFeature 54 non-null object 74 MiscVal 1460 non-null int64 75 MoSold 1460 non-null int64 76 YrSold 1460 non-null int64 77 SaleType 1460 non-null object 78 SaleCondition 1460 non-null object dtypes: float64(3), int64(32), object(44) memory usage: 944.8+ KB . Observations . There are 1460 entries with 79 features. | Some features have null values, and thus will need further inspection. | Some features can be accumulated to give new features. For eg. totalling different types of rooms to give a feature total_rooms. | . Null Values . # distribution of null values plt.figure(figsize = (25, 10)) sns.heatmap(X_train.isnull(), yticklabels = &quot;&quot;) plt.title(&quot;Distribution of null values&quot;) plt.show() . # count of null values null_count = X_train.isnull().sum() null_count = null_count.to_frame(name = &quot;null_values&quot;)[null_count &gt; 0] null_count[&quot;null_percentage&quot;] = null_count[&quot;null_values&quot;]/len(X_train)*100 null_vals = null_count.sort_values(by = [&quot;null_values&quot;], ascending = False) null_vals . null_values null_percentage . PoolQC 1453 | 99.520548 | . MiscFeature 1406 | 96.301370 | . Alley 1369 | 93.767123 | . Fence 1179 | 80.753425 | . FireplaceQu 690 | 47.260274 | . LotFrontage 259 | 17.739726 | . GarageType 81 | 5.547945 | . GarageYrBlt 81 | 5.547945 | . GarageFinish 81 | 5.547945 | . GarageQual 81 | 5.547945 | . GarageCond 81 | 5.547945 | . BsmtExposure 38 | 2.602740 | . BsmtFinType2 38 | 2.602740 | . BsmtFinType1 37 | 2.534247 | . BsmtCond 37 | 2.534247 | . BsmtQual 37 | 2.534247 | . MasVnrArea 8 | 0.547945 | . MasVnrType 8 | 0.547945 | . Electrical 1 | 0.068493 | . Observations . PoolQC, MiscFeature, Alley and Fence have a lot of null values. | Fortunately, the data description tells us that in all of the categorical feature columns bar one, null values indicate absence of those feature condition in the entries. Therefore, we&#39;ll impute null values in these feaure columns with &#39;None&#39;. | Electrical is the only feature column where null values don&#39;t indicate absence of any condition. But because there is only 1 null entry, we can drop this entry. | With regards to numerical columns, MasVnrArea and LotFrontage can be filled with 0 because here too, null values indicate absence of the conditions. GarageYrBlt can be filled with the median value, because although it is linked with the GarageCond column, absence of garage cannot have year 0 in GarageYrBlt. | For newer null value columns in the test dataset, we&#39;ll choose to impute them with either most_frequent(Categorical) or median(Numeric). | . # column dtypes X_train.dtypes.value_counts() . object 44 int64 32 float64 3 dtype: int64 . Handle Null values . null_cols = null_vals.index null_cols = null_cols.drop(&quot;Electrical&quot;) # drop null from `Electrical` def drop_electrical_null(X, y): drop_idx = X[X[&quot;Electrical&quot;].isnull()].index X = X.drop(drop_idx) y = y.drop(drop_idx) return X, y # categorical columns X_train[&#39;MSSubClass&#39;] = X_train[&#39;MSSubClass&#39;].apply(str) cat_cols = X_train.select_dtypes(include = [&quot;object&quot;]).columns cat_null_cols = null_cols.intersection(cat_cols) cat_null_cols_imp = SimpleImputer(strategy = &quot;constant&quot;, fill_value = &quot;None&quot;) cat_not_null_cols = cat_cols.difference(cat_null_cols) cat_not_null_coll_imp = SimpleImputer(strategy = &quot;most_frequent&quot;) cat_null_ct = make_column_transformer((cat_null_cols_imp, cat_null_cols), (cat_not_null_coll_imp, cat_not_null_cols)) # numeric columns num_cols = X_train.select_dtypes(exclude = [&quot;object&quot;]).columns num_null0_cols = pd.Index([&quot;MasVnrArea&quot;, &quot;LotFrontage&quot;]) num_null0_cols_imp = SimpleImputer(strategy = &quot;constant&quot;, fill_value = 0) num_not_null_cols = num_cols.difference(num_null0_cols) num_not_null_cols_imp = SimpleImputer(strategy = &quot;median&quot;) num_null_ct = make_column_transformer((num_null0_cols_imp, num_null0_cols), (num_not_null_cols_imp, num_not_null_cols)) # combine both into a common transformer null_ct = make_column_transformer((cat_null_ct, cat_cols), (num_null_ct, num_cols)) null_ct_features_in = cat_null_cols.append(cat_not_null_cols).append(num_null0_cols).append(num_not_null_cols) . Distribution and Outliers . # SalePrice (target) sns.distplot(y_train, kde = True) plt.title(&quot;Distribution of Sale Price&quot;) plt.show() . The target looks skewed. We&#39;ll normalise it. . # normalized SalePrice (target) y_train = np.log1p(y_train) sns.distplot(y_train, fit = norm, kde = True) plt.title(&quot;Distribution of Sale Price&quot;) plt.show() . The target is normalized. Now, we can look at the numerical features. . plt.figure(figsize = (12,10)) fig = sns.boxplot(data = X_train[num_cols], orient = &#39;h&#39;) fig.set_xscale(&quot;log&quot;) . There are quite a few outliers in the numeric columns. We&#39;ll need to scale numeric data using RobustScaler in the preprocessor building section. Also, many of the numeric features are skewed and will need to be normalized so that we can train ML models which assume normality in the features. . # features which deviate a lot from the normal curve imputed_features = pd.DataFrame(num_null_ct.fit_transform(X_train[num_cols])) num_features_in = num_null0_cols.append(num_not_null_cols) imputed_features.columns = num_features_in skew_features = imputed_features.apply(lambda x: skew(x)).sort_values(ascending = False) high_skew = skew_features.loc[skew_features &gt; 0.5] skewed_index = high_skew.index high_skew . MiscVal 24.451640 PoolArea 14.813135 LotArea 12.195142 3SsnPorch 10.293752 LowQualFinSF 9.002080 KitchenAbvGr 4.483784 BsmtFinSF2 4.250888 ScreenPorch 4.117977 BsmtHalfBath 4.099186 EnclosedPorch 3.086696 MasVnrArea 2.674865 OpenPorchSF 2.361912 BsmtFinSF1 1.683771 WoodDeckSF 1.539792 TotalBsmtSF 1.522688 1stFlrSF 1.375342 GrLivArea 1.365156 BsmtUnfSF 0.919323 2ndFlrSF 0.812194 OverallCond 0.692355 TotRmsAbvGrd 0.675646 HalfBath 0.675203 Fireplaces 0.648898 BsmtFullBath 0.595454 dtype: float64 . # normalize the skewed features norma = Normalizer() normalized_cols = pd.DataFrame(norma.fit_transform(imputed_features[skewed_index])) normalized_cols.columns = skewed_index normalized_cols.head() . MiscVal PoolArea LotArea 3SsnPorch LowQualFinSF KitchenAbvGr BsmtFinSF2 ScreenPorch BsmtHalfBath EnclosedPorch MasVnrArea OpenPorchSF BsmtFinSF1 WoodDeckSF TotalBsmtSF 1stFlrSF GrLivArea BsmtUnfSF 2ndFlrSF OverallCond TotRmsAbvGrd HalfBath Fireplaces BsmtFullBath . 0 0.0 | 0.0 | 0.962439 | 0.0 | 0.0 | 0.000114 | 0.0 | 0.0 | 0.000000 | 0.00000 | 0.022324 | 0.006948 | 0.080412 | 0.000000 | 0.097497 | 0.097497 | 0.194766 | 0.017085 | 0.097269 | 0.000569 | 0.000911 | 0.000114 | 0.000000 | 0.000114 | . 1 0.0 | 0.0 | 0.969430 | 0.0 | 0.0 | 0.000101 | 0.0 | 0.0 | 0.000101 | 0.00000 | 0.000000 | 0.000000 | 0.098761 | 0.030093 | 0.127440 | 0.127440 | 0.127440 | 0.028679 | 0.000000 | 0.000808 | 0.000606 | 0.000000 | 0.000101 | 0.000000 | . 2 0.0 | 0.0 | 0.976793 | 0.0 | 0.0 | 0.000087 | 0.0 | 0.0 | 0.000000 | 0.00000 | 0.014066 | 0.003647 | 0.042197 | 0.000000 | 0.079880 | 0.079880 | 0.155071 | 0.037683 | 0.075191 | 0.000434 | 0.000521 | 0.000087 | 0.000087 | 0.000087 | . 3 0.0 | 0.0 | 0.971507 | 0.0 | 0.0 | 0.000102 | 0.0 | 0.0 | 0.000000 | 0.02767 | 0.000000 | 0.003560 | 0.021973 | 0.000000 | 0.076907 | 0.097761 | 0.174668 | 0.054933 | 0.076907 | 0.000509 | 0.000712 | 0.000000 | 0.000102 | 0.000102 | . 4 0.0 | 0.0 | 0.977664 | 0.0 | 0.0 | 0.000069 | 0.0 | 0.0 | 0.000000 | 0.00000 | 0.023996 | 0.005759 | 0.044907 | 0.013163 | 0.078501 | 0.078501 | 0.150695 | 0.033594 | 0.072194 | 0.000343 | 0.000617 | 0.000069 | 0.000069 | 0.000069 | . plt.figure(figsize = (12, 10)) fig = sns.boxplot(data = normalized_cols, orient = &quot;h&quot;) fig.set_xscale(&quot;log&quot;) . Some features like MasVnrArea, OpenPorchSF, BsmtFinSF1, WoodDeckDF, 2ndFlrSF couldn&#39;t be normalized. These columns will thus need to be dropped after extracting important information from them, which we&#39;ll do in the feature engineering section. . Feature Engineering . Area related features are important in predicting sale price of houses, and thus we can engineer some features related to area. We can also add some binary features indicating presence of swimming pool, garage or fireplace, which also are an important determiners of real estate value. . def add_cols(df): # Add area related features df[&#39;TotalSF&#39;] = df[&#39;TotalBsmtSF&#39;] + df[&#39;1stFlrSF&#39;] + df[&#39;2ndFlrSF&#39;] df[&#39;Total_Bathrooms&#39;] = (df[&#39;FullBath&#39;] + (0.5 * df[&#39;HalfBath&#39;]) + df[&#39;BsmtFullBath&#39;] + (0.5 * df[&#39;BsmtHalfBath&#39;])) df[&#39;Total_porch_sf&#39;] = (df[&#39;OpenPorchSF&#39;] + df[&#39;3SsnPorch&#39;] + df[&#39;EnclosedPorch&#39;] + df[&#39;ScreenPorch&#39;] + df[&#39;WoodDeckSF&#39;]) # Add simplified categorical features df[&#39;haspool&#39;] = df[&#39;PoolArea&#39;].apply(lambda x: 1 if x &gt; 0 else 0) df[&#39;hasgarage&#39;] = df[&#39;GarageArea&#39;].apply(lambda x: 1 if x &gt; 0 else 0) df[&#39;hasbsmt&#39;] = df[&#39;TotalBsmtSF&#39;].apply(lambda x: 1 if x &gt; 0 else 0) df[&#39;hasfireplace&#39;] = df[&#39;Fireplaces&#39;].apply(lambda x: 1 if x &gt; 0 else 0) df[[&quot;haspool&quot;, &quot;hasgarage&quot;, &quot;hasbsmt&quot;, &quot;hasfireplace&quot;]] = df[[&quot;haspool&quot;, &quot;hasgarage&quot;, &quot;hasbsmt&quot;, &quot;hasfireplace&quot;]].astype(&quot;object&quot;) return df . Prepare the data . Now, we will combine all we have learnt and done in the previous sections to process the data on which an ML algorithm can be trained. We&#39;ll . normalize target | impute null values | add features | drop features | onehotencode categorical features | normalize skewed numeric features | scale numeric features | . # function to preprocess the data def get_prepared_data(transform_numeric = True): X_trn = train.copy() y_trn = X_trn.pop(&quot;SalePrice&quot;) X_tst = test.copy() X_trn[&#39;MSSubClass&#39;] = X_trn[&#39;MSSubClass&#39;].astype(&quot;object&quot;) X_tst[&#39;MSSubClass&#39;] = X_tst[&#39;MSSubClass&#39;].astype(&quot;object&quot;) # normalize target y_trn = np.log1p(y_trn) # handle null values X_trn, y_trn = drop_electrical_null(X_trn, y_trn) X_trn = pd.DataFrame(null_ct.fit_transform(X_trn)) X_tst = pd.DataFrame(null_ct.transform(X_tst)) X_trn = X_trn.infer_objects() X_tst = X_tst.infer_objects() # re add column names X_trn.columns = null_ct_features_in X_tst.columns = null_ct_features_in X_trn[&#39;MSSubClass&#39;] = X_trn[&#39;MSSubClass&#39;].astype(&quot;object&quot;) X_tst[&#39;MSSubClass&#39;] = X_tst[&#39;MSSubClass&#39;].astype(&quot;object&quot;) # add features X_trn = add_cols(X_trn) X_tst = add_cols(X_tst) # drop features X_trn.drop(columns = [&quot;MasVnrArea&quot;, &quot;OpenPorchSF&quot;, &quot;BsmtFinSF1&quot;, &quot;WoodDeckSF&quot;, &quot;2ndFlrSF&quot;], inplace = True) X_tst.drop(columns = [&quot;MasVnrArea&quot;, &quot;OpenPorchSF&quot;, &quot;BsmtFinSF1&quot;, &quot;WoodDeckSF&quot;, &quot;2ndFlrSF&quot;], inplace = True) # categorical features cat_cols = X_trn.select_dtypes(include = [&quot;object&quot;]).columns cat_ohe = OneHotEncoder(drop = &#39;first&#39;, handle_unknown = &#39;ignore&#39;, sparse = False, dtype = &#39;uint8&#39;) # normalize numeric features num_cols = X_trn.select_dtypes(exclude = [&quot;object&quot;]).columns num_pipe = make_pipeline(Normalizer(), RobustScaler()) # column transformer if transform_numeric: ct = make_column_transformer((cat_ohe, cat_cols), (num_pipe, num_cols)) else: ct = make_column_transformer((cat_ohe, cat_cols), remainder = &quot;passthrough&quot;) X_trn = pd.DataFrame(ct.fit_transform(X_trn)) X_tst = pd.DataFrame(ct.transform(X_tst)) return X_trn, y_trn, X_tst . # get the processed data X_trn, y_trn, X_tst = get_prepared_data(True) . Now, we can move on to ML model training. . Train a Baseline Model and Evaluate Performance . We&#39;ll train a dummy classifier to establish a baseline score. This will give us a score value, which our future models should at least beat. It helps to identify errors in training. . # define model base_model = DummyRegressor() . # model evaluation def evaluate_model(model, X_trn = X_trn): cvs = cross_val_score(model, X_trn, y_trn, scoring = &quot;neg_mean_squared_error&quot;) rmsle = np.sqrt(-cvs.mean()) print(f&quot;Model RMSLE: {rmsle:.5f}&quot;) evaluate_model(base_model) . Model RMSLE: 0.39968 . Make submission . # train model base_model.fit(X_trn, y_trn) # make predictions base_preds = base_model.predict(X_tst) . # create submission file submission_df[&quot;SalePrice&quot;] = base_preds submission_df.to_csv(&quot;baseline_model.csv&quot;, index = None) . The submission gives use a score of 0.42578. The future ML models should at least beat the cv rmsle score of 0.39968 and submission rmsle score of 0.42578. . Linear Models . First we&#39;ll train some linear models and compare their performance and make decision on training a final one. . # cv splitter k_folds = KFold(5, shuffle = True, random_state = seed) # parameters for cv e_alphas = np.arange(0.0001, 0.0007, 0.0001) e_l1ratio = np.arange(0.8, 1, 0.05) alphas_ridge = np.arange(10, 16, 0.5) alphas_lasso = np.arange(0.0001, 0.0008, 0.0001) . # linear models ridge = RidgeCV(alphas = alphas_ridge, scoring = &quot;neg_mean_squared_error&quot;, cv = k_folds) lasso = LassoCV(alphas = alphas_lasso, max_iter = 1e6, cv = k_folds, n_jobs = -1, random_state = seed) elastic_net = ElasticNetCV(l1_ratio = e_l1ratio, alphas = e_alphas, max_iter = 1e6, cv = k_folds, n_jobs = -1, random_state = seed) models = {&quot;Ridge&quot;: ridge, &quot;Lasso&quot;: lasso, &quot;ElasticNet&quot;: elastic_net} . # compare linear models scores = {} for model_name, model in models.items(): print(f&quot;{model_name}:&quot;) score = np.sqrt(-cross_val_score(model, X_trn, y_trn, scoring = &quot;neg_mean_squared_error&quot;, cv = k_folds)) print(score) print(f&quot;RMSLE mean: {score.mean():.5f} nRMSLE std: {score.std():.5f}&quot;) print(&quot;-&quot; * 50) scores[model_name] = (score.mean(), score.std()) . Ridge: [0.13595902 0.15096778 0.13826833 0.13605681 0.12490943] RMSLE mean: 0.13723 RMSLE std: 0.00830 -- Lasso: [0.13737072 0.15284838 0.13888969 0.13586582 0.12556263] RMSLE mean: 0.13811 RMSLE std: 0.00873 -- ElasticNet: [0.13721347 0.15238829 0.13744699 0.13594653 0.12550764] RMSLE mean: 0.13770 RMSLE std: 0.00858 -- . All these scores are improvements over baseline score and achieve rmsle scores in the range of 0.12-0.15. We can get a better score by blending all these linear models. Blending makes theses models complement each other and reduce their individual overfits. . Blended Model . %%time print(&quot;training started...&quot;) # train all models ridge.fit(X_trn, y_trn) lasso.fit(X_trn, y_trn) elastic_net.fit(X_trn, y_trn) print(&quot;training complete&quot;) . training started... training complete CPU times: total: 8.7 s Wall time: 2.62 s . Make Submission . # make predictions blended_preds = (ridge.predict(X_tst) + lasso.predict(X_tst) + elastic_net.predict(X_tst))/3 blended_preds = np.expm1(blended_preds) . # create submission file submission_df[&quot;SalePrice&quot;] = blended_preds submission_df.to_csv(&quot;blended_linear.csv&quot;, index = None) . This submission from blended linear models give us a rmsle score of 0.13819 which is similar to the performance of a single lasso model. But we can improve more on this score by using gradient boosting trees, which we&#39;ll do in the next section. . Gradient Boosting Models . In this section, first we&#39;ll train a lightgbm model and then we&#39;ll train an xgboost model. Normalizing and scaling that we applied on numeric features earlier deteriorates the performance of gradient boosting trees, which can actually use the information lost through transformation. Therefore, To train these models, we&#39;ll reload the processed data, this time without transforming the numeric features. . # load processed data X_trn, y_trn, X_tst = get_prepared_data(False) . LightGBM - Train and Evaluate . # get cpu core count core_count = psutil.cpu_count(logical = False) core_count . 4 . # lightgbm parameters param = {&quot;bagging_fraction&quot;: 0.8, &quot;bagging_freq&quot;: 2, &quot;learning_rate&quot;: 0.01, &quot;num_leaves&quot;: 10, &quot;max_depth&quot;: 5, &quot;min_data_in_leaf&quot;: 10, &quot;metric&quot;: &quot;rmse&quot;, &quot;num_threads&quot;: core_count, &quot;verbosity&quot;: -1} . # train and evaluate lightgbm val_scores = [] i = 1 for trn_idx, val_idx in k_folds.split(X_trn, y_trn): print(f&quot;Split {i}:&quot;) trn = lgb.Dataset(X_trn.iloc[trn_idx], y_trn.iloc[trn_idx]) val = lgb.Dataset(X_trn.iloc[val_idx], y_trn.iloc[val_idx]) bst = lgb.train(param, trn, num_boost_round = 3000, valid_sets = [trn, val], early_stopping_rounds = 10, verbose_eval = 50) score = bst.best_score[&quot;valid_1&quot;][&quot;rmse&quot;] val_scores.append(score) print(f&quot;RMSLE: {score:.5f}&quot;) print(&quot;-&quot; * 65) i += 1 . Split 1: Training until validation scores don&#39;t improve for 10 rounds [50] training&#39;s rmse: 0.2779 valid_1&#39;s rmse: 0.294501 [100] training&#39;s rmse: 0.208419 valid_1&#39;s rmse: 0.22513 [150] training&#39;s rmse: 0.168045 valid_1&#39;s rmse: 0.186805 [200] training&#39;s rmse: 0.143163 valid_1&#39;s rmse: 0.164704 [250] training&#39;s rmse: 0.126932 valid_1&#39;s rmse: 0.151274 [300] training&#39;s rmse: 0.115682 valid_1&#39;s rmse: 0.14419 [350] training&#39;s rmse: 0.10737 valid_1&#39;s rmse: 0.139166 [400] training&#39;s rmse: 0.101333 valid_1&#39;s rmse: 0.135577 [450] training&#39;s rmse: 0.0965005 valid_1&#39;s rmse: 0.133299 [500] training&#39;s rmse: 0.0926217 valid_1&#39;s rmse: 0.131744 [550] training&#39;s rmse: 0.0891204 valid_1&#39;s rmse: 0.130774 [600] training&#39;s rmse: 0.0863924 valid_1&#39;s rmse: 0.12962 [650] training&#39;s rmse: 0.0838749 valid_1&#39;s rmse: 0.129178 Early stopping, best iteration is: [644] training&#39;s rmse: 0.0840888 valid_1&#39;s rmse: 0.129117 RMSLE: 0.12912 -- Split 2: Training until validation scores don&#39;t improve for 10 rounds [50] training&#39;s rmse: 0.273782 valid_1&#39;s rmse: 0.313671 [100] training&#39;s rmse: 0.205466 valid_1&#39;s rmse: 0.244458 [150] training&#39;s rmse: 0.165209 valid_1&#39;s rmse: 0.204575 [200] training&#39;s rmse: 0.140476 valid_1&#39;s rmse: 0.181242 [250] training&#39;s rmse: 0.124644 valid_1&#39;s rmse: 0.166237 [300] training&#39;s rmse: 0.113833 valid_1&#39;s rmse: 0.157775 [350] training&#39;s rmse: 0.105639 valid_1&#39;s rmse: 0.151577 [400] training&#39;s rmse: 0.0995413 valid_1&#39;s rmse: 0.148238 [450] training&#39;s rmse: 0.0946096 valid_1&#39;s rmse: 0.145141 Early stopping, best iteration is: [484] training&#39;s rmse: 0.0917289 valid_1&#39;s rmse: 0.144245 RMSLE: 0.14425 -- Split 3: Training until validation scores don&#39;t improve for 10 rounds [50] training&#39;s rmse: 0.27963 valid_1&#39;s rmse: 0.281509 [100] training&#39;s rmse: 0.208148 valid_1&#39;s rmse: 0.220384 [150] training&#39;s rmse: 0.16596 valid_1&#39;s rmse: 0.188637 [200] training&#39;s rmse: 0.139878 valid_1&#39;s rmse: 0.171665 [250] training&#39;s rmse: 0.123095 valid_1&#39;s rmse: 0.162633 [300] training&#39;s rmse: 0.11184 valid_1&#39;s rmse: 0.158664 [350] training&#39;s rmse: 0.103994 valid_1&#39;s rmse: 0.156169 [400] training&#39;s rmse: 0.0980702 valid_1&#39;s rmse: 0.154727 [450] training&#39;s rmse: 0.0935271 valid_1&#39;s rmse: 0.153839 Early stopping, best iteration is: [466] training&#39;s rmse: 0.0922914 valid_1&#39;s rmse: 0.15353 RMSLE: 0.15353 -- Split 4: Training until validation scores don&#39;t improve for 10 rounds [50] training&#39;s rmse: 0.279398 valid_1&#39;s rmse: 0.284018 [100] training&#39;s rmse: 0.208896 valid_1&#39;s rmse: 0.220487 [150] training&#39;s rmse: 0.16791 valid_1&#39;s rmse: 0.185005 [200] training&#39;s rmse: 0.14287 valid_1&#39;s rmse: 0.165001 [250] training&#39;s rmse: 0.126724 valid_1&#39;s rmse: 0.153032 [300] training&#39;s rmse: 0.115538 valid_1&#39;s rmse: 0.145386 [350] training&#39;s rmse: 0.107436 valid_1&#39;s rmse: 0.140329 [400] training&#39;s rmse: 0.10126 valid_1&#39;s rmse: 0.136382 [450] training&#39;s rmse: 0.096186 valid_1&#39;s rmse: 0.133607 [500] training&#39;s rmse: 0.0921356 valid_1&#39;s rmse: 0.131694 [550] training&#39;s rmse: 0.0886764 valid_1&#39;s rmse: 0.12993 [600] training&#39;s rmse: 0.0858351 valid_1&#39;s rmse: 0.129052 Early stopping, best iteration is: [593] training&#39;s rmse: 0.0862509 valid_1&#39;s rmse: 0.129002 RMSLE: 0.12900 -- Split 5: Training until validation scores don&#39;t improve for 10 rounds [50] training&#39;s rmse: 0.286016 valid_1&#39;s rmse: 0.2487 [100] training&#39;s rmse: 0.21402 valid_1&#39;s rmse: 0.188862 [150] training&#39;s rmse: 0.171467 valid_1&#39;s rmse: 0.156549 [200] training&#39;s rmse: 0.145601 valid_1&#39;s rmse: 0.139452 [250] training&#39;s rmse: 0.129239 valid_1&#39;s rmse: 0.130076 [300] training&#39;s rmse: 0.117925 valid_1&#39;s rmse: 0.124245 [350] training&#39;s rmse: 0.109824 valid_1&#39;s rmse: 0.120353 [400] training&#39;s rmse: 0.103684 valid_1&#39;s rmse: 0.118195 [450] training&#39;s rmse: 0.0986858 valid_1&#39;s rmse: 0.116684 [500] training&#39;s rmse: 0.0947134 valid_1&#39;s rmse: 0.115513 [550] training&#39;s rmse: 0.0912204 valid_1&#39;s rmse: 0.114181 [600] training&#39;s rmse: 0.0884721 valid_1&#39;s rmse: 0.113515 Early stopping, best iteration is: [632] training&#39;s rmse: 0.0869814 valid_1&#39;s rmse: 0.113224 RMSLE: 0.11322 -- . . # Avg RMSLE np.mean(val_scores) . 0.13382349293051415 . Even without hyperparameter tuning, the validation scores are better than those of linear models. Now, we&#39;ll train on the whole dataset and make a submission. . trn = lgb.Dataset(X_trn, y_trn) lgb_cv = lgb.cv(param, trn, num_boost_round = 3000, folds = k_folds, early_stopping_rounds = 10) lgb_cv[&quot;rmse-mean&quot;][-1] . 0.13197370164429242 . # train on full data bst = lgb.train(param, trn, num_boost_round = len(lgb_cv[&quot;rmse-mean&quot;])) # make predictions lgb_preds = np.expm1(bst.predict(X_tst)) . Make submission . submission_df[&quot;SalePrice&quot;] = lgb_preds submission_df.to_csv(&quot;lgb.csv&quot;, index = None) . This submission gives us a score of 0.12901, which is an improvement over the last submission. Now, we can further optimize it with hyperparameter tuning. . LightGBM - Hyperparameter tuning . We&#39;ll use Bayesian Optimization to tune the hyperparameters in this section and then we&#39;ll make a submssion. . # black box function for Bayesian Optimization def LGB_bayesian(bagging_fraction, bagging_freq, lambda_l1, lambda_l2, learning_rate, max_depth, min_data_in_leaf, min_gain_to_split, min_sum_hessian_in_leaf, num_leaves, feature_fraction): # LightGBM expects these parameters to be integer. So we make them integer bagging_freq = int(bagging_freq) num_leaves = int(num_leaves) min_data_in_leaf = int(min_data_in_leaf) max_depth = int(max_depth) # parameters param = {&#39;bagging_fraction&#39;: bagging_fraction, &#39;bagging_freq&#39;: bagging_freq, &#39;lambda_l1&#39;: lambda_l1, &#39;lambda_l2&#39;: lambda_l2, &#39;learning_rate&#39;: learning_rate, &#39;max_depth&#39;: max_depth, &#39;min_data_in_leaf&#39;: min_data_in_leaf, &#39;min_gain_to_split&#39;: min_gain_to_split, &#39;min_sum_hessian_in_leaf&#39;: min_sum_hessian_in_leaf, &#39;num_leaves&#39;: num_leaves, &#39;feature_fraction&#39;: feature_fraction, &#39;seed&#39;: seed, &#39;feature_fraction_seed&#39;: seed, &#39;bagging_seed&#39;: seed, &#39;drop_seed&#39;: seed, &#39;boosting_type&#39;: &#39;gbdt&#39;, &#39;metric&#39;: &#39;rmse&#39;, &#39;verbosity&#39;: -1, &#39;num_threads&#39;: core_count} trn = lgb.Dataset(X_trn, y_trn) lgb_cv = lgb.cv(param, trn, num_boost_round = 1500, folds = k_folds, stratified = False, early_stopping_rounds = 10, seed = seed) score = lgb_cv[&quot;rmse-mean&quot;][-1] return 1/score . # parameter bounds bounds_LGB = { &#39;bagging_fraction&#39;: (0.5, 1), &#39;bagging_freq&#39;: (1, 4), &#39;lambda_l1&#39;: (0, 3.0), &#39;lambda_l2&#39;: (0, 3.0), &#39;learning_rate&#39;: (0.005, 0.3), &#39;max_depth&#39;:(3,8), &#39;min_data_in_leaf&#39;: (5, 20), &#39;min_gain_to_split&#39;: (0, 1), &#39;min_sum_hessian_in_leaf&#39;: (0.01, 20), &#39;num_leaves&#39;: (5, 20), &#39;feature_fraction&#39;: (0.05, 1) } . # optimizer LG_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state = seed) . # find the best hyperparameters LG_BO.maximize(init_points = 10, n_iter = 200) . | iter | target | baggin... | baggin... | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | min_da... | min_ga... | min_su... | num_le... | - | 1 | 6.338 | 0.6873 | 3.852 | 0.7454 | 1.796 | 0.4681 | 0.05102 | 3.29 | 17.99 | 0.6011 | 14.16 | 5.309 | | 2 | 6.913 | 0.985 | 3.497 | 0.2517 | 0.5455 | 0.5502 | 0.09475 | 5.624 | 11.48 | 0.2912 | 12.24 | 7.092 | | 3 | 6.262 | 0.6461 | 2.099 | 0.4833 | 2.356 | 0.599 | 0.1567 | 5.962 | 5.697 | 0.6075 | 3.419 | 5.976 | | 4 | 6.481 | 0.9744 | 3.897 | 0.818 | 0.9138 | 0.293 | 0.2068 | 5.201 | 6.831 | 0.4952 | 0.6974 | 18.64 | | 5 | 6.091 | 0.6294 | 2.988 | 0.3461 | 1.56 | 1.64 | 0.05953 | 7.848 | 16.63 | 0.9395 | 17.9 | 13.97 | | 6 | 7.011 | 0.9609 | 1.265 | 0.2362 | 0.1357 | 0.976 | 0.1197 | 4.357 | 17.43 | 0.3568 | 5.626 | 13.14 | | 7 | 5.948 | 0.5705 | 3.407 | 0.1208 | 2.961 | 2.317 | 0.06362 | 3.028 | 17.23 | 0.7069 | 14.58 | 16.57 | | 8 | 6.32 | 0.537 | 2.075 | 0.1601 | 2.589 | 1.87 | 0.1026 | 3.318 | 9.665 | 0.3252 | 14.59 | 14.56 | | 9 | 6.456 | 0.9436 | 2.417 | 0.1636 | 2.14 | 2.282 | 0.1706 | 6.855 | 12.41 | 0.5227 | 8.557 | 5.381 | | 10 | 6.361 | 0.5539 | 1.094 | 0.6546 | 0.9431 | 1.526 | 0.2727 | 4.246 | 11.16 | 0.7556 | 4.584 | 6.155 | | 11 | 7.721 | 1.0 | 2.125 | 0.1412 | 0.0 | 0.0 | 0.06735 | 5.043 | 15.68 | 0.0 | 7.024 | 11.51 | | 12 | 7.152 | 1.0 | 3.976 | 0.05 | 0.0 | 0.0 | 0.005 | 6.074 | 15.12 | 0.0 | 7.333 | 11.65 | | 13 | 7.117 | 1.0 | 1.0 | 0.05 | 0.0 | 0.0 | 0.005 | 4.35 | 15.28 | 0.0 | 8.484 | 10.99 | | 14 | 7.192 | 1.0 | 1.735 | 0.3443 | 0.0 | 0.0 | 0.3 | 5.697 | 15.98 | 0.0 | 5.747 | 10.45 | | 15 | 7.484 | 1.0 | 2.551 | 0.1004 | 0.0 | 0.0 | 0.005 | 3.799 | 14.85 | 0.0 | 6.269 | 12.16 | | 16 | 7.229 | 1.0 | 1.369 | 0.05 | 0.0 | 0.0 | 0.005 | 5.841 | 14.82 | 0.0 | 6.828 | 13.1 | | 17 | 7.598 | 0.5 | 3.165 | 1.0 | 0.0 | 0.0 | 0.005 | 3.953 | 16.9 | 0.0 | 7.403 | 11.63 | | 18 | 6.571 | 1.0 | 2.755 | 0.05 | 2.317 | 0.0 | 0.005 | 4.365 | 16.39 | 0.0 | 7.109 | 11.7 | | 19 | 7.101 | 0.5 | 2.554 | 1.0 | 0.0 | 1.657 | 0.3 | 4.573 | 15.56 | 0.0 | 7.148 | 11.65 | | 20 | 7.5 | 1.0 | 2.199 | 1.0 | 0.0 | 0.0 | 0.005 | 5.806 | 17.47 | 0.0 | 7.955 | 12.02 | | 21 | 7.663 | 0.5 | 3.658 | 1.0 | 0.0 | 0.0 | 0.005 | 4.533 | 19.86 | 0.0 | 7.692 | 10.71 | | 22 | 6.163 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.511 | 20.0 | 1.0 | 8.371 | 12.92 | | 23 | 7.701 | 0.5 | 3.072 | 1.0 | 0.0 | 0.0 | 0.005 | 5.102 | 18.32 | 0.0 | 7.531 | 9.631 | | 24 | 7.628 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 6.035 | 20.0 | 0.0 | 6.098 | 9.744 | | 25 | 7.427 | 1.0 | 3.651 | 1.0 | 0.0 | 0.0 | 0.005 | 3.433 | 20.0 | 0.0 | 6.478 | 8.486 | | 26 | 7.494 | 1.0 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 6.458 | 20.0 | 0.0 | 8.892 | 8.96 | | 27 | 7.722 | 0.5 | 1.094 | 1.0 | 0.0 | 0.0 | 0.005 | 5.619 | 20.0 | 0.0 | 7.365 | 9.731 | | 28 | 7.016 | 0.5 | 2.615 | 0.05 | 0.0 | 2.514 | 0.005 | 5.673 | 20.0 | 0.0 | 7.401 | 9.38 | | 29 | 7.696 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 6.844 | 6.929 | | 30 | 7.641 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 3.315 | 5.912 | | 31 | 6.112 | 0.5 | 3.916 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 1.0 | 5.31 | 5.0 | | 32 | 7.713 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 4.486 | 9.099 | | 33 | 7.736 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 0.4313 | 8.136 | | 34 | 7.696 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 4.89 | 20.0 | 0.0 | 2.015 | 7.902 | | 35 | 6.966 | 0.5 | 1.0 | 1.0 | 3.0 | 0.0 | 0.005 | 7.259 | 20.0 | 0.0 | 1.833 | 7.665 | | 36 | 7.658 | 0.5 | 1.0 | 1.0 | 0.0 | 2.869 | 0.005 | 6.494 | 20.0 | 0.0 | 0.01 | 6.427 | | 37 | 7.51 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 5.559 | 20.0 | 0.0 | 0.01 | 5.0 | | 38 | 7.705 | 0.5 | 1.0 | 1.0 | 0.0 | 2.56 | 0.005 | 6.076 | 20.0 | 0.0 | 0.01 | 10.63 | | 39 | 7.714 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 0.01 | 12.97 | | 40 | 7.618 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 5.622 | 20.0 | 0.0 | 0.01 | 11.08 | | 41 | 7.543 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 20.0 | 0.0 | 0.01 | 13.77 | | 42 | 7.713 | 0.5 | 1.0 | 1.0 | 0.0 | 0.647 | 0.005 | 8.0 | 20.0 | 0.0 | 0.01 | 17.94 | | 43 | 6.926 | 0.5 | 3.143 | 1.0 | 3.0 | 3.0 | 0.005 | 8.0 | 20.0 | 0.0 | 0.01 | 19.14 | | 44 | 7.717 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 16.55 | 0.0 | 0.01 | 15.91 | | 45 | 7.703 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 4.411 | 19.77 | 0.0 | 0.01 | 15.51 | | 46 | 6.15 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 6.572 | 18.47 | 1.0 | 0.01 | 16.1 | | 47 | 7.585 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 3.0 | 20.0 | 0.0 | 0.01 | 13.47 | | 48 | 7.685 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 20.0 | 0.0 | 0.01 | 15.03 | | 49 | 7.52 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 15.19 | 0.0 | 0.01 | 18.18 | | 50 | 7.044 | 0.5 | 1.0 | 0.05 | 0.0 | 0.0 | 0.005 | 3.0 | 20.0 | 0.0 | 0.01 | 11.03 | | 51 | 7.751 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 16.68 | 0.0 | 0.01 | 12.46 | | 52 | 7.68 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 20.0 | 0.0 | 0.01 | 9.493 | | 53 | 7.652 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 20.0 | 0.0 | 2.526 | 12.02 | | 54 | 7.379 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 4.384 | 15.97 | 0.0 | 0.01 | 15.01 | | 55 | 6.92 | 0.5 | 1.0 | 1.0 | 3.0 | 3.0 | 0.005 | 8.0 | 19.11 | 0.0 | 0.01 | 12.79 | | 56 | 7.122 | 0.5 | 1.0 | 0.05 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 3.681 | 20.0 | | 57 | 6.481 | 0.9844 | 3.004 | 0.461 | 0.1131 | 0.5129 | 0.09865 | 7.624 | 17.4 | 0.9014 | 0.1111 | 9.671 | | 58 | 7.112 | 1.0 | 1.0 | 1.0 | 3.0 | 0.0 | 0.3 | 8.0 | 5.0 | 0.0 | 20.0 | 5.0 | | 59 | 7.555 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 3.979 | 20.0 | 0.0 | 0.01 | 8.798 | | 60 | 7.585 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 3.0 | 20.0 | 0.0 | 0.01 | 18.13 | | 61 | 6.396 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 5.0 | 1.0 | 10.39 | 20.0 | | 62 | 7.206 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.3 | 3.0 | 20.0 | 0.0 | 2.043 | 5.0 | | 63 | 7.122 | 0.5 | 1.0 | 0.05 | 0.0 | 3.0 | 0.3 | 3.0 | 15.63 | 0.0 | 0.01 | 20.0 | | 64 | 7.759 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 13.28 | 0.0 | 0.01 | 14.36 | | 65 | 6.188 | 0.5 | 1.0 | 0.05 | 0.0 | 0.0 | 0.3 | 8.0 | 12.68 | 0.0 | 0.01 | 16.17 | | 66 | 7.683 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 16.53 | 0.0 | 1.6 | 15.2 | | 67 | 6.994 | 1.0 | 1.0 | 1.0 | 3.0 | 0.0 | 0.3 | 3.0 | 20.0 | 0.0 | 0.01 | 18.09 | | 68 | 7.713 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 9.356 | 9.508 | | 69 | 6.969 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.3 | 5.246 | 20.0 | 0.0 | 1.954 | 15.77 | | 70 | 6.964 | 0.5 | 1.0 | 1.0 | 3.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 8.347 | 8.476 | | 71 | 7.497 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 12.7 | 0.0 | 0.01 | 11.39 | | 72 | 7.27 | 0.7003 | 2.9 | 0.9973 | 0.1094 | 2.975 | 0.09622 | 3.103 | 5.03 | 0.1283 | 19.92 | 5.92 | | 73 | 6.659 | 0.5 | 4.0 | 0.05 | 0.0 | 0.0 | 0.3 | 3.0 | 5.0 | 0.0 | 14.83 | 5.0 | | 74 | 7.147 | 0.5 | 1.0 | 1.0 | 2.124 | 3.0 | 0.005 | 8.0 | 14.89 | 0.0 | 0.01 | 14.45 | | 75 | 7.766 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 14.1 | 0.0 | 2.427 | 13.15 | | 76 | 7.597 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 5.19 | 20.0 | 0.0 | 0.01 | 11.81 | | 77 | 7.691 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 20.0 | 0.0 | 2.412 | 8.24 | | 78 | 5.994 | 1.0 | 4.0 | 1.0 | 3.0 | 3.0 | 0.3 | 8.0 | 5.0 | 1.0 | 20.0 | 20.0 | | 79 | 6.312 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.3 | 8.0 | 11.0 | 1.0 | 20.0 | 5.0 | | 80 | 7.384 | 1.0 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 3.0 | 5.0 | 0.0 | 20.0 | 11.43 | | 81 | 6.07 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 5.0 | 1.0 | 20.0 | 10.16 | | 82 | 7.696 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 11.45 | 6.089 | | 83 | 6.986 | 1.0 | 1.0 | 1.0 | 3.0 | 0.0 | 0.005 | 3.0 | 5.0 | 0.0 | 20.0 | 9.878 | | 84 | 6.952 | 0.5 | 1.0 | 0.05 | 3.0 | 0.0 | 0.3 | 8.0 | 20.0 | 0.0 | 20.0 | 5.0 | | 85 | 6.961 | 1.0 | 1.0 | 0.05 | 0.0 | 3.0 | 0.005 | 8.0 | 20.0 | 0.0 | 13.09 | 5.0 | | 86 | 7.093 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.3 | 8.0 | 20.0 | 0.0 | 13.16 | 9.104 | | 87 | 7.46 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 3.0 | 5.0 | 0.0 | 0.01 | 11.73 | | 88 | 7.469 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 5.0 | 0.0 | 0.01 | 12.26 | | 89 | 6.317 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 3.0 | 5.0 | 1.0 | 0.01 | 13.96 | | 90 | 6.38 | 0.5 | 4.0 | 0.05542 | 2.958 | 3.0 | 0.005 | 8.0 | 5.0 | 0.0 | 0.01 | 11.45 | | 91 | 7.706 | 0.5 | 3.738 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 13.74 | 0.0 | 0.9225 | 13.42 | | 92 | 7.185 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.3 | 5.567 | 20.0 | 0.0 | 9.453 | 6.733 | | 93 | 7.481 | 1.0 | 2.642 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 10.85 | 0.0 | 2.701 | 13.31 | | 94 | 7.577 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 3.931 | 11.69 | 0.0 | 0.01 | 12.14 | | 95 | 6.869 | 0.5 | 4.0 | 0.05 | 0.0 | 3.0 | 0.005 | 3.0 | 5.0 | 0.0 | 6.951 | 12.17 | | 96 | 6.988 | 1.0 | 4.0 | 1.0 | 3.0 | 3.0 | 0.3 | 3.0 | 10.07 | 0.0 | 0.01 | 10.4 | | 97 | 7.409 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 5.0 | 0.0 | 0.01 | 9.278 | | 98 | 6.397 | 0.5 | 1.611 | 1.0 | 0.0 | 3.0 | 0.005 | 5.695 | 12.98 | 1.0 | 1.15 | 13.16 | | 99 | 7.153 | 0.5 | 2.703 | 1.0 | 0.0 | 1.932 | 0.3 | 6.105 | 20.0 | 0.0 | 2.283 | 9.836 | | 100 | 7.564 | 1.0 | 1.0 | 1.0 | 0.0 | 1.399 | 0.005 | 8.0 | 18.0 | 0.0 | 0.01 | 14.38 | | 101 | 7.541 | 1.0 | 1.0 | 1.0 | 0.0 | 2.267 | 0.005 | 8.0 | 17.94 | 0.0 | 0.01 | 17.56 | | 102 | 7.714 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 3.316 | 12.91 | | 103 | 7.532 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 14.19 | 0.0 | 3.933 | 16.08 | | 104 | 7.661 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 10.81 | 0.0 | 0.01 | 11.59 | | 105 | 7.555 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 6.718 | 11.51 | | 106 | 7.6 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 20.0 | 0.0 | 0.01 | 6.529 | | 107 | 6.931 | 0.5 | 4.0 | 1.0 | 3.0 | 0.0 | 0.005 | 3.0 | 20.0 | 0.0 | 0.01 | 5.0 | | 108 | 7.155 | 1.0 | 1.0 | 0.05 | 0.0 | 3.0 | 0.005 | 8.0 | 20.0 | 0.0 | 0.01 | 9.308 | | 109 | 7.771 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 15.34 | 0.0 | 3.898 | 20.0 | | 110 | 7.761 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 14.25 | 0.0 | 6.871 | 20.0 | | 111 | 6.885 | 1.0 | 1.0 | 1.0 | 2.867 | 3.0 | 0.005 | 8.0 | 14.4 | 0.0 | 5.394 | 20.0 | | 112 | 6.402 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 16.99 | 1.0 | 6.552 | 20.0 | | 113 | 7.744 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 12.91 | 0.0 | 4.779 | 19.72 | | 114 | 7.683 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 11.92 | 0.0 | 7.652 | 20.0 | | 115 | 7.698 | 0.5 | 3.461 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 13.06 | 0.0 | 6.217 | 20.0 | | 116 | 6.947 | 0.5 | 1.56 | 1.0 | 0.0 | 0.5025 | 0.3 | 8.0 | 13.04 | 0.0 | 6.418 | 20.0 | | 117 | 7.743 | 0.5 | 1.839 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 12.97 | 0.0 | 6.914 | 17.71 | | 118 | 7.721 | 0.5 | 1.572 | 1.0 | 0.0 | 3.0 | 0.005 | 5.583 | 13.05 | 0.0 | 6.423 | 19.37 | | 119 | 7.707 | 0.5 | 3.264 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 14.21 | 0.0 | 2.684 | 20.0 | | 120 | 7.446 | 1.0 | 3.063 | 1.0 | 0.0 | 3.0 | 0.005 | 6.612 | 13.0 | 0.0 | 9.332 | 19.46 | | 121 | 7.679 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 12.69 | 0.0 | 4.214 | 17.12 | | 122 | 7.637 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 4.938 | 13.18 | 0.0 | 4.092 | 20.0 | | 123 | 6.861 | 0.5 | 4.0 | 0.05 | 0.0 | 3.0 | 0.005 | 3.0 | 11.14 | 0.0 | 6.848 | 20.0 | | 124 | 7.72 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 5.764 | 14.13 | 0.0 | 3.12 | 20.0 | | 125 | 7.478 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 10.03 | 0.0 | 6.52 | 15.74 | | 126 | 6.526 | 0.5 | 4.0 | 0.05 | 0.0 | 3.0 | 0.3 | 8.0 | 10.66 | 0.0 | 7.016 | 17.2 | | 127 | 7.543 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 9.337 | 0.0 | 0.01 | 11.31 | | 128 | 7.5 | 1.0 | 2.932 | 1.0 | 0.0 | 3.0 | 0.005 | 6.703 | 14.46 | 0.0 | 4.53 | 18.53 | | 129 | 7.743 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 13.53 | 0.0 | 9.766 | 17.79 | | 130 | 7.124 | 0.5 | 1.0 | 0.05 | 0.0 | 0.0 | 0.005 | 8.0 | 20.0 | 0.0 | 1.39 | 15.51 | | 131 | 7.599 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 20.0 | 0.0 | 2.035 | 5.0 | | 132 | 7.701 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 17.46 | 0.0 | 0.01 | 20.0 | | 133 | 7.751 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 13.23 | 0.0 | 12.34 | 20.0 | | 134 | 6.433 | 0.8563 | 1.084 | 0.8246 | 1.864 | 2.859 | 0.2361 | 7.893 | 11.11 | 0.5258 | 13.22 | 19.96 | | 135 | 7.771 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 15.79 | 0.0 | 11.99 | 20.0 | | 136 | 7.737 | 0.5 | 1.0 | 1.0 | 0.0 | 0.1906 | 0.005 | 8.0 | 14.74 | 0.0 | 12.11 | 20.0 | | 137 | 7.155 | 1.0 | 1.0 | 1.0 | 0.0 | 1.618 | 0.3 | 8.0 | 15.76 | 0.0 | 14.82 | 20.0 | | 138 | 7.69 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 14.51 | 0.0 | 3.827 | 14.25 | | 139 | 7.645 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 20.0 | 0.0 | 20.0 | 20.0 | | 140 | 7.72 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 5.964 | 14.37 | 0.0 | 10.68 | 20.0 | | 141 | 6.144 | 0.703 | 3.009 | 0.9848 | 2.826 | 1.277 | 0.07818 | 4.065 | 19.83 | 0.578 | 19.77 | 19.91 | | 142 | 7.613 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 11.17 | 0.0 | 1.945 | 20.0 | | 143 | 7.005 | 0.5 | 1.0 | 0.05 | 0.0 | 3.0 | 0.005 | 8.0 | 14.04 | 0.0 | 10.12 | 20.0 | | 144 | 7.735 | 0.5 | 1.0 | 1.0 | 0.0 | 2.229 | 0.005 | 6.85 | 14.55 | 0.0 | 12.14 | 18.01 | | 145 | 7.53 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 13.48 | 0.0 | 8.942 | 15.1 | | 146 | 7.728 | 0.5 | 1.0 | 1.0 | 0.0 | 0.3714 | 0.005 | 4.877 | 14.62 | 0.0 | 11.85 | 20.0 | | 147 | 7.726 | 0.5 | 1.0 | 1.0 | 0.0 | 0.7395 | 0.005 | 6.036 | 17.27 | 0.0 | 11.66 | 20.0 | | 148 | 7.703 | 0.5 | 3.218 | 1.0 | 0.0 | 1.823 | 0.005 | 6.511 | 15.24 | 0.0 | 12.17 | 20.0 | | 149 | 7.691 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 5.666 | 20.0 | 0.0 | 0.01 | 20.0 | | 150 | 7.074 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.3 | 4.744 | 15.87 | 0.0 | 12.67 | 20.0 | | 151 | 7.727 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 5.209 | 13.45 | 0.0 | 9.067 | 17.37 | | 152 | 7.664 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.536 | 15.33 | 0.0 | 9.038 | 20.0 | | 153 | 7.688 | 0.5 | 3.665 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 17.47 | 0.0 | 11.57 | 20.0 | | 154 | 6.824 | 0.8679 | 1.189 | 0.6865 | 1.241 | 0.2776 | 0.2391 | 7.836 | 17.99 | 0.3011 | 11.28 | 18.28 | | 155 | 7.765 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 5.745 | 13.59 | 0.0 | 10.19 | 18.0 | | 156 | 7.637 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 12.1 | 0.0 | 10.06 | 20.0 | | 157 | 5.829 | 0.5 | 1.0 | 1.0 | 3.0 | 0.0 | 0.005 | 3.0 | 13.65 | 1.0 | 9.414 | 20.0 | | 158 | 7.722 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 14.14 | 0.0 | 11.83 | 18.6 | | 159 | 7.696 | 0.5 | 3.769 | 1.0 | 0.0 | 0.0 | 0.005 | 5.468 | 15.68 | 0.0 | 10.23 | 20.0 | | 160 | 7.735 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 12.44 | 0.0 | 11.57 | 16.4 | | 161 | 7.678 | 0.5 | 3.038 | 1.0 | 0.0 | 0.0 | 0.005 | 5.759 | 12.45 | 0.0 | 11.87 | 20.0 | | 162 | 7.651 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 19.93 | 0.0 | 10.31 | 20.0 | | 163 | 7.512 | 1.0 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 13.06 | 0.0 | 1.242 | 16.52 | | 164 | 6.866 | 0.5 | 1.0 | 0.05 | 0.0 | 0.0 | 0.005 | 3.0 | 11.23 | 0.0 | 20.0 | 20.0 | | 165 | 7.59 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 4.937 | 20.0 | 0.0 | 11.45 | 20.0 | | 166 | 7.716 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 9.52 | 0.0 | 10.1 | 13.67 | | 167 | 7.762 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 7.836 | 0.0 | 9.655 | 11.97 | | 168 | 7.466 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 5.822 | 0.0 | 10.12 | 11.86 | | 169 | 6.291 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 10.17 | 1.0 | 11.29 | 13.24 | | 170 | 7.75 | 0.5 | 1.0 | 1.0 | 0.0 | 1.089 | 0.005 | 8.0 | 7.938 | 0.0 | 7.208 | 12.35 | | 171 | 7.233 | 0.5 | 1.0 | 1.0 | 1.813 | 3.0 | 0.005 | 8.0 | 5.894 | 0.0 | 8.166 | 11.77 | | 172 | 7.7 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 5.798 | 8.182 | 0.0 | 8.413 | 14.1 | | 173 | 7.713 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 10.43 | 0.0 | 7.339 | 13.99 | | 174 | 7.71 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 6.127 | 10.56 | 0.0 | 9.516 | 16.69 | | 175 | 7.686 | 0.5 | 3.691 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 8.28 | 0.0 | 8.72 | 12.18 | | 176 | 7.045 | 0.5 | 1.0 | 1.0 | 2.637 | 0.0 | 0.005 | 8.0 | 8.554 | 0.0 | 8.412 | 13.73 | | 177 | 7.633 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 8.996 | 0.0 | 4.923 | 11.66 | | 178 | 7.69 | 0.5 | 3.174 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 8.15 | 0.0 | 7.805 | 10.37 | | 179 | 7.589 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 18.62 | 0.0 | 8.369 | 20.0 | | 180 | 7.134 | 0.5 | 2.777 | 1.0 | 0.0 | 0.0 | 0.3 | 3.0 | 17.39 | 0.0 | 11.15 | 20.0 | | 181 | 7.701 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 8.916 | 0.0 | 8.803 | 10.23 | | 182 | 7.734 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 6.595 | 0.0 | 10.12 | 8.767 | | 183 | 5.831 | 0.5883 | 3.886 | 0.9875 | 2.478 | 2.413 | 0.1524 | 7.782 | 5.016 | 0.955 | 11.68 | 6.71 | | 184 | 6.93 | 0.5 | 4.0 | 0.05 | 0.0 | 3.0 | 0.005 | 3.0 | 20.0 | 0.0 | 8.267 | 20.0 | | 185 | 7.645 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 20.0 | 0.0 | 6.37 | 20.0 | | 186 | 7.707 | 0.5 | 1.0 | 1.0 | 0.0 | 2.342 | 0.005 | 5.92 | 7.563 | 0.0 | 8.868 | 10.45 | | 187 | 7.467 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 9.747 | 0.0 | 6.339 | 11.01 | | 188 | 7.577 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 11.48 | 0.0 | 0.01 | 5.0 | | 189 | 7.543 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 3.0 | 14.29 | 0.0 | 0.01 | 5.0 | | 190 | 7.665 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 16.57 | 0.0 | 4.447 | 20.0 | | 191 | 7.431 | 1.0 | 2.88 | 1.0 | 0.0 | 0.0 | 0.005 | 5.406 | 9.268 | 0.0 | 7.44 | 11.86 | | 192 | 7.558 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 9.919 | 0.0 | 0.01 | 8.668 | | 193 | 7.376 | 1.0 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 10.94 | 0.0 | 0.01 | 5.0 | | 194 | 7.677 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 10.99 | 0.0 | 2.338 | 9.088 | | 195 | 7.14 | 0.5 | 1.0 | 0.05 | 0.0 | 2.317 | 0.005 | 8.0 | 7.301 | 0.0 | 8.58 | 10.18 | | 196 | 7.723 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 5.91 | 5.767 | 0.0 | 11.35 | 10.89 | | 197 | 6.866 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.3 | 4.858 | 7.589 | 0.0 | 10.84 | 11.05 | | 198 | 7.574 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 3.0 | 12.83 | 0.0 | 0.01 | 8.644 | | 199 | 7.548 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 16.21 | 0.0 | 5.393 | 20.0 | | 200 | 7.725 | 0.5 | 1.398 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 9.514 | 0.0 | 0.01 | 8.145 | | 201 | 7.081 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.3 | 8.0 | 20.0 | 0.0 | 0.01 | 5.348 | | 202 | 7.579 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 8.526 | 0.0 | 0.01 | 7.379 | | 203 | 7.704 | 0.5 | 4.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 10.12 | 0.0 | 7.341 | 9.8 | | 204 | 7.414 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 5.0 | 0.0 | 6.01 | 14.66 | | 205 | 7.69 | 0.5 | 1.0 | 1.0 | 0.0 | 3.0 | 0.005 | 5.959 | 6.401 | 0.0 | 8.928 | 13.36 | | 206 | 7.672 | 0.5 | 3.405 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 5.613 | 0.0 | 10.28 | 12.21 | | 207 | 7.677 | 0.5 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 8.0 | 7.738 | 0.0 | 0.01 | 10.18 | | 208 | 5.809 | 0.5 | 1.0 | 1.0 | 3.0 | 3.0 | 0.005 | 3.0 | 20.0 | 1.0 | 20.0 | 8.587 | | 209 | 7.649 | 0.5 | 4.0 | 1.0 | 0.0 | 3.0 | 0.005 | 8.0 | 12.52 | 0.0 | 0.01 | 8.392 | | 210 | 7.427 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.005 | 3.0 | 20.0 | 0.0 | 2.523 | 20.0 | ============================================================================================================================================================= . . # get the performance of best hyperparameters tuned_lgbm_score = 1/LG_BO.max[&#39;target&#39;] print(f&quot;RMSLE of tuned lightgbm: {tuned_lgbm_score:.5f}&quot;) . RMSLE of tuned lightgbm: 0.12868 . # best parameters params = LG_BO.max[&quot;params&quot;] int_params = [&quot;bagging_freq&quot;, &quot;max_depth&quot;, &quot;min_data_in_leaf&quot;, &quot;num_leaves&quot;] for parameter in int_params: params[parameter] = int(params[parameter]) other_lgbm_params = {&#39;seed&#39;: seed, &#39;feature_fraction_seed&#39;: seed, &#39;bagging_seed&#39;: seed, &#39;drop_seed&#39;: seed, &#39;boosting_type&#39;: &#39;gbdt&#39;, &#39;metric&#39;: &#39;rmse&#39;, &#39;verbosity&#39;: -1, &#39;num_threads&#39;: core_count} params.update(other_lgbm_params) params . {&#39;bagging_fraction&#39;: 0.5, &#39;bagging_freq&#39;: 1, &#39;feature_fraction&#39;: 1.0, &#39;lambda_l1&#39;: 0.0, &#39;lambda_l2&#39;: 3.0, &#39;learning_rate&#39;: 0.005, &#39;max_depth&#39;: 8, &#39;min_data_in_leaf&#39;: 15, &#39;min_gain_to_split&#39;: 0.0, &#39;min_sum_hessian_in_leaf&#39;: 3.8982726762658557, &#39;num_leaves&#39;: 20, &#39;seed&#39;: 42, &#39;feature_fraction_seed&#39;: 42, &#39;bagging_seed&#39;: 42, &#39;drop_seed&#39;: 42, &#39;boosting_type&#39;: &#39;gbdt&#39;, &#39;metric&#39;: &#39;rmse&#39;, &#39;verbosity&#39;: -1, &#39;num_threads&#39;: 4} . Train and Make Submission . # get the num_boost_rounds trn = lgb.Dataset(X_trn, y_trn) lgb_cv = lgb.cv(params, trn, num_boost_round = 3000, folds = k_folds, early_stopping_rounds = 10) num_boost_round = len(lgb_cv[&quot;rmse-mean&quot;]) - 10 num_boost_round . 1479 . # train model bst = lgb.train(params, trn, num_boost_round = num_boost_round) # make predictions lgb_preds = np.expm1(bst.predict(X_tst)) . # create submission file submission_df[&quot;SalePrice&quot;] = lgb_preds submission_df.to_csv(&quot;lgb_tuned.csv&quot;, index = None) . This submission gives us a score of 0.12715. The hyperparameter tuning helped to improve the performance of the lightgb model by a little bit. Now, we&#39;ll see how xgboost performs. . XGBoost - Train and Evaluate . # load the datasets into xgboost DMatrices train_d = xgb.DMatrix(X_trn, y_trn) test_d = xgb.DMatrix(X_tst) . # xgboost parameters xgb_params = {&quot;eta&quot;: 0.1, &quot;subsample&quot;: 0.7, &quot;tree_method&quot;: &quot;hist&quot;, &quot;random_state&quot;: seed} . # train and evaluate xgboost xgb_cv = xgb.cv(xgb_params, train_d, num_boost_round = 1500, nfold = 5, early_stopping_rounds = 10) xgb_cv.tail() . train-rmse-mean train-rmse-std test-rmse-mean test-rmse-std . 126 0.033865 | 0.001209 | 0.127286 | 0.018559 | . 127 0.033507 | 0.001256 | 0.127247 | 0.018577 | . 128 0.033214 | 0.001191 | 0.127263 | 0.018581 | . 129 0.032872 | 0.001175 | 0.127245 | 0.018490 | . 130 0.032607 | 0.001167 | 0.127240 | 0.018507 | . Even without hyperparameter tuning, xgboost is giving us a rmsle validation score of 0.127240. We can improve this score by hyperparameter tuning. First, we&#39;ll make predictions and a submission. . Make Submission . # train model xgb_bst = xgb.train(xgb_params, train_d, num_boost_round = len(xgb_cv)) # make predictions xgb_preds = np.expm1(xgb_bst.predict(test_d)) . # create submission file submission_df[&quot;SalePrice&quot;] = xgb_preds submission_df.to_csv(&quot;xgb_non_tuned.csv&quot;, index = None) . This submission gives a score of 0.13190, which looks like the model overfit a little bit. We can tune the hyperparameters and improve the performance. . XGBoost - Hyperparameter tuning . # black box function for Bayesian Optimization def xgb_bayesian(eta, gamma, subsample, colsample_bytree, colsample_bynode, colsample_bylevel, max_depth): # this parameter has to an integer max_depth = int(max_depth) # xgboost parameters params = {&quot;eta&quot;: eta, &quot;gamma&quot;: gamma, &quot;subsample&quot;: subsample, &quot;colsample_bytree&quot;: colsample_bytree, &quot;colsample_bynode&quot;: colsample_bynode, &quot;colsample_bylevel&quot;: colsample_bylevel, &quot;max_depth&quot;: max_depth, &quot;tree_method&quot;: &quot;hist&quot;} # train and score xgb_cv = xgb.cv(params, train_d, num_boost_round = 1500, nfold = 5, early_stopping_rounds = 10, seed = seed) score = xgb_cv.iloc[-10][&quot;test-rmse-mean&quot;] return 1/score . # parameter bounds xgb_bounds = {&quot;eta&quot;: (0.01, 0.05), &quot;gamma&quot;: (0, 20), &quot;subsample&quot;: (0.4, 1), &quot;colsample_bytree&quot;: (0.5, 1), &quot;colsample_bynode&quot;: (0.5, 1), &quot;colsample_bylevel&quot;: (0.5, 1), &quot;max_depth&quot;: (2, 7)} . # optimizer xgb_bo = BayesianOptimization(xgb_bayesian, xgb_bounds, random_state = seed) . # find the best hyperparameters xgb_bo.maximize(init_points = 3, n_iter = 60) . | iter | target | colsam... | colsam... | colsam... | eta | gamma | max_depth | subsample | - | 1 | 5.275 | 0.6873 | 0.9754 | 0.866 | 0.03395 | 3.12 | 2.78 | 0.4349 | | 2 | 3.383 | 0.9331 | 0.8006 | 0.854 | 0.01082 | 19.4 | 6.162 | 0.5274 | | 3 | 4.52 | 0.5909 | 0.5917 | 0.6521 | 0.03099 | 8.639 | 3.456 | 0.7671 | | 4 | 7.739 | 0.5 | 1.0 | 0.5 | 0.05 | 0.0 | 7.0 | 1.0 | | 5 | 5.792 | 1.0 | 0.5 | 1.0 | 0.01 | 2.192 | 7.0 | 1.0 | | 6 | 7.648 | 0.5 | 1.0 | 0.5 | 0.05 | 0.0 | 5.578 | 0.4 | | 7 | 7.691 | 0.5 | 1.0 | 0.5 | 0.05 | 0.0 | 2.0 | 1.0 | | 8 | 7.76 | 1.0 | 0.5 | 0.5 | 0.01 | 0.0 | 3.967 | 1.0 | | 9 | 7.818 | 0.5 | 0.5 | 1.0 | 0.05 | 0.0 | 5.085 | 1.0 | | 10 | 7.422 | 0.7925 | 0.8704 | 0.985 | 0.04765 | 0.008645 | 2.932 | 0.7795 | | 11 | 7.732 | 0.5 | 0.5 | 0.5 | 0.01 | 0.0 | 3.967 | 1.0 | | 12 | 7.775 | 1.0 | 1.0 | 0.5 | 0.01 | 0.0 | 5.477 | 1.0 | | 13 | 7.671 | 1.0 | 0.5 | 0.5 | 0.05 | 0.0 | 7.0 | 0.4 | | 14 | 7.869 | 0.5 | 0.5 | 0.5 | 0.05 | 0.0 | 6.08 | 1.0 | | 15 | 7.797 | 0.5413 | 0.7305 | 0.9571 | 0.0127 | 0.01644 | 6.651 | 0.9833 | | 16 | 7.615 | 1.0 | 0.5 | 0.5 | 0.01 | 0.0 | 2.0 | 1.0 | | 17 | 7.672 | 1.0 | 0.5 | 0.5 | 0.05 | 0.0 | 5.607 | 1.0 | | 18 | 4.074 | 0.5 | 0.5 | 1.0 | 0.05 | 14.46 | 2.0 | 1.0 | | 19 | 4.196 | 1.0 | 1.0 | 0.5 | 0.01 | 11.69 | 7.0 | 1.0 | | 20 | 7.66 | 0.8068 | 0.9216 | 0.5596 | 0.01745 | 0.04023 | 4.768 | 0.995 | | 21 | 3.822 | 0.5 | 0.5 | 0.5 | 0.05 | 20.0 | 2.0 | 1.0 | | 22 | 7.696 | 0.5 | 1.0 | 1.0 | 0.01 | 0.0 | 5.865 | 1.0 | | 23 | 7.522 | 0.7704 | 0.6646 | 0.5423 | 0.02316 | 0.07314 | 2.729 | 0.4044 | | 24 | 7.616 | 0.5 | 0.5 | 1.0 | 0.01 | 0.0 | 2.0 | 1.0 | | 25 | 7.566 | 0.5 | 0.5 | 0.5 | 0.01 | 0.0 | 2.0 | 0.4 | | 26 | 7.887 | 0.5 | 0.5 | 0.5 | 0.01 | 0.0 | 7.0 | 0.4 | | 27 | 6.545 | 0.5 | 0.5 | 0.5 | 0.01 | 0.8323 | 2.0 | 1.0 | | 28 | 4.306 | 0.5 | 1.0 | 0.5 | 0.01 | 6.463 | 7.0 | 0.4 | | 29 | 7.789 | 0.5 | 1.0 | 1.0 | 0.01 | 0.0 | 7.0 | 0.4 | | 30 | 7.773 | 0.5007 | 0.5555 | 0.7216 | 0.03146 | 0.08095 | 6.635 | 0.5173 | | 31 | 7.842 | 1.0 | 0.5 | 0.5 | 0.01 | 0.0 | 4.195 | 0.4 | | 32 | 7.491 | 0.9942 | 0.5431 | 0.9432 | 0.04836 | 0.03342 | 3.756 | 0.4115 | | 33 | 7.886 | 0.5 | 0.5 | 0.5 | 0.01 | 0.0 | 5.175 | 1.0 | | 34 | 7.679 | 1.0 | 1.0 | 0.5 | 0.01 | 0.0 | 3.824 | 0.4 | | 35 | 7.808 | 0.5 | 1.0 | 0.5 | 0.01 | 0.0 | 7.0 | 0.4 | | 36 | 7.789 | 0.5 | 0.5 | 0.5 | 0.01 | 0.0 | 4.712 | 0.4 | | 37 | 7.835 | 0.5 | 0.5 | 0.5 | 0.01 | 0.0 | 7.0 | 1.0 | | 38 | 7.833 | 0.5 | 0.5 | 1.0 | 0.01 | 0.0 | 6.063 | 1.0 | | 39 | 7.886 | 0.5 | 0.5 | 0.5 | 0.01 | 0.0 | 5.654 | 1.0 | | 40 | 7.762 | 0.5803 | 0.505 | 0.5571 | 0.02706 | 0.004659 | 6.536 | 0.8425 | | 41 | 7.706 | 0.5 | 0.5 | 1.0 | 0.05 | 0.0 | 7.0 | 0.4 | | 42 | 7.044 | 0.5109 | 0.5865 | 0.7321 | 0.03324 | 0.4898 | 6.953 | 0.8459 | | 43 | 4.434 | 1.0 | 0.5 | 1.0 | 0.01 | 6.029 | 2.0 | 0.4 | | 44 | 7.363 | 1.0 | 1.0 | 1.0 | 0.01 | 0.0 | 2.0 | 0.4 | | 45 | 7.829 | 1.0 | 0.5 | 0.5 | 0.01 | 0.0 | 5.106 | 0.4 | | 46 | 7.809 | 0.5 | 0.5 | 1.0 | 0.01 | 0.0 | 5.964 | 0.4 | | 47 | 7.689 | 1.0 | 0.5 | 1.0 | 0.01 | 0.0 | 4.909 | 0.4 | | 48 | 7.76 | 1.0 | 0.5 | 0.5 | 0.01 | 0.0 | 3.017 | 1.0 | | 49 | 7.843 | 1.0 | 0.5 | 0.5 | 0.01 | 0.0 | 4.755 | 1.0 | | 50 | 7.734 | 0.5 | 0.5 | 0.5 | 0.01 | 0.0 | 3.453 | 0.4 | | 51 | 7.522 | 1.0 | 1.0 | 1.0 | 0.01 | 0.0 | 7.0 | 1.0 | | 52 | 7.81 | 0.5 | 0.5 | 0.5 | 0.01 | 0.0 | 6.282 | 0.4 | | 53 | 7.699 | 1.0 | 0.5 | 0.5 | 0.05 | 0.0 | 4.654 | 0.4 | | 54 | 3.392 | 0.5034 | 0.7218 | 0.5388 | 0.04978 | 15.4 | 6.853 | 0.4056 | | 55 | 6.381 | 1.0 | 0.5 | 0.5 | 0.01 | 1.022 | 4.746 | 1.0 | | 56 | 7.643 | 1.0 | 1.0 | 1.0 | 0.01 | 0.0 | 6.238 | 0.4 | | 57 | 4.014 | 0.6623 | 0.9548 | 0.6727 | 0.0302 | 11.18 | 2.002 | 0.5538 | | 58 | 7.491 | 0.5 | 1.0 | 0.5 | 0.01 | 0.0 | 2.789 | 1.0 | | 59 | 7.747 | 0.9694 | 0.8278 | 0.8129 | 0.02669 | 0.0109 | 5.277 | 0.6284 | | 60 | 7.651 | 1.0 | 0.5 | 0.5 | 0.01 | 0.0 | 3.44 | 0.4 | | 61 | 7.858 | 0.5 | 0.8667 | 0.8305 | 0.01 | 0.0 | 6.538 | 0.4 | | 62 | 7.788 | 1.0 | 1.0 | 0.5 | 0.01 | 0.0 | 5.181 | 0.4 | | 63 | 7.683 | 0.5313 | 0.5206 | 0.7006 | 0.03925 | 0.000322 | 5.92 | 0.7606 | ============================================================================================================= . # get the performance of best hyperparameters tuned_xgb_score = 1/xgb_bo.max[&#39;target&#39;] print(f&quot;RMSLE of tuned xgboost: {tuned_xgb_score:.5f}&quot;) . RMSLE of tuned xgboost: 0.12679 . xgb_bo.max . {&#39;target&#39;: 7.787494840784668, &#39;params&#39;: {&#39;colsample_bylevel&#39;: 0.5, &#39;colsample_bynode&#39;: 0.5, &#39;colsample_bytree&#39;: 0.5, &#39;eta&#39;: 0.01, &#39;gamma&#39;: 0.0, &#39;max_depth&#39;: 4.676653597241575, &#39;subsample&#39;: 1.0}} . # parameters xgb_tuned_params = {&quot;eta&quot;: 0.01, &quot;gamma&quot;: 0, &quot;subsample&quot;: 1.0, &quot;colsample_bytree&quot;: 0.5, &quot;colsample_bynode&quot;: 0.5, &quot;colsample_bylevel&quot;: 0.5, &quot;max_depth&quot;: 4, &quot;tree_method&quot;: &quot;hist&quot;} . Train and Make Submission . # get the num_boost_round xgb_cv = xgb.cv(xgb_tuned_params, train_d, num_boost_round = 1500, nfold = 5, early_stopping_rounds = 10) num_boost_round = len(xgb_cv) - 10 xgb_cv.tail() . train-rmse-mean train-rmse-std test-rmse-mean test-rmse-std . 1495 0.063411 | 0.002336 | 0.121000 | 0.020686 | . 1496 0.063391 | 0.002330 | 0.120999 | 0.020687 | . 1497 0.063374 | 0.002329 | 0.120994 | 0.020690 | . 1498 0.063356 | 0.002330 | 0.120985 | 0.020681 | . 1499 0.063333 | 0.002331 | 0.120981 | 0.020679 | . # train model bst = xgb.train(xgb_tuned_params, train_d, num_boost_round = num_boost_round) # make predictions xgb_preds = np.expm1(bst.predict(test_d)) . # create submission file submission_df[&quot;SalePrice&quot;] = xgb_preds submission_df.to_csv(&quot;xgb_tuned.csv&quot;, index = None) . This submission gives us the best score yet of rmsle 0.12488. This is our final submission. . Summary and Conclusion . In this project, we worked on the Ames housing data provided as part of a competition on Kaggle. We tried to predict the Sale Price of houses in Ames from this dataset. Before we could train ML models, we evaluated the data and prepared it for ML algorithms. We also trained a dummy model to spot errors in training. In the ML model training part, we first trained some linear models. Then we combined these linear models to form blended predictions. This improved the overall performance. Then we trained two gradient boosting models. First we trained a lightgbm model, which improved on the performance by blended linear models. Hyperparameter tuning also helped to further increase the submission score. Then we trained an XGBoost model, for which we also tuned its hyperparameters. This gave us the best rmsle score of 0.12488. .",
            "url": "https://ncitshubham.github.io/blogs/2021/08/20/ames-housing-competition.html",
            "relUrl": "/2021/08/20/ames-housing-competition.html",
            "date": " • Aug 20, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "EDA and Prediction of Mushroom Edibility using Select ML Algorithms",
            "content": "Introduction . Today, we&#39;ll work on a classification problem. The dataset we have chosen is the mushroom-classification dataset available on Kaggle. This dataset was provided by UCI Machine Learning repository nearly 3 decades ago. The dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. . Our task making successful predictions begins first by the setup of the system for training. . Setup . # Import the required libraries and get the file path import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # data visualization import seaborn as sns # data visualization from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV # validation from sklearn.preprocessing import OneHotEncoder, LabelEncoder # data preparation from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay, precision_score, recall_score, accuracy_score, f1_score # metrics from sklearn.pipeline import make_pipeline # build pipeline # ML models from sklearn.dummy import DummyClassifier from sklearn.linear_model import LogisticRegression, RidgeClassifier from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier from xgboost import XGBClassifier from sklearn.naive_bayes import GaussianNB from sklearn.svm import SVC # ignore warnings import warnings warnings.filterwarnings(&quot;ignore&quot;) # get file path import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . /kaggle/input/mushroom-classification/mushrooms.csv . file_dir = &quot;/kaggle/input/mushroom-classification/mushrooms.csv&quot; . # inspect file size !ls -lh {file_dir} . -rw-r--r-- 1 nobody nogroup 366K Nov 21 08:43 /kaggle/input/mushroom-classification/mushrooms.csv . # inspect first 5 rows of dataset !head {file_dir} . class,cap-shape,cap-surface,cap-color,bruises,odor,gill-attachment,gill-spacing,gill-size,gill-color,stalk-shape,stalk-root,stalk-surface-above-ring,stalk-surface-below-ring,stalk-color-above-ring,stalk-color-below-ring,veil-type,veil-color,ring-number,ring-type,spore-print-color,population,habitat p,x,s,n,t,p,f,c,n,k,e,e,s,s,w,w,p,w,o,p,k,s,u e,x,s,y,t,a,f,c,b,k,e,c,s,s,w,w,p,w,o,p,n,n,g e,b,s,w,t,l,f,c,b,n,e,c,s,s,w,w,p,w,o,p,n,n,m p,x,y,w,t,p,f,c,n,n,e,e,s,s,w,w,p,w,o,p,k,s,u e,x,s,g,f,n,f,w,b,k,t,e,s,s,w,w,p,w,o,e,n,a,g e,x,y,y,t,a,f,c,b,n,e,c,s,s,w,w,p,w,o,p,k,n,g e,b,s,w,t,a,f,c,b,g,e,c,s,s,w,w,p,w,o,p,k,n,m e,b,y,w,t,l,f,c,b,n,e,c,s,s,w,w,p,w,o,p,n,s,m p,x,y,w,t,p,f,c,n,p,e,e,s,s,w,w,p,w,o,p,k,v,g . Observations . The dataset file size is 366 KB. | It will be safe to import the whole dataset. | The prediction class is the first column. | There appears to be no index column | . # read file df = pd.read_csv(file_dir) # view all columns pd.set_option(&quot;display.max_columns&quot;, None) df.head() . class cap-shape cap-surface cap-color bruises odor gill-attachment gill-spacing gill-size gill-color stalk-shape stalk-root stalk-surface-above-ring stalk-surface-below-ring stalk-color-above-ring stalk-color-below-ring veil-type veil-color ring-number ring-type spore-print-color population habitat . 0 p | x | s | n | t | p | f | c | n | k | e | e | s | s | w | w | p | w | o | p | k | s | u | . 1 e | x | s | y | t | a | f | c | b | k | e | c | s | s | w | w | p | w | o | p | n | n | g | . 2 e | b | s | w | t | l | f | c | b | n | e | c | s | s | w | w | p | w | o | p | n | n | m | . 3 p | x | y | w | t | p | f | c | n | n | e | e | s | s | w | w | p | w | o | p | k | s | u | . 4 e | x | s | g | f | n | f | w | b | k | t | e | s | s | w | w | p | w | o | e | n | a | g | . EDA and Data Preparation . # split datasets for training and testing X = df.copy() y = X.pop(&quot;class&quot;) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) . Preliminary Analysis . X_train.shape . (6499, 22) . X_train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 6499 entries, 3777 to 767 Data columns (total 22 columns): # Column Non-Null Count Dtype -- -- 0 cap-shape 6499 non-null object 1 cap-surface 6499 non-null object 2 cap-color 6499 non-null object 3 bruises 6499 non-null object 4 odor 6499 non-null object 5 gill-attachment 6499 non-null object 6 gill-spacing 6499 non-null object 7 gill-size 6499 non-null object 8 gill-color 6499 non-null object 9 stalk-shape 6499 non-null object 10 stalk-root 6499 non-null object 11 stalk-surface-above-ring 6499 non-null object 12 stalk-surface-below-ring 6499 non-null object 13 stalk-color-above-ring 6499 non-null object 14 stalk-color-below-ring 6499 non-null object 15 veil-type 6499 non-null object 16 veil-color 6499 non-null object 17 ring-number 6499 non-null object 18 ring-type 6499 non-null object 19 spore-print-color 6499 non-null object 20 population 6499 non-null object 21 habitat 6499 non-null object dtypes: object(22) memory usage: 1.1+ MB . X_train.describe() . cap-shape cap-surface cap-color bruises odor gill-attachment gill-spacing gill-size gill-color stalk-shape stalk-root stalk-surface-above-ring stalk-surface-below-ring stalk-color-above-ring stalk-color-below-ring veil-type veil-color ring-number ring-type spore-print-color population habitat . count 6499 | 6499 | 6499 | 6499 | 6499 | 6499 | 6499 | 6499 | 6499 | 6499 | 6499 | 6499 | 6499 | 6499 | 6499 | 6499 | 6499 | 6499 | 6499 | 6499 | 6499 | 6499 | . unique 6 | 4 | 10 | 2 | 9 | 2 | 2 | 2 | 12 | 2 | 5 | 4 | 4 | 9 | 9 | 1 | 4 | 3 | 5 | 9 | 6 | 7 | . top x | y | n | f | n | f | c | b | b | t | b | s | s | w | w | p | w | o | p | w | v | d | . freq 2970 | 2583 | 1817 | 3780 | 2822 | 6333 | 5448 | 4511 | 1360 | 3675 | 3036 | 4148 | 3969 | 3559 | 3523 | 6499 | 6343 | 5989 | 3191 | 1884 | 3224 | 2530 | . Observations . The dataset has 22 features. | There are 6499 enries. | All features are categorical in nature. | Most of the features have unique values less than 10. | . Target Class . class_vc = df[&quot;class&quot;].value_counts() class_vc . e 4208 p 3916 Name: class, dtype: int64 . sns.barplot(x = class_vc.index, y = class_vc) . &lt;AxesSubplot:ylabel=&#39;class&#39;&gt; . Observations . The class counts are not much imbalanced. | . Distribution of Features . for i, cols in enumerate(df): feature_vc = df[cols].value_counts() print(feature_vc, &quot; n_________ n&quot;) plt.figure(i) sns.barplot(x = feature_vc.index, y = feature_vc) . e 4208 p 3916 Name: class, dtype: int64 _________ x 3656 f 3152 k 828 b 452 s 32 c 4 Name: cap-shape, dtype: int64 _________ y 3244 s 2556 f 2320 g 4 Name: cap-surface, dtype: int64 _________ n 2284 g 1840 e 1500 y 1072 w 1040 b 168 p 144 c 44 u 16 r 16 Name: cap-color, dtype: int64 _________ f 4748 t 3376 Name: bruises, dtype: int64 _________ n 3528 f 2160 y 576 s 576 a 400 l 400 p 256 c 192 m 36 Name: odor, dtype: int64 _________ f 7914 a 210 Name: gill-attachment, dtype: int64 _________ c 6812 w 1312 Name: gill-spacing, dtype: int64 _________ b 5612 n 2512 Name: gill-size, dtype: int64 _________ b 1728 p 1492 w 1202 n 1048 g 752 h 732 u 492 k 408 e 96 y 86 o 64 r 24 Name: gill-color, dtype: int64 _________ t 4608 e 3516 Name: stalk-shape, dtype: int64 _________ b 3776 ? 2480 e 1120 c 556 r 192 Name: stalk-root, dtype: int64 _________ s 5176 k 2372 f 552 y 24 Name: stalk-surface-above-ring, dtype: int64 _________ s 4936 k 2304 f 600 y 284 Name: stalk-surface-below-ring, dtype: int64 _________ w 4464 p 1872 g 576 n 448 b 432 o 192 e 96 c 36 y 8 Name: stalk-color-above-ring, dtype: int64 _________ w 4384 p 1872 g 576 n 512 b 432 o 192 e 96 c 36 y 24 Name: stalk-color-below-ring, dtype: int64 _________ p 8124 Name: veil-type, dtype: int64 _________ w 7924 n 96 o 96 y 8 Name: veil-color, dtype: int64 _________ o 7488 t 600 n 36 Name: ring-number, dtype: int64 _________ p 3968 e 2776 l 1296 f 48 n 36 Name: ring-type, dtype: int64 _________ w 2388 n 1968 k 1872 h 1632 r 72 u 48 o 48 y 48 b 48 Name: spore-print-color, dtype: int64 _________ v 4040 y 1712 s 1248 n 400 a 384 c 340 Name: population, dtype: int64 _________ d 3148 g 2148 p 1144 l 832 u 368 m 292 w 192 Name: habitat, dtype: int64 _________ . Observations . All the feautures are categorical in nature. | All classes are anonymized. | We&#39;ll need to OneHotEncode the data. | Because the number of no feature classes is too large, we can OneHotEncode each feature class. | Feature vei-type has only one class. Thus it&#39;ll need to be dropped. | . Feature Distribution against Target Class . for i, feature in enumerate(X_train): plt.figure(i) sns.countplot(x = feature, hue = y_train, data = X_train) . Observations . Allmost all the features differ and distinguish between the two target classes. | Their distributions are different for the target classes. | . It means that models may also offer incredible scores in classifying the two target classes. . Null Values . Now, we&#39;ll look at the number of null values in the data. . df.isnull().sum() . class 0 cap-shape 0 cap-surface 0 cap-color 0 bruises 0 odor 0 gill-attachment 0 gill-spacing 0 gill-size 0 gill-color 0 stalk-shape 0 stalk-root 0 stalk-surface-above-ring 0 stalk-surface-below-ring 0 stalk-color-above-ring 0 stalk-color-below-ring 0 veil-type 0 veil-color 0 ring-number 0 ring-type 0 spore-print-color 0 population 0 habitat 0 dtype: int64 . Observations . There are no null values in the dataset. | It will need no imputation or transformation | . Data Preparation . There isn&#39;t much to prepare in the data. We just need to OneHotEncode all the categorical features. And we&#39;ll also encode the target class. . # OneHotEncoding Categorical features ohe = OneHotEncoder(drop = &quot;first&quot;, handle_unknown = &quot;ignore&quot;, sparse = False) ohe_train_data = ohe.fit_transform(X_train) ohe_test_data = ohe.transform(X_test) . ohe_train_data . array([[0., 0., 0., ..., 0., 0., 0.], [0., 1., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 1., 0., 0.], ..., [0., 0., 0., ..., 1., 0., 0.], [0., 0., 0., ..., 0., 1., 0.], [0., 1., 0., ..., 0., 1., 0.]]) . # Encode the target in train dataset le = LabelEncoder() y_train_le = le.fit_transform(y_train) . # Encode target in test dataset y_test_le = le.transform(y_test) . So, we can now move on to training ML models. . Train Baseline Model and Evaluate Results . First we&#39;ll train a baseline model. This will help us in giving us a baseline score which our future models should at least beat. It helps to find errors in training. . base_model = DummyClassifier(random_state = 42, strategy = &quot;most_frequent&quot;) base_cross_val = cross_validate(base_model, ohe_train_data, y_train_le, scoring = [&quot;accuracy&quot;, &quot;precision&quot;, &quot;recall&quot;, &quot;f1&quot;, &quot;roc_auc&quot;]) . base_cross_val . {&#39;fit_time&#39;: array([0.00430179, 0.00245929, 0.00171161, 0.0016439 , 0.0016706 ]), &#39;score_time&#39;: array([0.00662088, 0.00684905, 0.00624609, 0.00612545, 0.00620723]), &#39;test_accuracy&#39;: array([0.51769231, 0.51769231, 0.51692308, 0.51692308, 0.51732102]), &#39;test_precision&#39;: array([0., 0., 0., 0., 0.]), &#39;test_recall&#39;: array([0., 0., 0., 0., 0.]), &#39;test_f1&#39;: array([0., 0., 0., 0., 0.]), &#39;test_roc_auc&#39;: array([0.5, 0.5, 0.5, 0.5, 0.5])} . Observations . The test roc_auc score is 0.5 in all validations, which means the model is not able to distinguish between the two target classes in any way. | This model also gives a test accuracy of 0.52. | . We&#39;ll need to at least improve on these scores. . Model Selection . In this section, we&#39;ll train multiple ML models and compare their performance after cross validation. Then we&#39;ll choose the best performing one for final training. . models = {&quot;LogisticRegression&quot;: LogisticRegression(random_state = 42), &quot;RidgeClassification&quot;: RidgeClassifier(random_state = 42), &quot;GaussianNB&quot;: GaussianNB(), &quot;RandomForestClassifier&quot;: RandomForestClassifier(n_estimators = 70, random_state = 42), &quot;XGBClassifier&quot;: XGBClassifier(n_estimators = 70, objective = &quot;binary:logistic&quot;, learning_rate = 0.05, n_jobs = -1, scoring = &quot;auc&quot;, random_state = 42)} . model_scores = {} # cross validate all models for model_name, model in models.items(): cross_val = cross_validate(model, ohe_train_data, y_train_le, n_jobs = -1, scoring = [&quot;accuracy&quot;, &quot;precision&quot;, &quot;recall&quot;, &quot;f1&quot;, &quot;roc_auc&quot;]) del cross_val[&quot;fit_time&quot;] del cross_val[&quot;score_time&quot;] model_scores[model_name] = cross_val . # put results into a dataframe model_scores_df = pd.DataFrame.from_dict(model_scores) model_scores_df = model_scores_df.applymap(np.mean) . model_scores_df . LogisticRegression RidgeClassification GaussianNB RandomForestClassifier XGBClassifier . test_accuracy 0.999538 | 0.999538 | 0.947377 | 1.0 | 1.0 | . test_precision 1.000000 | 1.000000 | 0.902635 | 1.0 | 1.0 | . test_recall 0.999044 | 0.999044 | 0.999044 | 1.0 | 1.0 | . test_f1 0.999521 | 0.999521 | 0.948331 | 1.0 | 1.0 | . test_roc_auc 1.000000 | 1.000000 | 0.996180 | 1.0 | 1.0 | . Observations . Surprisingly, all of the models performed incredibly well and fit almost perfectly to the training dataset. | Out of these, RandomForestClassifier performed the best with a perfect score of 1.0 in all scores. | . Thus, we&#39;ll train the RandomForestClassifier to make final predictions. . Train Final Model and make predictions . # train model forest_clas = RandomForestClassifier(random_state = 42) forest_clas.fit(ohe_train_data, y_train_le) . RandomForestClassifier(random_state=42) . # make predictions preds = forest_clas.predict(ohe_test_data) # decode predictions into their original labels preds_in = le.inverse_transform(preds) preds_in . array([&#39;e&#39;, &#39;p&#39;, &#39;e&#39;, ..., &#39;e&#39;, &#39;p&#39;, &#39;p&#39;], dtype=object) . # plot results ConfusionMatrixDisplay.from_predictions(y_test, preds_in) . &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fae14daf950&gt; . accuracy_score(y_test, preds_in) . 1.0 . recall_score(y_test_le, preds) . 1.0 . f1_score(y_test_le, preds) . 1.0 . RocCurveDisplay.from_estimator(forest_clas, ohe_test_data, y_test_le) . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7fae14cd1510&gt; . Observations . As seen in the model selection phase, RandomForestClassifier achieved perfect accuracy in classification. | This gave it perfect scores in other metrics as well such as roc_auc, recall and f1 score. | . Summary and Conclusion . In this project we worked on a classification problem. We needed to classify Mushroom bases on their edibility, classifying them into either edible or poisonous mushrooms. The dataset had 22 features which were to be used to make predictions. As observed in the EDA sections, the features had different distributions for target classes, which helped in ML training phase. This meant we achieved perfect scores in model selection phase with RandomForestClassifier. And as expected, the RandomForestClassifier achieved perfect scores in the final training section too. . Thanks for reading. .",
            "url": "https://ncitshubham.github.io/blogs/2021/06/02/classifying-mushrooms-on-edibility.html",
            "relUrl": "/2021/06/02/classifying-mushrooms-on-edibility.html",
            "date": " • Jun 2, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Exploratory Data Analysis of Road Accidents in USA",
            "content": "Introduction . Every year 1.3 million people die as a result of a road traffic crash around the world. And 20 - 50 million people suffer non-fatal injuries, with many incurring a disability as a result of their injury.1 In USA alone, there were 33,244 fatal motor vehicle crashes in 2019 in which 36,096 deaths occurred.2 . In this project, we are going to study the &#39;US Accidents&#39; dataset provided by Sobhan Moosavi on https://www.kaggle.com. We can analyse this data to gain some really interesting insights about road accidents in USA, such as the impact of environmental stimuli on accidental occurance, the change in the occurance of accidents with change in months, or which 5 states have the highest (or lowest) number of accidents. [Note: This Dataset covers 49 mainland states of the USA (excluding Alaska) for the time period: February 2016 to Dec 2020] . We will use the approach of Exploratory Data Analysis (EDA) for studying this data, which is used to analyse datasets to summarize their main characteristics, often using statistical graphics and other data visualization methods. EDA can help in seeing what the data can tell us beyond the formal modeling or hypothesis testing task.3 . Because this dataset is huge, with dozens of features, it can be used to answer a lot of questions. For this project, we are going to limit ourselves to 5-10 questions and then try to answer these questions. . Setup . Import the necessary libraries and get the file_path for the dataset. . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # data visualization import seaborn as sns # data visualization import os for dirname, _, filenames in os.walk(&#39;US_Accidents_Dec20_updated.csv/&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . US_Accidents_Dec20_updated.csv/US_Accidents_Dec20_updated.csv . # Read the file file_path = &quot;US_Accidents_Dec20_updated.csv/US_Accidents_Dec20_updated.csv&quot; us_accidents = pd.read_csv(file_path) . pd.set_option(&quot;display.max_columns&quot;, None) us_accidents.head() . ID Severity Start_Time End_Time Start_Lat Start_Lng End_Lat End_Lng Distance(mi) Description Number Street Side City County State Zipcode Country Timezone Airport_Code Weather_Timestamp Temperature(F) Wind_Chill(F) Humidity(%) Pressure(in) Visibility(mi) Wind_Direction Wind_Speed(mph) Precipitation(in) Weather_Condition Amenity Bump Crossing Give_Way Junction No_Exit Railway Roundabout Station Stop Traffic_Calming Traffic_Signal Turning_Loop Sunrise_Sunset Civil_Twilight Nautical_Twilight Astronomical_Twilight . 0 A-2716600 | 3 | 2016-02-08 00:37:08 | 2016-02-08 06:37:08 | 40.10891 | -83.09286 | 40.11206 | -83.03187 | 3.230 | Between Sawmill Rd/Exit 20 and OH-315/Olentang... | NaN | Outerbelt E | R | Dublin | Franklin | OH | 43017 | US | US/Eastern | KOSU | 2016-02-08 00:53:00 | 42.1 | 36.1 | 58.0 | 29.76 | 10.0 | SW | 10.4 | 0.00 | Light Rain | False | False | False | False | False | False | False | False | False | False | False | False | False | Night | Night | Night | Night | . 1 A-2716601 | 2 | 2016-02-08 05:56:20 | 2016-02-08 11:56:20 | 39.86542 | -84.06280 | 39.86501 | -84.04873 | 0.747 | At OH-4/OH-235/Exit 41 - Accident. | NaN | I-70 E | R | Dayton | Montgomery | OH | 45424 | US | US/Eastern | KFFO | 2016-02-08 05:58:00 | 36.9 | NaN | 91.0 | 29.68 | 10.0 | Calm | NaN | 0.02 | Light Rain | False | False | False | False | False | False | False | False | False | False | False | False | False | Night | Night | Night | Night | . 2 A-2716602 | 2 | 2016-02-08 06:15:39 | 2016-02-08 12:15:39 | 39.10266 | -84.52468 | 39.10209 | -84.52396 | 0.055 | At I-71/US-50/Exit 1 - Accident. | NaN | I-75 S | R | Cincinnati | Hamilton | OH | 45203 | US | US/Eastern | KLUK | 2016-02-08 05:53:00 | 36.0 | NaN | 97.0 | 29.70 | 10.0 | Calm | NaN | 0.02 | Overcast | False | False | False | False | True | False | False | False | False | False | False | False | False | Night | Night | Night | Day | . 3 A-2716603 | 2 | 2016-02-08 06:15:39 | 2016-02-08 12:15:39 | 39.10148 | -84.52341 | 39.09841 | -84.52241 | 0.219 | At I-71/US-50/Exit 1 - Accident. | NaN | US-50 E | R | Cincinnati | Hamilton | OH | 45202 | US | US/Eastern | KLUK | 2016-02-08 05:53:00 | 36.0 | NaN | 97.0 | 29.70 | 10.0 | Calm | NaN | 0.02 | Overcast | False | False | False | False | True | False | False | False | False | False | False | False | False | Night | Night | Night | Day | . 4 A-2716604 | 2 | 2016-02-08 06:51:45 | 2016-02-08 12:51:45 | 41.06213 | -81.53784 | 41.06217 | -81.53547 | 0.123 | At Dart Ave/Exit 21 - Accident. | NaN | I-77 N | R | Akron | Summit | OH | 44311 | US | US/Eastern | KAKR | 2016-02-08 06:54:00 | 39.0 | NaN | 55.0 | 29.65 | 10.0 | Calm | NaN | NaN | Overcast | False | False | False | False | False | False | False | False | False | False | False | False | False | Night | Night | Day | Day | . Data Preparation and Cleaning . Premilinary analysis of dataset . us_accidents.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1516064 entries, 0 to 1516063 Data columns (total 47 columns): # Column Non-Null Count Dtype -- -- 0 ID 1516064 non-null object 1 Severity 1516064 non-null int64 2 Start_Time 1516064 non-null object 3 End_Time 1516064 non-null object 4 Start_Lat 1516064 non-null float64 5 Start_Lng 1516064 non-null float64 6 End_Lat 1516064 non-null float64 7 End_Lng 1516064 non-null float64 8 Distance(mi) 1516064 non-null float64 9 Description 1516064 non-null object 10 Number 469969 non-null float64 11 Street 1516064 non-null object 12 Side 1516064 non-null object 13 City 1515981 non-null object 14 County 1516064 non-null object 15 State 1516064 non-null object 16 Zipcode 1515129 non-null object 17 Country 1516064 non-null object 18 Timezone 1513762 non-null object 19 Airport_Code 1511816 non-null object 20 Weather_Timestamp 1485800 non-null object 21 Temperature(F) 1473031 non-null float64 22 Wind_Chill(F) 1066748 non-null float64 23 Humidity(%) 1470555 non-null float64 24 Pressure(in) 1479790 non-null float64 25 Visibility(mi) 1471853 non-null float64 26 Wind_Direction 1474206 non-null object 27 Wind_Speed(mph) 1387202 non-null float64 28 Precipitation(in) 1005515 non-null float64 29 Weather_Condition 1472057 non-null object 30 Amenity 1516064 non-null bool 31 Bump 1516064 non-null bool 32 Crossing 1516064 non-null bool 33 Give_Way 1516064 non-null bool 34 Junction 1516064 non-null bool 35 No_Exit 1516064 non-null bool 36 Railway 1516064 non-null bool 37 Roundabout 1516064 non-null bool 38 Station 1516064 non-null bool 39 Stop 1516064 non-null bool 40 Traffic_Calming 1516064 non-null bool 41 Traffic_Signal 1516064 non-null bool 42 Turning_Loop 1516064 non-null bool 43 Sunrise_Sunset 1515981 non-null object 44 Civil_Twilight 1515981 non-null object 45 Nautical_Twilight 1515981 non-null object 46 Astronomical_Twilight 1515981 non-null object dtypes: bool(13), float64(13), int64(1), object(20) memory usage: 412.1+ MB . # basic statistics us_accidents.describe() . Severity Start_Lat Start_Lng End_Lat End_Lng Distance(mi) Number Temperature(F) Wind_Chill(F) Humidity(%) Pressure(in) Visibility(mi) Wind_Speed(mph) Precipitation(in) . count 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 4.699690e+05 | 1.473031e+06 | 1.066748e+06 | 1.470555e+06 | 1.479790e+06 | 1.471853e+06 | 1.387202e+06 | 1.005515e+06 | . mean 2.238630e+00 | 3.690056e+01 | -9.859919e+01 | 3.690061e+01 | -9.859901e+01 | 5.872617e-01 | 8.907533e+03 | 5.958460e+01 | 5.510976e+01 | 6.465960e+01 | 2.955495e+01 | 9.131755e+00 | 7.630812e+00 | 8.477855e-03 | . std 6.081481e-01 | 5.165653e+00 | 1.849602e+01 | 5.165629e+00 | 1.849590e+01 | 1.632659e+00 | 2.242190e+04 | 1.827316e+01 | 2.112735e+01 | 2.325986e+01 | 1.016756e+00 | 2.889112e+00 | 5.637364e+00 | 1.293168e-01 | . min 1.000000e+00 | 2.457022e+01 | -1.244976e+02 | 2.457011e+01 | -1.244978e+02 | 0.000000e+00 | 0.000000e+00 | -8.900000e+01 | -8.900000e+01 | 1.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 25% 2.000000e+00 | 3.385422e+01 | -1.182076e+02 | 3.385420e+01 | -1.182077e+02 | 0.000000e+00 | 1.212000e+03 | 4.700000e+01 | 4.080000e+01 | 4.800000e+01 | 2.944000e+01 | 1.000000e+01 | 4.600000e+00 | 0.000000e+00 | . 50% 2.000000e+00 | 3.735113e+01 | -9.438100e+01 | 3.735134e+01 | -9.437987e+01 | 1.780000e-01 | 4.000000e+03 | 6.100000e+01 | 5.700000e+01 | 6.800000e+01 | 2.988000e+01 | 1.000000e+01 | 7.000000e+00 | 0.000000e+00 | . 75% 2.000000e+00 | 4.072593e+01 | -8.087469e+01 | 4.072593e+01 | -8.087449e+01 | 5.940000e-01 | 1.010000e+04 | 7.300000e+01 | 7.100000e+01 | 8.400000e+01 | 3.004000e+01 | 1.000000e+01 | 1.040000e+01 | 0.000000e+00 | . max 4.000000e+00 | 4.900058e+01 | -6.711317e+01 | 4.907500e+01 | -6.710924e+01 | 1.551860e+02 | 9.999997e+06 | 1.706000e+02 | 1.130000e+02 | 1.000000e+02 | 5.804000e+01 | 1.400000e+02 | 9.840000e+02 | 2.400000e+01 | . Number of null values in the dataset, column vise . null_val_cols = us_accidents.isnull().sum() null_val_cols = null_val_cols[null_val_cols &gt; 0].sort_values(ascending = False) null_val_cols . Number 1046095 Precipitation(in) 510549 Wind_Chill(F) 449316 Wind_Speed(mph) 128862 Humidity(%) 45509 Visibility(mi) 44211 Weather_Condition 44007 Temperature(F) 43033 Wind_Direction 41858 Pressure(in) 36274 Weather_Timestamp 30264 Airport_Code 4248 Timezone 2302 Zipcode 935 City 83 Sunrise_Sunset 83 Civil_Twilight 83 Nautical_Twilight 83 Astronomical_Twilight 83 dtype: int64 . # plot percentage of missing values missing_percentage = null_val_cols / len(us_accidents) * 100 missing_percentage.plot(kind = &#39;barh&#39;, figsize = (14, 8)) plt.xlabel(&quot;Percentage of missing values&quot;) plt.ylabel(&quot;Columns&quot;) plt.show() . Observations . The Number column represents the street number in address record. | It has over a million null values. | . Because we already have the location data from the Start_Lat, Start_Lng, County and State features, we can choose to drop this column from the dataset. With regards to columns representing weather conditions, we can choose to drop the columns having a significant number of null values, and impute other columns to the mean values from the same State and month, which should represent similar weather conditions. . us_accidents.drop(labels = [&quot;Number&quot;, &quot;Precipitation(in)&quot;], axis = 1, inplace = True) us_accidents.shape . (1516064, 45) . We&#39;ll also convert the Start_Time column to datetime dtype, so that we can use use it to apply datetime functions later on. . us_accidents.Start_Time = us_accidents.Start_Time.astype(&quot;datetime64&quot;) . Exploratory Analysis and Visualization . With these columns in mind, we will try to answer the following questions from the dataset: . Are there more accidents in warmer or colder areas? | Which 5 states have the highest number of accidents? | Among the top 100 cities in number of accidents, which states do they belong to most frequently. | What time of the day are accidents most frequent in? | Which days of the week have the most accidents? | Which months have the most accidents? | What is the trend of accidents year over year (decreasing/increasing?) | How does accident severity change with change in precipitation? | We can start with plotting longitude and lattitude data to get a sense of where is the data concentrated . lat, long = us_accidents.Start_Lat, us_accidents.Start_Lng . # plot locations of accidents plt.figure(figsize = (10, 6)) sns.scatterplot(x = long, y = lat, s = 0.5) . &lt;AxesSubplot:xlabel=&#39;Start_Lng&#39;, ylabel=&#39;Start_Lat&#39;&gt; . It appears that accidents are more concentrated near the coasts, which are more populated areas of US. There seems to be a sharp decline in the mid - mid-western US. Apart from lack of accidents in those areas, it could also be a signal of poor data collection in those states. . Q1. Are there more accidents in warmer or colder areas? . First, we&#39;ll plot a histogram for the temperature data. . us_accidents[&quot;Temperature(F)&quot;].plot(kind = &quot;hist&quot;, logy = True) plt.xlabel(&quot;Temperature(F)&quot;) plt.show() . print(f&#39;&#39;&#39;Mean: {us_accidents[&quot;Temperature(F)&quot;].mean()} Skewness: {us_accidents[&quot;Temperature(F)&quot;].skew()} Kurtosis: {us_accidents[&quot;Temperature(F)&quot;].kurtosis()}&#39;&#39;&#39;) . Mean: 58.5816432896377 Skewness: -0.30533266112446805 Kurtosis: -0.13261978426888454 . Observations . Data is normally distributed with very low skewness and kurtosis values. | Thus, the data doesn&#39;t support the idea that warmer areas have more accidents than colder ones or vice versa. | . Q2. Which 5 states have the highest number of accidents? . accidents_by_states = us_accidents.State.value_counts() print(&quot;The top 5 states in terms of number of accidents are:&quot;) accidents_by_states.head() . The top 5 states in terms of number of accidents are: . CA 448833 FL 153007 OR 87484 TX 75142 NY 60974 Name: State, dtype: int64 . plt.figure(figsize = (16, 5)) sns.barplot(x = accidents_by_states.index, y = accidents_by_states.values) . &lt;AxesSubplot:&gt; . Q3. Among the top 100 cities in number of accidents, which states do they belong to most frequently. . city_accidents = us_accidents.City.value_counts() city_accidents . Los Angeles 39984 Miami 36233 Charlotte 22203 Houston 20843 Dallas 19497 ... Manzanita 1 West Brooklyn 1 Garfield Heights 1 Belding 1 American Fork-Pleasant Grove 1 Name: City, Length: 10657, dtype: int64 . city_states = us_accidents.groupby(&quot;City&quot;)[&quot;State&quot;].aggregate(pd.Series.mode) city_states . City Aaronsburg PA Abbeville SC Abbotsford WI Abbottstown PA Aberdeen MD .. Zortman MT Zumbro Falls MN Zumbrota MN Zuni VA Zwingle IA Name: State, Length: 8769, dtype: object . top_100_cities = pd.concat([city_accidents, city_states], axis = 1)[:100] top_100_cities . City State . Los Angeles 27760 | CA | . Miami 26831 | FL | . Orlando 10772 | FL | . Dallas 10522 | TX | . Charlotte 10312 | NC | . ... ... | ... | . Flint 1465 | MI | . Hollywood 1431 | FL | . Eugene 1427 | OR | . Silver Spring 1425 | MD | . Birmingham 1422 | AL | . 100 rows × 2 columns . #Most frequent states in the top 100 cities in the number of accidents most_freq_states = top_100_cities.State.value_counts() most_freq_states . CA 35 FL 13 TX 5 NY 4 OR 4 MI 3 VA 3 LA 3 PA 3 SC 2 MO 2 UT 2 AZ 2 TN 2 MN 2 NC 2 OH 2 OK 1 NJ 1 MD 1 KY 1 IN 1 CO 1 DC 1 WA 1 IL 1 GA 1 AL 1 Name: State, dtype: int64 . # plot most_freq_states.plot(kind = &quot;bar&quot;, figsize = (15,5)) plt.xlabel(&quot;States&quot;) plt.ylabel(&quot;Frequency in top 100 cities by number of accidents&quot;) plt.title(&quot;Most frequent states in the top 100 cities in the number of accidents&quot;) plt.show() . Q4. What time of the day are accidents most frequent in? . Do more accidents tend to occur at a particular time of the day? . # plot us_accidents.Start_Time.dt.hour.plot(kind = &#39;hist&#39;, bins = 24) plt.xlabel(&quot;Hour of the day&quot;) plt.show() . Observations . It appears that accidents tend to occur more in the morning between 7-9 and in the afternoon between 13-17. Office hours could be the reason behind this trend. We can examine this by separating the data for weekdays and weekends. . # plot weekdays = us_accidents.Start_Time.dt.dayofweek &lt; 5 us_accidents.loc[weekdays].Start_Time.dt.hour.plot(kind = &#39;hist&#39;, bins = 24) plt.xlabel(&quot;Hour of the day&quot;) plt.title(&quot;Accidents frequency of Weekdays&quot;) plt.show() . # plot data for weekends us_accidents.loc[~weekdays].Start_Time.dt.hour.plot(kind = &#39;hist&#39;, bins = 24) plt.xlabel(&quot;Hour of the day&quot;) plt.title(&quot;Accidents frequency of Weekends&quot;) plt.show() . Observations . On weekends, there seems to be a breakaway from the pattern that we saw earlier, which supports the idea that the trend we saw earlier could be due to the traffic resulting from office timings, which are usually closed on weekends. | On weekends, accidents tend to occur more during daylight, which should be due to more traffic during those hours. | . Q5. Which days of the week have the most accidents? . sns.histplot(us_accidents.Start_Time.dt.dayofweek, bins = 7) plt.xlabel(&quot;Day of Week&quot;) plt.show() . Observations . Accidents occur more on weekdays and there is a sharp drop in their count on the weekends. | . Q6. Which months have the most accidents? . sns.histplot(us_accidents.Start_Time.dt.month, bins = 12) plt.xlabel(&quot;Month&quot;) plt.show() . Observations . Later months of the year appear to witness more accidents. | . This cannot be due to temperature(winter season) because the count drops suddenly in the starting months of the year. Due to there being no other apparent cause, it demands further analysis. . We can start by looking at the trend year over year. . fig, axes = plt.subplots(2, 3, figsize = (15,10), ) year = 2016 for r in range(2): for c in range(3): if year &lt; 2021: year_data = us_accidents.loc[us_accidents.Start_Time.dt.year == year] sns.histplot(year_data.Start_Time.dt.month, bins = 12, ax = axes[r, c]) axes[r, c].set_title(year) axes[r, c].set_xlabel(&quot;Month of the year&quot;) year += 1 . Observations . Probably because the data was started being collected in the year 2016, the starting months have a much lower number of datapoints. | Also, in the year 2020, because of the coronavirus pandemic lockdown restrictions, there was a suddent drop in the middle of the year. And as the restrictions eased, more accidents started to occur. | . But this data requires further more analysis on the month wise trends. . Q7. What is the trend of accidents year over year (decreasing/increasing?) . us_accidents.Start_Time.dt.year.plot(kind = &#39;hist&#39;, bins = 5, title = &quot;Year wise trend&quot;, xticks = np.arange(2016, 2021), figsize = (7, 5)) plt.show() . Observations . The number of accidents year over year looks to be increasing. | . This can be attributed to better data collection in the later years. Thus, it need further analysis. . Summary and Conclusion . Many questions could be asked and explored, but here we analysed this dataset to answer 7 questions about road accidents in USA. To answer these questions, we first did some basic data preparation, and then went on to analyse the data. It is important to take note that the findings from the data analysis are correlational in nature and do not establish a causal relationship in any way. These findings are: . The number of accidents don&#39;t differ much between warmer and colder temperatures. | The top 5 states in number of accidents are California, Florida, Oregon, Texas and New York. | These 5 states are also the most frequent states in the top 100 cities by number of road accidents. | The number of accidents look to be highly correlated with the office hours. | This idea is also supported by the fact that weekdays see more accidents than weekends. | Although the number of accidents look to be increasing with years, but this could be attributed to better data collection in the later years. | . Thanks for reading. .",
            "url": "https://ncitshubham.github.io/blogs/2021/06/01/exploratory-data-analysis-of-road-accidents-in-usa.html",
            "relUrl": "/2021/06/01/exploratory-data-analysis-of-road-accidents-in-usa.html",
            "date": " • Jun 1, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Scraping Top Repositories for Topics on GitHub",
            "content": "GitHub is an increasingly popular programming resource used for code sharing. It&#39;s a social networking site for programmers that many companies and organizations use to facilitate project management and collaboration. According to statistics collected in August 2021, it was the most prominent source code host, with over 60 million new repositories created in 2020 and boasting over 67 million total developers. . All the projects on Github are stored as repositories. These repositories can get upvotes which are stored as stars. The stars that a repository gets can give us a guage of how popular the repository is. We can further filter all the repositores on GitHub by the topic they ascribe to. The list of topics is available here. . Thus, we&#39;ll scrape GitHub for the top repocistories on each topic and then save that to a csv file for future use. In order to do this, we&#39;ll use the following tools: . Python as the programming language | Requests library for downloading the webpage contents | BeautifulSoup library for finding and accessing the relevant information from the downloaded webpage. | Pandas library for saving the accessed information to a csv file. | . Introduction about GitHub and the problem statement | . Here are the steps we&#39;ll follow: . We&#39;re going to scrape https://github.com/topics | We&#39;ll get a list of topics. For each topic, we&#39;ll get topic title, topic page URL and topic description | For each topic, we&#39;ll get the top 30 repositories in the topic from the topic page | For each repository, we&#39;ll grab the repo name, username, stars and repo URL | For each topic we&#39;ll create a CSV file in the following format: | . Repo Name,Username,Stars,Repo URL three.js,mrdoob,69700,https://github.com/mrdoob/three.js libgdx,libgdx,18300,https://github.com/libgdx/libgdx . Setup . Import the required libraries . import os import requests from bs4 import BeautifulSoup import pandas as pd . Set up URLs and the user-agent. . topics_url = &quot;https://github.com/topics&quot; base_url = &#39;https://github.com&#39; header = {&quot;User-Agent&quot;: &quot;Mozilla/5.0&quot;} . Create variables to store scraped information. . topic_titles = [] topic_desc = [] topic_URL = [] . Scrape the list of topics. . Download the topics webpage and create a BeautifulSoup object . Let&#39;s write a function to download the page. . def get_soup(url): &#39;&#39;&#39; This function will download the webpage for the url supplied as argument and return the BeautifulSoup object for the webpage which can be used to grab required information for the webpage. &#39;&#39;&#39; response = requests.get(url, &quot;html.parser&quot;, headers = header) if response.status_code != 200: raise Exception(&#39;Failed to load page {}&#39;.format(url)) soup = BeautifulSoup(response.text) return soup . # Example soup = get_soup(topics_url) type(soup) . bs4.BeautifulSoup . Create a transform function . Let&#39;s create some helper functions to parse information from the page. . Get topic titles . To get topic titles, we can pick p tags with the class &quot;f3 lh-condensed mb-0 mt-1 Link--primary&quot; . ![My Image alt text](https://i.imgur.com/OnzIdyP.png) # finding all topic titles def get_topic_titles(soup): selection_class = &#39;f3 lh-condensed mb-0 mt-1 Link--primary&#39; topic_title_tags = soup.find_all(&#39;p&#39;, {&#39;class&#39;: selection_class}) topic_titles = [] for tag in topic_title_tags: topic_titles.append(tag.text) return topic_titles . # Example titles = get_topic_titles(soup) len(titles) . 30 . titles[:5] . [&#39;3D&#39;, &#39;Ajax&#39;, &#39;Algorithm&#39;, &#39;Amp&#39;, &#39;Android&#39;] . This is the list of topics on page number 1. We will today scrape information for topics only on this page. In the future, we can scrape information from other pages as well by changing the page number in the url. Now we&#39;ll find the topic descriptions similarly. . Get topic descriptions . # finding all topics descriptions def get_topic_descs(soup): desc_selector = &#39;f5 color-fg-muted mb-0 mt-1&#39; topic_desc_tags = soup.find_all(&#39;p&#39;, {&#39;class&#39;: desc_selector}) topic_descs = [] for tag in topic_desc_tags: topic_descs.append(tag.text.strip()) return topic_descs . # Example topics_descs = get_topic_descs(soup) len(topics_descs) . 30 . topics_descs[:5] . [&#39;3D modeling is the process of virtually developing the surface and structure of a 3D object.&#39;, &#39;Ajax is a technique for creating interactive web applications.&#39;, &#39;Algorithms are self-contained sequences that carry out a variety of tasks.&#39;, &#39;Amp is a non-blocking concurrency library for PHP.&#39;, &#39;Android is an operating system built by Google designed for mobile devices.&#39;] . Similary, we&#39;ll find the topic urls. . Get topic URLs . def get_topic_urls(soup): topic_link_tags = soup.find_all(&#39;a&#39;, {&#39;class&#39;: &#39;no-underline flex-1 d-flex flex-column&#39;}) topic_urls = [] for tag in topic_link_tags: topic_urls.append(base_url + tag[&#39;href&#39;]) return topic_urls . # Example topic_urls = get_topic_urls(soup) len(topic_urls) . 30 . topic_urls[:5] . [&#39;https://github.com/topics/3d&#39;, &#39;https://github.com/topics/ajax&#39;, &#39;https://github.com/topics/algorithm&#39;, &#39;https://github.com/topics/amphp&#39;, &#39;https://github.com/topics/android&#39;] . Save all information . We&#39;ll put together all this information into a single function and then save the scraped information into a pandas DataFrame. . def scrape_topics(): topics_url = &#39;https://github.com/topics&#39; soup = get_soup(topics_url) topics_dict = { &#39;Title&#39;: get_topic_titles(soup), &#39;Description&#39;: get_topic_descs(soup), &#39;URL&#39;: get_topic_urls(soup) } return pd.DataFrame(topics_dict) . topics_df = scrape_topics() topics_df.head() . Title Description URL . 0 3D | 3D modeling is the process of virtually develo... | https://github.com/topics/3d | . 1 Ajax | Ajax is a technique for creating interactive w... | https://github.com/topics/ajax | . 2 Algorithm | Algorithms are self-contained sequences that c... | https://github.com/topics/algorithm | . 3 Amp | Amp is a non-blocking concurrency library for ... | https://github.com/topics/amphp | . 4 Android | Android is an operating system built by Google... | https://github.com/topics/android | . Scraping for top 30 repos for each topic . Now that we have the topics with their titles, descriptions and url, we can access each topic url to grab information about the top 30 repositories from that topic individually and then save the scraped information for each topic as a separate csv file. . Each topic page looks like this . . From this page, we&#39;ll grab information about the top 30 repositories based on their popularity as measured by the number of stars. The repositories are already sorted by popularity by default, so we can grab 30 of them from the first page on each topic itself. We&#39;ll begin by writing a function to download each topic page and create its BeautifulSoup object. . Download each topic page and create a BeautifulSoup Object . def get_topic_page(topic_url): # Download the page response = requests.get(topic_url, &quot;html.parser&quot;, headers = header) # Check successful response if response.status_code != 200: raise Exception(&#39;Failed to load page {}&#39;.format(topic_url)) # Parse using Beautiful soup topic_soup = BeautifulSoup(response.text) return topic_soup . page = get_topic_page(&#39;https://github.com/topics/3d&#39;) . Transform the topic Beautiful Object . Get all the required information about a repository . All the information that we need about a repository is given under a div tag with class d-flex flex-justify-between my-3. So we will make a function which takes in the content of each repository from these tags as arguement. It will then grab and return the required information from the content. . def get_repo_info(repo): # returns all the required info about a repository info = repo.find(&#39;h3&#39;, {&#39;class&#39;: &#39;f3 color-fg-muted text-normal lh-condensed&#39;}).find_all(&#39;a&#39;) username = info[0].text.strip() repo_name = info[1].text.strip() repo_url = base_url + info[0][&#39;href&#39;].strip() stars = repo.find(&#39;span&#39;, {&#39;id&#39;: &#39;repo-stars-counter-star&#39;}).text.strip() return username, repo_name, stars, repo_url . # Example repo_contents = page.find_all(&#39;div&#39;, {&#39;class&#39;: &#39;d-flex flex-justify-between my-3&#39;}) get_repo_info(repo_contents[0]) . (&#39;mrdoob&#39;, &#39;three.js&#39;, &#39;80.6k&#39;, &#39;https://github.com/mrdoob&#39;) . Here we can see that the function returns the information about the first repository from the topic page. The top repository in this case is &#39;three.js&#39; with 80.6k stars. . Grab the information from top 30 repos under a topic. . Now, we&#39;ll write a function to grab information about repositories within a topic. It will take in a topic soup and return a pandas DataFrame on the top 30 repos in that topic. . def get_topic_repos(topic_soup): div_selection_class = &#39;d-flex flex-justify-between my-3&#39; repo_tags = topic_soup.find_all(&#39;div&#39;, {&#39;class&#39;: div_selection_class}) topic_repos_dict = { &#39;username&#39;: [], &#39;repo_name&#39;: [], &#39;stars&#39;: [],&#39;repo_url&#39;: []} # Get repo info for i in range(len(repo_tags)): username, repo_name, stars, repo_url = get_repo_info(repo_tags[i]) topic_repos_dict[&#39;username&#39;].append(username) topic_repos_dict[&#39;repo_name&#39;].append(repo_name) topic_repos_dict[&#39;stars&#39;].append(stars) topic_repos_dict[&#39;repo_url&#39;].append(repo_url) return pd.DataFrame(topic_repos_dict) . # Example get_topic_repos(page) . username repo_name stars repo_url . 0 mrdoob | three.js | 80.6k | https://github.com/mrdoob | . 1 libgdx | libgdx | 19.8k | https://github.com/libgdx | . 2 pmndrs | react-three-fiber | 17.4k | https://github.com/pmndrs | . 3 BabylonJS | Babylon.js | 16.2k | https://github.com/BabylonJS | . 4 aframevr | aframe | 14k | https://github.com/aframevr | . 5 ssloy | tinyrenderer | 13.3k | https://github.com/ssloy | . 6 lettier | 3d-game-shaders-for-beginners | 12.5k | https://github.com/lettier | . 7 FreeCAD | FreeCAD | 11k | https://github.com/FreeCAD | . 8 metafizzy | zdog | 9.1k | https://github.com/metafizzy | . 9 CesiumGS | cesium | 8.5k | https://github.com/CesiumGS | . 10 timzhang642 | 3D-Machine-Learning | 7.8k | https://github.com/timzhang642 | . 11 a1studmuffin | SpaceshipGenerator | 7.1k | https://github.com/a1studmuffin | . 12 isl-org | Open3D | 6.4k | https://github.com/isl-org | . 13 blender | blender | 5.2k | https://github.com/blender | . 14 domlysz | BlenderGIS | 5k | https://github.com/domlysz | . 15 spritejs | spritejs | 4.8k | https://github.com/spritejs | . 16 openscad | openscad | 4.7k | https://github.com/openscad | . 17 tensorspace-team | tensorspace | 4.6k | https://github.com/tensorspace-team | . 18 jagenjo | webglstudio.js | 4.6k | https://github.com/jagenjo | . 19 YadiraF | PRNet | 4.6k | https://github.com/YadiraF | . 20 AaronJackson | vrn | 4.4k | https://github.com/AaronJackson | . 21 google | model-viewer | 4.1k | https://github.com/google | . 22 ssloy | tinyraytracer | 4.1k | https://github.com/ssloy | . 23 mosra | magnum | 3.9k | https://github.com/mosra | . 24 FyroxEngine | Fyrox | 3.5k | https://github.com/FyroxEngine | . 25 gfxfundamentals | webgl-fundamentals | 3.5k | https://github.com/gfxfundamentals | . 26 tengbao | vanta | 3.3k | https://github.com/tengbao | . 27 cleardusk | 3DDFA | 3.2k | https://github.com/cleardusk | . 28 jasonlong | isometric-contributions | 3.1k | https://github.com/jasonlong | . 29 cnr-isti-vclab | meshlab | 2.9k | https://github.com/cnr-isti-vclab | . As we can see, the function has returned a pandas DataFrame of the top 30 repos from the topic &#39;3d&#39;. Now, we&#39;ll make function to save this DataFrame as a csv file if we haven&#39;t already created a file on that topic. . Save topic file . def scrape_topic(topic_url, path): if os.path.exists(path): print(&quot;The file {} already exists. Skipping...&quot;.format(path)) return topic_df = get_topic_repos(get_topic_page(topic_url)) topic_df.to_csv(path, index=None) . Putting it all together . We have a funciton to get the list of topics | We have a function to create a CSV file for scraped repos from a topics page | Let&#39;s create a function to put them together | . def scrape_topics_repos(): print(&#39;Scraping list of topics&#39;) topics_df = scrape_topics() os.makedirs(&#39;data&#39;, exist_ok=True) for index, row in topics_df.iterrows(): print(f&quot;Scraping top repositories for {row[&#39;Title&#39;]}&quot;) scrape_topic(row[&#39;URL&#39;], f&quot;data/{row[&#39;Title&#39;]}.csv&quot;) . Let&#39;s run it to scrape the top repos for the all the topics on the first page of https://github.com/topics . scrape_topics_repos() . Scraping list of topics Scraping top repositories for 3D Scraping top repositories for Ajax Scraping top repositories for Algorithm Scraping top repositories for Amp Scraping top repositories for Android Scraping top repositories for Angular Scraping top repositories for Ansible Scraping top repositories for API Scraping top repositories for Arduino Scraping top repositories for ASP.NET Scraping top repositories for Atom Scraping top repositories for Awesome Lists Scraping top repositories for Amazon Web Services Scraping top repositories for Azure Scraping top repositories for Babel Scraping top repositories for Bash Scraping top repositories for Bitcoin Scraping top repositories for Bootstrap Scraping top repositories for Bot Scraping top repositories for C Scraping top repositories for Chrome Scraping top repositories for Chrome extension Scraping top repositories for Command line interface Scraping top repositories for Clojure Scraping top repositories for Code quality Scraping top repositories for Code review Scraping top repositories for Compiler Scraping top repositories for Continuous integration Scraping top repositories for COVID-19 Scraping top repositories for C++ . Summary and Conclusion . As we can see, we have successfully scraped top 30 repositories for 30 topics. And we have saved the information on these top 30 repositories as a csv file for each topic separately. The information that we have scraped for each repository is its title, owner username, star count and its url. . We have scraped repositories for only 30 topics today. This was the number of topics available on the page 1 of https://github.com/topics. But it is easy top scrape more topics. What we just need to do is change the page number in the url https://github.com/topics?page={i} where &#39;i&#39; is the page number. This way, we can scrape info on top repos for all the topics of GitHub. .",
            "url": "https://ncitshubham.github.io/blogs/2021/05/10/Scrape_GitHub_topic_repos.html",
            "relUrl": "/2021/05/10/Scrape_GitHub_topic_repos.html",
            "date": " • May 10, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a Data Analyst and a Software Developer in Python. .",
          "url": "https://ncitshubham.github.io/blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ncitshubham.github.io/blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}