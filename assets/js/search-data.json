{
  
    
        "post0": {
            "title": "Predicting Taxi Fares in NYC with Select ML Algorithms",
            "content": "Introduction . In this project, we&#39;ll participate in a Kaggle playground competition - New York City Taxi Fare Prediction. In this competition, hosted in partnership with Google Cloud and Coursera, we are tasked with predicting the fare amount for a taxi ride in New York City given the pickup and dropoff locations. The competition provides three files in csv format - train.csv, test.csv and sample_submission.csv. We have to train ML algorithms on data provided in the test.csv file and then make predictions on test.csv file, which will then be submitted on Kaggle in the format provided in the sample_submission.csv file. The submission will be evaluated using the root mean-squared error or RMSE between the predictions in the submission file and the corresponding ground truth. Therefore, our primary goal in this project will be to improve on this rmse score by minimizing it. . We begin the project by first setting up the training environment and loading the files. . Setup . First, we&#39;ll import the required libraries and get the data directory. . Import required libraries . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from sklearn.metrics import mean_squared_error # Scoring metric from sklearn.model_selection import train_test_split # validation from sklearn.preprocessing import MinMaxScaler, OneHotEncoder # data preparation import psutil # get cpu info from bayes_opt import BayesianOptimization # hyperparameter tuning from tqdm import tqdm # progress meter # data visualization import matplotlib.pyplot as plt import seaborn as sns # ML models from sklearn.dummy import DummyRegressor from sklearn.linear_model import LinearRegression, Ridge from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor import lightgbm as lgb # model pipelines from sklearn.compose import make_column_transformer from sklearn.pipeline import make_pipeline # ignore warnings import warnings warnings.filterwarnings(&quot;ignore&quot;) # get file path import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . /kaggle/input/new-york-city-taxi-fare-prediction/sample_submission.csv /kaggle/input/new-york-city-taxi-fare-prediction/GCP-Coupons-Instructions.rtf /kaggle/input/new-york-city-taxi-fare-prediction/train.csv /kaggle/input/new-york-city-taxi-fare-prediction/test.csv . Load dataset files . We&#39;ll first inspect the data using shell commands. . data_dir = &quot;/kaggle/input/new-york-city-taxi-fare-prediction&quot; !ls -l {data_dir} # get the files info . total 5564952 -rw-r--r-- 1 nobody nogroup 486 Apr 2 14:07 GCP-Coupons-Instructions.rtf -rw-r--r-- 1 nobody nogroup 343271 Apr 2 14:07 sample_submission.csv -rw-r--r-- 1 nobody nogroup 983020 Apr 2 14:07 test.csv -rw-r--r-- 1 nobody nogroup 5697178298 Apr 2 14:07 train.csv . # get the no. of lines in the train dataset !wc -l {data_dir}/train.csv . 55423856 /kaggle/input/new-york-city-taxi-fare-prediction/train.csv . # get the no. of lines in the test dataset !wc -l {data_dir}/test.csv . 9914 /kaggle/input/new-york-city-taxi-fare-prediction/test.csv . # get the no. of lines in the sample submission. !wc -l {data_dir}/sample_submission.csv . 9915 /kaggle/input/new-york-city-taxi-fare-prediction/sample_submission.csv . Observations . This is a supervised learning regression problem | Training data is 5.5 GB in size | Training data has 5.5 million rows | Test set is much smaller (&lt; 10,000 rows) | . Loading this whole dataset direcltly will slowdown our initial training. Thus, we&#39;ll initially import only about 1% of it. This will still give us about 550k rows of data to train our model, which should be enough for initial training. Once, we have finalized our model and its hyperparameters, we can then use the full dataset for training the final model and making the final predictions. . We&#39;ll now inspect how the dataset is structured and import only the columns we need. . # Train set !head -n 20 {data_dir}/train.csv . key,fare_amount,pickup_datetime,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count 2009-06-15 17:26:21.0000001,4.5,2009-06-15 17:26:21 UTC,-73.844311,40.721319,-73.84161,40.712278,1 2010-01-05 16:52:16.0000002,16.9,2010-01-05 16:52:16 UTC,-74.016048,40.711303,-73.979268,40.782004,1 2011-08-18 00:35:00.00000049,5.7,2011-08-18 00:35:00 UTC,-73.982738,40.76127,-73.991242,40.750562,2 2012-04-21 04:30:42.0000001,7.7,2012-04-21 04:30:42 UTC,-73.98713,40.733143,-73.991567,40.758092,1 2010-03-09 07:51:00.000000135,5.3,2010-03-09 07:51:00 UTC,-73.968095,40.768008,-73.956655,40.783762,1 2011-01-06 09:50:45.0000002,12.1,2011-01-06 09:50:45 UTC,-74.000964,40.73163,-73.972892,40.758233,1 2012-11-20 20:35:00.0000001,7.5,2012-11-20 20:35:00 UTC,-73.980002,40.751662,-73.973802,40.764842,1 2012-01-04 17:22:00.00000081,16.5,2012-01-04 17:22:00 UTC,-73.9513,40.774138,-73.990095,40.751048,1 2012-12-03 13:10:00.000000125,9,2012-12-03 13:10:00 UTC,-74.006462,40.726713,-73.993078,40.731628,1 2009-09-02 01:11:00.00000083,8.9,2009-09-02 01:11:00 UTC,-73.980658,40.733873,-73.99154,40.758138,2 2012-04-08 07:30:50.0000002,5.3,2012-04-08 07:30:50 UTC,-73.996335,40.737142,-73.980721,40.733559,1 2012-12-24 11:24:00.00000098,5.5,2012-12-24 11:24:00 UTC,0,0,0,0,3 2009-11-06 01:04:03.0000002,4.1,2009-11-06 01:04:03 UTC,-73.991601,40.744712,-73.983081,40.744682,2 2013-07-02 19:54:00.000000232,7,2013-07-02 19:54:00 UTC,-74.00536,40.728867,-74.008913,40.710907,1 2011-04-05 17:11:05.0000001,7.7,2011-04-05 17:11:05 UTC,-74.001821,40.737547,-73.99806,40.722788,2 2013-11-23 12:57:00.000000190,5,2013-11-23 12:57:00 UTC,0,0,0,0,1 2014-02-19 07:22:00.00000074,12.5,2014-02-19 07:22:00 UTC,-73.98643,40.760465,-73.98899,40.737075,1 2009-07-22 16:08:00.000000163,5.3,2009-07-22 16:08:00 UTC,-73.98106,40.73769,-73.994177,40.728412,1 2010-07-07 14:52:00.00000044,5.3,2010-07-07 14:52:00 UTC,-73.969505,40.784843,-73.958732,40.783357,1 . # Test set !head -n 20 {data_dir}/test.csv . key,pickup_datetime,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count 2015-01-27 13:08:24.0000002,2015-01-27 13:08:24 UTC,-73.973320007324219,40.7638053894043,-73.981430053710938,40.74383544921875,1 2015-01-27 13:08:24.0000003,2015-01-27 13:08:24 UTC,-73.986862182617188,40.719383239746094,-73.998886108398438,40.739200592041016,1 2011-10-08 11:53:44.0000002,2011-10-08 11:53:44 UTC,-73.982524,40.75126,-73.979654,40.746139,1 2012-12-01 21:12:12.0000002,2012-12-01 21:12:12 UTC,-73.98116,40.767807,-73.990448,40.751635,1 2012-12-01 21:12:12.0000003,2012-12-01 21:12:12 UTC,-73.966046,40.789775,-73.988565,40.744427,1 2012-12-01 21:12:12.0000005,2012-12-01 21:12:12 UTC,-73.960983,40.765547,-73.979177,40.740053,1 2011-10-06 12:10:20.0000001,2011-10-06 12:10:20 UTC,-73.949013,40.773204,-73.959622,40.770893,1 2011-10-06 12:10:20.0000003,2011-10-06 12:10:20 UTC,-73.777282,40.646636,-73.985083,40.759368,1 2011-10-06 12:10:20.0000002,2011-10-06 12:10:20 UTC,-74.014099,40.709638,-73.995106,40.741365,1 2014-02-18 15:22:20.0000002,2014-02-18 15:22:20 UTC,-73.969582,40.765519,-73.980686,40.770725,1 2014-02-18 15:22:20.0000003,2014-02-18 15:22:20 UTC,-73.989374,40.741973,-73.9993,40.722534,1 2014-02-18 15:22:20.0000001,2014-02-18 15:22:20 UTC,-74.001614,40.740893,-73.956387,40.767437,1 2010-03-29 20:20:32.0000002,2010-03-29 20:20:32 UTC,-73.991198,40.739937,-73.997166,40.735269,1 2010-03-29 20:20:32.0000001,2010-03-29 20:20:32 UTC,-73.982034,40.762723,-74.001867,40.761545,1 2011-10-06 03:59:12.0000002,2011-10-06 03:59:12 UTC,-73.992455,40.728701,-73.983397,40.750149,1 2011-10-06 03:59:12.0000001,2011-10-06 03:59:12 UTC,-73.983583,40.746993,-73.951178,40.785903,1 2012-07-15 16:45:04.0000006,2012-07-15 16:45:04 UTC,-74.006746,40.731721,-74.010204,40.732318,1 2012-07-15 16:45:04.0000002,2012-07-15 16:45:04 UTC,-73.976446,40.785598,-73.95222,40.772121,1 2012-07-15 16:45:04.0000003,2012-07-15 16:45:04 UTC,-73.973548,40.763349,-73.972096,40.756417,1 . # Sample Submission !head -n 20 {data_dir}/sample_submission.csv . key,fare_amount 2015-01-27 13:08:24.0000002,11.35 2015-01-27 13:08:24.0000003,11.35 2011-10-08 11:53:44.0000002,11.35 2012-12-01 21:12:12.0000002,11.35 2012-12-01 21:12:12.0000003,11.35 2012-12-01 21:12:12.0000005,11.35 2011-10-06 12:10:20.0000001,11.35 2011-10-06 12:10:20.0000003,11.35 2011-10-06 12:10:20.0000002,11.35 2014-02-18 15:22:20.0000002,11.35 2014-02-18 15:22:20.0000003,11.35 2014-02-18 15:22:20.0000001,11.35 2010-03-29 20:20:32.0000002,11.35 2010-03-29 20:20:32.0000001,11.35 2011-10-06 03:59:12.0000002,11.35 2011-10-06 03:59:12.0000001,11.35 2012-07-15 16:45:04.0000006,11.35 2012-07-15 16:45:04.0000002,11.35 2012-07-15 16:45:04.0000003,11.35 . Observations . The training set has 8 columns: key (a unique identifier) | fare_amount (target column) | pickup_datetime | pickup_longitude | pickup_latitude | dropoff_longitude | dropoff_latitude | passenger_count | . | The test set has all columns except the target column fare_amount. | The submission file should contain the key and fare_amount for each test sample. | key has the same entries as the pickup_datetime column. | Because key doesn&#39;t offer any new information, we can drop it. | Some of the entries have 0 as entry in the longitude and lattitude columns, which will need further inspection in the EDA section. | We can optimize file reading by setting dtypes in advance and not leaving it to pandas to infer, which adds overhead. Parsing dates will be an exception here, which we&#39;ll do using string methods which is much faster. | The data looks to be randomized by pickup_datetime. Because we also wanted to have a randomized representative sample from the training dataset for initial training, we can just import the first 1% of the rows. | . # columns to import columns = &quot;fare_amount,pickup_datetime,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count&quot;.split(&quot;,&quot;) columns . [&#39;fare_amount&#39;, &#39;pickup_datetime&#39;, &#39;pickup_longitude&#39;, &#39;pickup_latitude&#39;, &#39;dropoff_longitude&#39;, &#39;dropoff_latitude&#39;, &#39;passenger_count&#39;] . %%time # set dtypes dtypes = {&#39;fare_amount&#39;: &quot;float32&quot;, &#39;pickup_longitude&#39;: &quot;float32&quot;, &#39;pickup_latitude&#39;: &quot;float32&quot;, &#39;dropoff_longitude&#39;: &quot;float32&quot;, &#39;dropoff_latitude&#39;: &quot;float32&quot;, &#39;passenger_count&#39;: &quot;uint8&quot;} # set nrows to import nrows_to_import = 550_000 # import the training data df = pd.read_csv(f&quot;{data_dir}/train.csv&quot;, usecols = columns, dtype = dtypes, nrows = nrows_to_import) . CPU times: user 989 ms, sys: 111 ms, total: 1.1 s Wall time: 1.1 s . # inspect df df.head() . fare_amount pickup_datetime pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count . 0 4.5 | 2009-06-15 17:26:21 UTC | -73.844315 | 40.721317 | -73.841614 | 40.712276 | 1 | . 1 16.9 | 2010-01-05 16:52:16 UTC | -74.016045 | 40.711304 | -73.979271 | 40.782005 | 1 | . 2 5.7 | 2011-08-18 00:35:00 UTC | -73.982735 | 40.761269 | -73.991241 | 40.750561 | 2 | . 3 7.7 | 2012-04-21 04:30:42 UTC | -73.987129 | 40.733143 | -73.991570 | 40.758091 | 1 | . 4 5.3 | 2010-03-09 07:51:00 UTC | -73.968094 | 40.768009 | -73.956657 | 40.783764 | 1 | . Observations . The data has loaded correctly. Now we&#39;ll change the column pickup_datetime dtype to datetime. . %%time # parse dates def datetime_parser(dataframe): datetime = dataframe[&quot;pickup_datetime&quot;].str.slice(0, 16) datetime = pd.to_datetime(datetime, utc = True, format = &quot;%Y-%m-%d %H:%M&quot;) return datetime df[&quot;pickup_datetime&quot;] = datetime_parser(df) # inspect dtypes df.dtypes . CPU times: user 488 ms, sys: 37.9 ms, total: 526 ms Wall time: 529 ms . fare_amount float32 pickup_datetime datetime64[ns, UTC] pickup_longitude float32 pickup_latitude float32 dropoff_longitude float32 dropoff_latitude float32 passenger_count uint8 dtype: object . Now that all the columns are loaded correctly, we can similarly load the test dataset and the sample submission file. . # load test set test_df = pd.read_csv(data_dir + &quot;/test.csv&quot;, index_col = &quot;key&quot;, dtype = dtypes) test_df[&quot;pickup_datetime&quot;] = datetime_parser(test_df) test_df.dtypes . pickup_datetime datetime64[ns, UTC] pickup_longitude float32 pickup_latitude float32 dropoff_longitude float32 dropoff_latitude float32 passenger_count uint8 dtype: object . test_df . pickup_datetime pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count . key . 2015-01-27 13:08:24.0000002 2015-01-27 13:08:00+00:00 | -73.973320 | 40.763805 | -73.981430 | 40.743835 | 1 | . 2015-01-27 13:08:24.0000003 2015-01-27 13:08:00+00:00 | -73.986862 | 40.719383 | -73.998886 | 40.739201 | 1 | . 2011-10-08 11:53:44.0000002 2011-10-08 11:53:00+00:00 | -73.982521 | 40.751259 | -73.979652 | 40.746140 | 1 | . 2012-12-01 21:12:12.0000002 2012-12-01 21:12:00+00:00 | -73.981163 | 40.767807 | -73.990448 | 40.751637 | 1 | . 2012-12-01 21:12:12.0000003 2012-12-01 21:12:00+00:00 | -73.966049 | 40.789776 | -73.988564 | 40.744427 | 1 | . ... ... | ... | ... | ... | ... | ... | . 2015-05-10 12:37:51.0000002 2015-05-10 12:37:00+00:00 | -73.968124 | 40.796997 | -73.955643 | 40.780388 | 6 | . 2015-01-12 17:05:51.0000001 2015-01-12 17:05:00+00:00 | -73.945511 | 40.803600 | -73.960213 | 40.776371 | 6 | . 2015-04-19 20:44:15.0000001 2015-04-19 20:44:00+00:00 | -73.991600 | 40.726608 | -73.789742 | 40.647011 | 6 | . 2015-01-31 01:05:19.0000005 2015-01-31 01:05:00+00:00 | -73.985573 | 40.735432 | -73.939178 | 40.801731 | 6 | . 2015-01-18 14:06:23.0000006 2015-01-18 14:06:00+00:00 | -73.988022 | 40.754070 | -74.000282 | 40.759220 | 6 | . 9914 rows × 6 columns . # load sample submission submission_df = pd.read_csv(data_dir + &quot;/sample_submission.csv&quot;, index_col = &quot;key&quot;) submission_df.head() . fare_amount . key . 2015-01-27 13:08:24.0000002 11.35 | . 2015-01-27 13:08:24.0000003 11.35 | . 2011-10-08 11:53:44.0000002 11.35 | . 2012-12-01 21:12:12.0000002 11.35 | . 2012-12-01 21:12:12.0000003 11.35 | . Now, we can set the random state seed and then move on to EDA and Data Preparation. . # set random state seed seed = 42 . Exploratory Data Analysis . Preliminary Analysis . Inspect Training Data . # inspect top 5 rows df.head() . fare_amount pickup_datetime pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count . 0 4.5 | 2009-06-15 17:26:00+00:00 | -73.844315 | 40.721317 | -73.841614 | 40.712276 | 1 | . 1 16.9 | 2010-01-05 16:52:00+00:00 | -74.016045 | 40.711304 | -73.979271 | 40.782005 | 1 | . 2 5.7 | 2011-08-18 00:35:00+00:00 | -73.982735 | 40.761269 | -73.991241 | 40.750561 | 2 | . 3 7.7 | 2012-04-21 04:30:00+00:00 | -73.987129 | 40.733143 | -73.991570 | 40.758091 | 1 | . 4 5.3 | 2010-03-09 07:51:00+00:00 | -73.968094 | 40.768009 | -73.956657 | 40.783764 | 1 | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 550000 entries, 0 to 549999 Data columns (total 7 columns): # Column Non-Null Count Dtype -- -- 0 fare_amount 550000 non-null float32 1 pickup_datetime 550000 non-null datetime64[ns, UTC] 2 pickup_longitude 550000 non-null float32 3 pickup_latitude 550000 non-null float32 4 dropoff_longitude 549994 non-null float32 5 dropoff_latitude 549994 non-null float32 6 passenger_count 550000 non-null uint8 dtypes: datetime64[ns, UTC](1), float32(5), uint8(1) memory usage: 15.2 MB . # basic statistics df.describe() . fare_amount pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count . count 550000.000000 | 550000.000000 | 550000.000000 | 549994.000000 | 549994.000000 | 550000.000000 | . mean 11.349306 | -72.322418 | 39.845139 | -72.320076 | 39.841064 | 1.683707 | . std 9.882444 | 12.547520 | 7.917986 | 11.692136 | 7.288269 | 1.307607 | . min -44.900002 | -3377.680908 | -3116.285400 | -3383.296631 | -2559.749023 | 0.000000 | . 25% 6.000000 | -73.992043 | 40.734943 | -73.991386 | 40.734058 | 1.000000 | . 50% 8.500000 | -73.981789 | 40.752682 | -73.980141 | 40.753136 | 1.000000 | . 75% 12.500000 | -73.967110 | 40.767094 | -73.963600 | 40.768124 | 2.000000 | . max 500.000000 | 2140.601074 | 1703.092773 | 40.851028 | 404.616669 | 6.000000 | . # train data time range df.pickup_datetime.min(), df.pickup_datetime.max() . (Timestamp(&#39;2009-01-01 00:31:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2015-06-30 23:38:00+0000&#39;, tz=&#39;UTC&#39;)) . Observations . The first 5 rows look to be alright. | There are some null values in the drop location columns. | There are some negative values in the fare amount and the location features. These will need to be dealt with. | The max values also suggest that there are some outliers, we&#39;ll also deal with them. | The training data looks to be from 1st Jan 2009 to 30th June 2015. | The data takes about 15 MB in memory. | . Inspect Test Data . test_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 9914 entries, 2015-01-27 13:08:24.0000002 to 2015-01-18 14:06:23.0000006 Data columns (total 6 columns): # Column Non-Null Count Dtype -- -- 0 pickup_datetime 9914 non-null datetime64[ns, UTC] 1 pickup_longitude 9914 non-null float32 2 pickup_latitude 9914 non-null float32 3 dropoff_longitude 9914 non-null float32 4 dropoff_latitude 9914 non-null float32 5 passenger_count 9914 non-null uint8 dtypes: datetime64[ns, UTC](1), float32(4), uint8(1) memory usage: 319.5+ KB . # basic statistics test_df.describe() . pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count . count 9914.000000 | 9914.000000 | 9914.000000 | 9914.000000 | 9914.000000 | . mean -73.976181 | 40.750954 | -73.974945 | 40.751553 | 1.671273 | . std 0.042799 | 0.033542 | 0.039093 | 0.035436 | 1.278747 | . min -74.252190 | 40.573143 | -74.263245 | 40.568974 | 1.000000 | . 25% -73.992500 | 40.736125 | -73.991249 | 40.735253 | 1.000000 | . 50% -73.982327 | 40.753052 | -73.980015 | 40.754065 | 1.000000 | . 75% -73.968012 | 40.767113 | -73.964062 | 40.768757 | 2.000000 | . max -72.986534 | 41.709557 | -72.990967 | 41.696682 | 6.000000 | . # test data time range test_df[&quot;pickup_datetime&quot;].min(), test_df[&quot;pickup_datetime&quot;].max() . (Timestamp(&#39;2009-01-01 11:04:00+0000&#39;, tz=&#39;UTC&#39;), Timestamp(&#39;2015-06-30 20:03:00+0000&#39;, tz=&#39;UTC&#39;)) . Observations . There are 9914 rows of data. | There are no null values in the test data. | There are no obvious data entry errors as seen from the basic statistics. | Its time range is also from 1st Jan 2009 to 30th June 2015. | Its min and max values can inform us on removing outliers from the training data. | . EDA . In this subsection we&#39;ll these questions from the data: . In which locations are rides mostly located? | What is the busiest day of the week? | What is the busiest time of the day? | In which month are fares the highest? | Which pickup locations have the highest fares? | Which drop locations have the highest fares? | What is the average ride distance? | Q1. What is the busiest day of the week? . # Extract day of week and their counts dayofweek = df[&quot;pickup_datetime&quot;].dt.dayofweek.value_counts() dayofweek.sort_values(ascending = False) . 4 84826 5 83691 3 82322 2 79525 1 76975 6 72016 0 70645 Name: pickup_datetime, dtype: int64 . %matplotlib inline # visualize its frequency sns.barplot(x = dayofweek.index, y = dayofweek) . &lt;AxesSubplot:ylabel=&#39;pickup_datetime&#39;&gt; . Observations . In this plot, week starts with Monday denoted by 0 and ends on Sunday denoted by 6. | The taxi trips look to increase in number as the week goes by, peeking on Friday and then fall off on Sunday. | Thus, Friday is the busiest day of week followed closely by Saturday. | . Q2. What is the busiest time of the day? . # Extract hour of day and its counts hourofday = df[&quot;pickup_datetime&quot;].dt.hour.value_counts() hourofday . 19 34510 18 33091 20 32169 21 31448 22 30594 14 27994 23 27314 12 27141 17 27053 13 26901 15 26408 9 25860 11 25656 8 24847 10 24706 16 22643 0 21647 7 20010 1 16019 2 11983 6 11352 3 8757 4 6437 5 5460 Name: pickup_datetime, dtype: int64 . # visualize frequency sns.barplot(x = hourofday.index, y = hourofday) . &lt;AxesSubplot:ylabel=&#39;pickup_datetime&#39;&gt; . Observations . The number of trips are at their lowest between 5-6 AM in the morning, and then they start to rise. | They peak in the evening between 7-8 PM, and then again start to fall. | The busiest hour of the day is between 7-8 PM. | . Q3. In which month are fares the highest? . # Extract months from datetime and get their average fare_amount month_avg_fare = df.groupby(by = df[&quot;pickup_datetime&quot;].dt.month)[&quot;fare_amount&quot;].agg(&quot;mean&quot;) month_avg_fare = month_avg_fare.sort_values() month_avg_fare . pickup_datetime 1 10.768173 2 10.864570 3 11.129817 7 11.140449 8 11.204127 4 11.267707 6 11.540161 11 11.572874 12 11.625045 5 11.654774 10 11.690510 9 11.816459 Name: fare_amount, dtype: float32 . # visualize sns.barplot(x = month_avg_fare, y = month_avg_fare.index, orient = &quot;h&quot;) plt.ylabel(&quot;Month&quot;) . Text(0, 0.5, &#39;Month&#39;) . Observations . The fare_amount is the highest in September. | The taxi fares look to generally rise as the year goes. | . Q4. Which pickup locations have the highest fares? . # remove outliers and extract pickup data pickup = df[[&#39;pickup_longitude&#39;, &#39;pickup_latitude&#39;, &#39;fare_amount&#39;]] pickup = pickup.loc[(pickup[&#39;pickup_latitude&#39;] &gt;= 40.5) &amp; (pickup[&#39;pickup_latitude&#39;] &lt;= 41) &amp; (pickup[&#39;pickup_longitude&#39;] &gt;= -74.1) &amp; (pickup[&#39;pickup_longitude&#39;] &lt;= -73.7) &amp; (pickup[&#39;fare_amount&#39;] &gt; 0) &amp; (pickup[&#39;fare_amount&#39;] &lt;= 200)] # plot taxi fares&#39; distribution sns.kdeplot(pickup[&#39;fare_amount&#39;]) . &lt;AxesSubplot:xlabel=&#39;fare_amount&#39;, ylabel=&#39;Density&#39;&gt; . The taxi fares look to be highly skewed with quite a few outliers. We can confirm this by looking at the skewness value. . # Taxi fare skewness pickup[&#39;fare_amount&#39;].skew() . 3.3180816 . # Visualize its outliers sns.boxplot(pickup[&#39;fare_amount&#39;]) . &lt;AxesSubplot:xlabel=&#39;fare_amount&#39;&gt; . # get fare limits to remove outliers in visualization Q1, Q3 = pickup[&#39;fare_amount&#39;].quantile(q = [0.25, 0.75]).values IQR = Q3 - Q1 fare_min, fare_max = Q1 - (1.5 * IQR), Q3 + (1.5 * IQR) fare_min, fare_max . (-3.75, 22.25) . # visualize plt.figure(figsize = (16, 12)) ax = sns.scatterplot(data = pickup, x=&#39;pickup_longitude&#39;, y=&#39;pickup_latitude&#39;, hue = pickup[&#39;fare_amount&#39;], palette = &#39;rocket_r&#39;, hue_norm = (fare_min, fare_max), s = 0.1) norm = plt.Normalize(fare_min, fare_max) sm = plt.cm.ScalarMappable(cmap=&quot;rocket_r&quot;, norm=norm) sm.set_array([]) ax.get_legend().remove() cbar = ax.figure.colorbar(sm) cbar.set_label(&#39;Fare Amount&#39;, rotation=270) plt.show() . We can look at the map below to get a sense of where the taxi fares are high. . Observation . The pickups from John F. Kennedy Airport and East Elmhurst, which also has an airport, which can be seen using the satellite option on google maps are generallly more expensive. | The southern side of the city also looks to be more expensive. | . This suggests that maybe some landmarks like airports mean high taxi fares. . Q5. Which drop locations have the highest fares? . Similar to pickup locations, we can look at the drop locations. . # remove outliers and extract dropoff data dropoff = df[[&#39;dropoff_longitude&#39;, &#39;dropoff_latitude&#39;, &#39;fare_amount&#39;]] dropoff = dropoff.loc[(dropoff[&#39;dropoff_latitude&#39;] &gt;= 40.5) &amp; (dropoff[&#39;dropoff_latitude&#39;] &lt;= 41) &amp; (dropoff[&#39;dropoff_longitude&#39;] &gt;= -74.1) &amp; (dropoff[&#39;dropoff_longitude&#39;] &lt;= -73.7) &amp; (dropoff[&#39;fare_amount&#39;] &gt; 0) &amp; (dropoff[&#39;fare_amount&#39;] &lt;= 200)] . &lt;AxesSubplot:xlabel=&#39;fare_amount&#39;, ylabel=&#39;Density&#39;&gt; . # visualize plt.figure(figsize = (16, 12)) ax = sns.scatterplot(data = dropoff, x=&#39;dropoff_longitude&#39;, y=&#39;dropoff_latitude&#39;, hue = pickup[&#39;fare_amount&#39;], palette = &#39;rocket_r&#39;, hue_norm = (fare_min, fare_max), s = 0.1) norm = plt.Normalize(fare_min, fare_max) sm = plt.cm.ScalarMappable(cmap=&quot;rocket_r&quot;, norm=norm) sm.set_array([]) ax.get_legend().remove() c_bar = ax.figure.colorbar(sm) c_bar.set_label(&quot;Fare Amount&quot;, rotation = 270) plt.show() . Observations . This map looks very similar to the map of pickup locations. | Popular landmarks seem to have expensive taxi rides here also. | One different thing is that there are quite a few dropoffs which are outside the city. In other words, There are more dropoffs than there are pickups outside the city. | . Data Preparation . Null Values . # number of null values df.isnull().sum() . fare_amount 0 pickup_datetime 0 pickup_longitude 0 pickup_latitude 0 dropoff_longitude 6 dropoff_latitude 6 passenger_count 0 dtype: int64 . There are 6 missing values in the dropoff_longitude and the dropoff_latitude columns. This is quite a small number considering the dataset size. So, we&#39;ll drop the rows with the missing values instead of trying to fill them. . # drop null values df.dropna(inplace = True) . Separate target from predictors . # Separate target from predictors X = df.copy() y = X.pop(&quot;fare_amount&quot;) . # predictors X.head() . pickup_datetime pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count . 0 2009-06-15 17:26:00+00:00 | -73.844315 | 40.721317 | -73.841614 | 40.712276 | 1 | . 1 2010-01-05 16:52:00+00:00 | -74.016045 | 40.711304 | -73.979271 | 40.782005 | 1 | . 2 2011-08-18 00:35:00+00:00 | -73.982735 | 40.761269 | -73.991241 | 40.750561 | 2 | . 3 2012-04-21 04:30:00+00:00 | -73.987129 | 40.733143 | -73.991570 | 40.758091 | 1 | . 4 2010-03-09 07:51:00+00:00 | -73.968094 | 40.768009 | -73.956657 | 40.783764 | 1 | . # target y.head() . 0 4.5 1 16.9 2 5.7 3 7.7 4 5.3 Name: fare_amount, dtype: float32 . Split training data into trainind and validation sets. . # make train_test_split X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.25, random_state = seed) . Train Hardcoded &amp; Baseline Models . We&#39;ll now train a hardcoded model and a baseline model to establish some scores that our future models should at least beat. If they don&#39;t, that would indicate some error in training. . Hardcoded model: Will always predict average fare | Baseline model: Linear regression | . Train and Evaluate Hardcoded Model. . # train hardcoded model dummy_model = DummyRegressor(strategy = &quot;mean&quot;) dummy_model.fit(X_train, y_train) . DummyRegressor() . # make training predictions train_dummy_preds = dummy_model.predict(X_train) train_dummy_preds . array([11.347245, 11.347245, 11.347245, ..., 11.347245, 11.347245, 11.347245], dtype=float32) . # score mean_squared_error(y_train, train_dummy_preds, squared = False) . 9.886141 . # make validation predictions valid_dummy_preds = dummy_model.predict(X_valid) valid_dummy_preds . array([11.347245, 11.347245, 11.347245, ..., 11.347245, 11.347245, 11.347245], dtype=float32) . # score mean_squared_error(y_valid, valid_dummy_preds, squared = False) . 9.8728695 . Train and Evaluate Baseline Model . Because LinearRegression can&#39;t take dtype datetime as input, we&#39;ll drop the pickup_datetime column. . # remove &quot;pickup_datetime&quot; from input column for training input_cols = X_train.columns[1:] input_cols . Index([&#39;pickup_longitude&#39;, &#39;pickup_latitude&#39;, &#39;dropoff_longitude&#39;, &#39;dropoff_latitude&#39;, &#39;passenger_count&#39;], dtype=&#39;object&#39;) . # train baseline model base_model = LinearRegression() base_model.fit(X_train[input_cols], y_train) . LinearRegression() . # make training predictions train_base_preds = base_model.predict(X_train[input_cols]) train_base_preds . array([11.262208 , 11.2621765, 11.262113 , ..., 11.681306 , 11.262474 , 11.471427 ], dtype=float32) . # score mean_squared_error(y_train, train_base_preds, squared = False) . 9.884694 . # make validation predictions valid_base_preds = base_model.predict(X_valid[input_cols]) valid_base_preds . array([11.262225 , 11.785624 , 11.262499 , ..., 11.2622795, 11.262311 , 11.366884 ], dtype=float32) . # score mean_squared_error(y_valid, valid_base_preds, squared = False) . 9.871208 . Observations . The linear regression model is off by $ $$9.871, which isn&#39;t much better than simply predicting the average, which was off by $ $$9.873. . This is mainly because the training data (geocoordinates) is not in a format that&#39;s useful for the model, and we&#39;re not using one of the most important columns: pickup date &amp; time. . However, now we have a baseline that our other models should ideally beat. . Make submission . # make test predictions test_base_preds = base_model.predict(test_df[input_cols]) test_base_preds . array([11.262217, 11.262282, 11.262238, ..., 11.786515, 11.785327, 11.78521 ], dtype=float32) . # save_submission_file submission_df[&quot;fare_amount&quot;] = test_base_preds submission_df.to_csv(&quot;baseline_model.csv&quot;) . . Submission also gives a similar score of 9.406. . Feature engineering . Now that we have got a baseline score, we&#39;ll do feature engineering, wherein we&#39;ll: . Extract parts of datetime | Remove outliers &amp; invalid data | Add distance between pickup &amp; drop | Add distance from landmarks | . All this will be done using functions, which we&#39;ll combine into a single preprocessing function in the end. . Extract parts of datetime. . The datetime column alone can give us many useful features for training the model, like hour of the day, day of week, month, etc. . # Extract parts of datetime. def add_datetime_cols(dataframe): dataframe[&quot;date&quot;] = dataframe[&quot;pickup_datetime&quot;].dt.day dataframe[&quot;month&quot;] = dataframe[&quot;pickup_datetime&quot;].dt.month dataframe[&quot;weekday&quot;] = dataframe[&quot;pickup_datetime&quot;].dt.dayofweek dataframe[&quot;year&quot;] = dataframe[&quot;pickup_datetime&quot;].dt.year return dataframe . Remove outliers &amp; invalid data . There seems to be some invalide data in each of the following columns: . Fare amount | Passenger count | Pickup latitude &amp; longitude | Drop latitude &amp; longitude | . Using the limits from test data, we&#39;ll limit the data to range: . fare_amount: 1 to 200 | longitudes: -75 to -72 | latitudes: 40 to 42 | passenger_count: 1 to 10 | . # remove outliers def remove_outliers(dataframe): dataframe = dataframe.loc[(dataframe[&quot;fare_amount&quot;] &gt;= 1) &amp; (dataframe[&quot;fare_amount&quot;] &lt;= 200) &amp; (dataframe[&quot;pickup_longitude&quot;] &gt;= -75) &amp; (dataframe[&quot;pickup_longitude&quot;] &lt;= -72) &amp; (dataframe[&quot;pickup_latitude&quot;] &gt;= 40) &amp; (dataframe[&quot;pickup_latitude&quot;] &lt;= 42) &amp; (dataframe[&quot;dropoff_longitude&quot;] &gt;= -75) &amp; (dataframe[&quot;dropoff_longitude&quot;] &lt;= -72) &amp; (dataframe[&quot;dropoff_latitude&quot;] &gt;= 40) &amp; (dataframe[&quot;dropoff_latitude&quot;] &lt;= 42) &amp; (dataframe[&quot;passenger_count&quot;] &gt;= 1) &amp; (dataframe[&quot;passenger_count&quot;] &lt;= 10)] return dataframe . Add distance between pickup &amp; drop . This is usually the biggest factor affecting the taxi fare. While we cannot truly know the route and the total distance that the taxi ran for, from the data given we can calculate the distance between pickup &amp; drop using haversine distance. The formula for calculating it is taken from here. . # haversine for calcualting distance def haversine(lon1, lat1, lon2, lat2): &quot;&quot;&quot; Calculate the great circle distance in kilometers between two points on the earth (specified in decimal degrees) &quot;&quot;&quot; # convert decimal degrees to radians lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2]) # haversine formula dlon = lon2 - lon1 dlat = lat2 - lat1 a = np.sin(dlat/2)**2 +np. cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2 c = 2 * np.arcsin(np.sqrt(a)) r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units. return c * r # Add distance between pickup &amp; drop def add_distance(dataframe): dataframe[&quot;trip_dist&quot;] = haversine(dataframe[&quot;pickup_longitude&quot;], dataframe[&quot;pickup_latitude&quot;], dataframe[&quot;dropoff_longitude&quot;], dataframe[&quot;dropoff_latitude&quot;]) return dataframe . Add distance from landmarks . We&#39;ll also add the distance between the dropoff location and some landmarks like: . JFK Airport | LGA Airport | EWR Airport | Times Square | Met Meuseum | World Trade Center | . Because popular locations means more traffic, which affects the waiting time in taxis, this information can also help in prediction. . The names of popular landmarks and their locations are taken from google. To calculate the distance, we&#39;ll use the function that we have just created. . # landmark locations jfk_lonlat = -73.7781, 40.6413 lga_lonlat = -73.8740, 40.7769 ewr_lonlat = -74.1745, 40.6895 met_lonlat = -73.9632, 40.7794 wtc_lonlat = -74.0099, 40.7126 # landmarks landmarks = [(&quot;jfk&quot;, jfk_lonlat), (&quot;lga&quot;, lga_lonlat), (&quot;ewr&quot;, ewr_lonlat), (&quot;met&quot;, met_lonlat), (&quot;wtc&quot;, wtc_lonlat)] . # Add distance from landmarks def add_dist_from_landmarks(dataframe): for lmrk_name, longlat in landmarks: dataframe[lmrk_name + &quot;_dist&quot;] = haversine(longlat[0], longlat[1], dataframe[&quot;dropoff_longitude&quot;], dataframe[&quot;dropoff_latitude&quot;]) return dataframe . Preprocessor . Now, we&#39;ll combine all data preparation steps into a single function. It will: . drop null values if it is the training data | remove outliers if it is the training data | add additional datetime columns | add trip distance column | add distance from landmark columns | drop pickup_datetime column | . # full data preparation def preprocessor(dataframe): if &quot;fare_amount&quot; in dataframe: dataframe = dataframe.dropna() dataframe = remove_outliers(dataframe) dataframe = add_datetime_cols(dataframe) dataframe = add_distance(dataframe) dataframe = add_dist_from_landmarks(dataframe) dataframe = dataframe.drop(columns = [&quot;pickup_datetime&quot;]) return dataframe . # Prepare the data df = preprocessor(df) # training data test_df = preprocessor(test_df) # test data # save processed files for future use df.to_csv(&quot;processed_train_1_perc.csv&quot;, index = None) test_df.to_csv(&quot;preprocessed_test_df.csv&quot;) # Separate training data from validation data X = df.copy() y = X.pop(&quot;fare_amount&quot;) X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.25, random_state = seed) . # inspect preprocessing result X_train.head() . pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count date month weekday year trip_dist jfk_dist lga_dist ewr_dist met_dist wtc_dist . 106009 -73.975952 | 40.776409 | -73.979698 | 40.760006 | 2 | 14 | 12 | 5 | 2013 | 1.851406 | 21.518574 | 9.097258 | 18.191933 | 2.565198 | 5.853306 | . 455614 -73.990707 | 40.751011 | -73.982460 | 40.745422 | 1 | 2 | 8 | 0 | 2010 | 0.932484 | 20.758091 | 9.782166 | 17.338640 | 4.111363 | 4.320646 | . 114078 -73.991280 | 40.730213 | -73.985870 | 40.745068 | 2 | 14 | 12 | 4 | 2012 | 1.713230 | 20.975971 | 10.065125 | 17.055971 | 4.268357 | 4.139222 | . 72126 -73.973801 | 40.784374 | -73.978233 | 40.764561 | 1 | 17 | 6 | 1 | 2014 | 2.234654 | 21.736300 | 8.883799 | 18.525803 | 2.079320 | 6.364183 | . 549968 -73.982056 | 40.776432 | -73.790337 | 40.646755 | 2 | 22 | 7 | 6 | 2012 | 21.657000 | 1.197777 | 16.097822 | 32.747494 | 20.732004 | 19.909977 | . test_df.head() . pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude passenger_count date month weekday year trip_dist jfk_dist lga_dist ewr_dist met_dist wtc_dist . key . 2015-01-27 13:08:24.0000002 -73.973320 | 40.763805 | -73.981430 | 40.743835 | 1 | 27 | 1 | 1 | 2015 | 2.323358 | 20.587837 | 9.766299 | 17.357740 | 4.242006 | 4.221359 | . 2015-01-27 13:08:24.0000003 -73.986862 | 40.719383 | -73.998886 | 40.739201 | 1 | 27 | 1 | 1 | 2015 | 2.425299 | 21.564516 | 11.323100 | 15.799543 | 5.386261 | 3.100082 | . 2011-10-08 11:53:44.0000002 -73.982521 | 40.751259 | -73.979652 | 40.746140 | 1 | 8 | 10 | 5 | 2011 | 0.618403 | 20.607008 | 9.532814 | 17.588009 | 3.949200 | 4.517339 | . 2012-12-01 21:12:12.0000002 -73.981163 | 40.767807 | -73.990448 | 40.751637 | 1 | 1 | 12 | 5 | 2012 | 1.960912 | 21.702991 | 10.201496 | 16.980310 | 3.846307 | 4.639962 | . 2012-12-01 21:12:12.0000003 -73.966049 | 40.789776 | -73.988564 | 40.744427 | 1 | 1 | 12 | 5 | 2012 | 5.387211 | 21.127256 | 10.302326 | 16.818926 | 4.436549 | 3.969716 | . The files have been succesfully preprocessed. Now we&#39;ll select ML models for training. . Model Selection . In this section, we&#39;ll train some linear and some tree based models and compare their performance. . Linear Models . Prepare Data . We&#39;ll train a LinearRegression model and a Ridge in this subsection. But before proceeding, we&#39;ll also scale numerical columns and onehotencode categorical columns. This transformation will only be used with linear models, which can benefit from such transformation. Other tree based models that we&#39;ll train work really well even without it. So with linear models, additional steps will be to: . Scale numerical features with MinMaxScaler | Encode categorical features with OneHotEncoder | . # encode categorical columns cat_cols = pd.Index([&quot;weekday&quot;, &quot;month&quot;]) ohe = OneHotEncoder(drop = &quot;first&quot;, sparse = False) # scale numeric columns num_cols = X_train.columns.difference(cat_cols) scaler = MinMaxScaler() # combine transformation steps lin_ct = make_column_transformer((ohe, cat_cols), (scaler, num_cols)) lin_ct . ColumnTransformer(transformers=[(&#39;onehotencoder&#39;, OneHotEncoder(drop=&#39;first&#39;, sparse=False), Index([&#39;weekday&#39;, &#39;month&#39;], dtype=&#39;object&#39;)), (&#39;minmaxscaler&#39;, MinMaxScaler(), Index([&#39;date&#39;, &#39;dropoff_latitude&#39;, &#39;dropoff_longitude&#39;, &#39;ewr_dist&#39;, &#39;jfk_dist&#39;, &#39;lga_dist&#39;, &#39;met_dist&#39;, &#39;passenger_count&#39;, &#39;pickup_latitude&#39;, &#39;pickup_longitude&#39;, &#39;trip_dist&#39;, &#39;wtc_dist&#39;, &#39;year&#39;], dtype=&#39;object&#39;))]) . # transform X_lin_train = lin_ct.fit_transform(X_train) X_lin_valid = lin_ct.transform(X_valid) X_lin_train . array([[0. , 0. , 0. , ..., 0.01670442, 0.04239011, 0.66666667], [0. , 0. , 0. , ..., 0.00841339, 0.03126889, 0.16666667], [0. , 0. , 0. , ..., 0.01545772, 0.02995245, 0.5 ], ..., [0. , 0. , 0. , ..., 0.0341279 , 0.03019252, 0.83333333], [0. , 0. , 0. , ..., 0.0882236 , 0.03355575, 0. ], [0. , 1. , 0. , ..., 0.06261117, 0.05642887, 0. ]]) . Train and Evaluate . # evaluation function def lin_eval(model): train_preds = model.predict(X_lin_train) train_rmse = mean_squared_error(y_train, train_preds, squared = False) val_preds = model.predict(X_lin_valid) val_rmse = mean_squared_error(y_valid, val_preds, squared = False) return train_rmse, val_rmse, train_preds, val_preds . # models l_models = {&quot;Linear Regression&quot;: LinearRegression(), &quot;Ridge&quot;: Ridge(random_state = seed)} . l_scores = {} # train and evaluate for model_name, model in l_models.items(): print(model_name) model.fit(X_lin_train, y_train) train_rmse, val_rmse, train_preds, val_preds = lin_eval(model) l_scores[model_name] = {&quot;train_rmse&quot;: train_rmse, &quot;validation_rmse&quot;: val_rmse} print(l_scores[model_name]) print(&quot;-&quot; * 70) . Linear Regression {&#39;train_rmse&#39;: 5.10889904076924, &#39;validation_rmse&#39;: 5.024879081297552} - Ridge {&#39;train_rmse&#39;: 5.110620297096774, &#39;validation_rmse&#39;: 5.024162833522911} - . The linear models show improvement over the baseline model and score about 5 rmse. We&#39;ll compare them now with tree based models. . Tree based models . We&#39;ll train a DecisionTreeRegressor and a RandomForestRegressor in this subsection. These models won&#39;t need any more transformation to the data. Therefore, we can move on to training and evaluation part directly. . # evaluation function def tree_eval(model): train_preds = model.predict(X_train) train_rmse = mean_squared_error(y_train, train_preds, squared = False) val_preds = model.predict(X_valid) val_rmse = mean_squared_error(y_valid, val_preds, squared = False) return train_rmse, val_rmse, train_preds, val_preds . # models t_models = {&quot;DecisionTree&quot;: DecisionTreeRegressor(max_depth = 6, random_state = seed), &quot;RandomForest&quot;: RandomForestRegressor(max_depth = 6, n_jobs = -1, random_state = seed)} . t_scores = {} # train and evaluate for model_name, model in t_models.items(): print(model_name) model.fit(X_train, y_train) train_rmse, val_rmse, train_preds, val_preds = tree_eval(model) t_scores[model_name] = {&quot;train_rmse&quot;: train_rmse, &quot;validation_rmse&quot;: val_rmse} print(t_scores[model_name]) print(&quot;-&quot; * 70) . DecisionTree {&#39;train_rmse&#39;: 4.153546496377676, &#39;validation_rmse&#39;: 4.3580147846085975} - RandomForest {&#39;train_rmse&#39;: 4.047689087955997, &#39;validation_rmse&#39;: 4.246326819808587} - . Tree based models performed better than linear models. Both the models scores about 4.25 rmse on the validation data. Based on this we&#39;ll train a gradient boosting model - lightgbm, evaluate its performance and make a submission next. Then we can move on to tune its hyperparameters before training on full training data and making a final submission. . LightGBM . # get cpu core count core_count = psutil.cpu_count(logical = False) core_count . 2 . # model parameters params = {&quot;num_leaves&quot;: 25, &quot;learning_rate&quot;: 0.03, &quot;seed&quot;: seed, &quot;metric&quot;: &quot;rmse&quot;, &quot;num_threads&quot;: core_count} # train and evaluate train_lgb = lgb.Dataset(X_train, y_train) valid_lgb = lgb.Dataset(X_valid, y_valid) bst = lgb.train(params, train_lgb, num_boost_round = 1500, valid_sets = [train_lgb, valid_lgb], early_stopping_rounds = 10, verbose_eval = 20) . [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011514 seconds. You can set `force_row_wise=true` to remove the overhead. And if memory is not enough, you can set `force_col_wise=true`. [LightGBM] [Info] Total Bins 2617 [LightGBM] [Info] Number of data points in the train set: 402413, number of used features: 15 [LightGBM] [Info] Start training from score 11.326931 Training until validation scores don&#39;t improve for 10 rounds [20] training&#39;s rmse: 6.35493 valid_1&#39;s rmse: 6.44781 [40] training&#39;s rmse: 4.89231 valid_1&#39;s rmse: 5.02841 [60] training&#39;s rmse: 4.2942 valid_1&#39;s rmse: 4.46046 [80] training&#39;s rmse: 4.04413 valid_1&#39;s rmse: 4.23143 [100] training&#39;s rmse: 3.92997 valid_1&#39;s rmse: 4.1298 [120] training&#39;s rmse: 3.86415 valid_1&#39;s rmse: 4.07594 [140] training&#39;s rmse: 3.81705 valid_1&#39;s rmse: 4.03604 [160] training&#39;s rmse: 3.78547 valid_1&#39;s rmse: 4.01541 [180] training&#39;s rmse: 3.7564 valid_1&#39;s rmse: 3.9956 [200] training&#39;s rmse: 3.7309 valid_1&#39;s rmse: 3.98351 [220] training&#39;s rmse: 3.70612 valid_1&#39;s rmse: 3.96904 [240] training&#39;s rmse: 3.68621 valid_1&#39;s rmse: 3.95755 [260] training&#39;s rmse: 3.66694 valid_1&#39;s rmse: 3.94732 [280] training&#39;s rmse: 3.65078 valid_1&#39;s rmse: 3.94048 [300] training&#39;s rmse: 3.63347 valid_1&#39;s rmse: 3.93003 [320] training&#39;s rmse: 3.61758 valid_1&#39;s rmse: 3.92148 [340] training&#39;s rmse: 3.6042 valid_1&#39;s rmse: 3.91516 [360] training&#39;s rmse: 3.59117 valid_1&#39;s rmse: 3.91013 [380] training&#39;s rmse: 3.57992 valid_1&#39;s rmse: 3.90559 [400] training&#39;s rmse: 3.56915 valid_1&#39;s rmse: 3.90191 [420] training&#39;s rmse: 3.55754 valid_1&#39;s rmse: 3.89855 [440] training&#39;s rmse: 3.54752 valid_1&#39;s rmse: 3.89467 [460] training&#39;s rmse: 3.53806 valid_1&#39;s rmse: 3.89107 [480] training&#39;s rmse: 3.52856 valid_1&#39;s rmse: 3.88794 [500] training&#39;s rmse: 3.52008 valid_1&#39;s rmse: 3.88553 [520] training&#39;s rmse: 3.51151 valid_1&#39;s rmse: 3.88371 [540] training&#39;s rmse: 3.50374 valid_1&#39;s rmse: 3.88035 [560] training&#39;s rmse: 3.49658 valid_1&#39;s rmse: 3.87734 [580] training&#39;s rmse: 3.48752 valid_1&#39;s rmse: 3.87406 [600] training&#39;s rmse: 3.48019 valid_1&#39;s rmse: 3.87159 [620] training&#39;s rmse: 3.4738 valid_1&#39;s rmse: 3.86963 [640] training&#39;s rmse: 3.46723 valid_1&#39;s rmse: 3.86759 Early stopping, best iteration is: [642] training&#39;s rmse: 3.46642 valid_1&#39;s rmse: 3.86745 . LightGBM improves on the performance of RandomForestRegressor by scoring an an impressive rmse of 3.86. Now, we&#39;ll make a submission with this model. . Make Submsission . # make predictions preds = bst.predict(test_df) # save submission file submission_df[&quot;fare_amount&quot;] = preds submission_df.to_csv(&quot;lightgbm.csv&quot;) . . This submission gives us a score of RMSE: 3.29476 which places us at roughly 35th percentile out of a total of 1400 current participants. We can now improve on this by hyperparameter tuning and by using the full training data. . Tune Hyperparameters . For hyperparameter tuning, we&#39;ll use bayesian optimization. This will involve first creating a black box function which the later the bayes optimizer will try to maximize using different hyperparameters. . # black box function for Bayesian Optimization def LGB_bayesian(bagging_fraction, bagging_freq, lambda_l1, lambda_l2, learning_rate, max_depth, min_data_in_leaf, min_gain_to_split, min_sum_hessian_in_leaf, num_leaves, feature_fraction): # LightGBM expects these parameters to be integer. So we make them integer bagging_freq = int(bagging_freq) num_leaves = int(num_leaves) min_data_in_leaf = int(min_data_in_leaf) max_depth = int(max_depth) # parameters param = {&#39;bagging_fraction&#39;: bagging_fraction, &#39;bagging_freq&#39;: bagging_freq, &#39;lambda_l1&#39;: lambda_l1, &#39;lambda_l2&#39;: lambda_l2, &#39;learning_rate&#39;: learning_rate, &#39;max_depth&#39;: max_depth, &#39;min_data_in_leaf&#39;: min_data_in_leaf, &#39;min_gain_to_split&#39;: min_gain_to_split, &#39;min_sum_hessian_in_leaf&#39;: min_sum_hessian_in_leaf, &#39;num_leaves&#39;: num_leaves, &#39;feature_fraction&#39;: feature_fraction, &#39;seed&#39;: seed, &#39;feature_fraction_seed&#39;: seed, &#39;bagging_seed&#39;: seed, &#39;drop_seed&#39;: seed, &#39;boosting_type&#39;: &#39;gbdt&#39;, &#39;metric&#39;: &#39;rmse&#39;, &#39;force_col_wise&#39;: True, &#39;verbosity&#39;: -1, &#39;num_threads&#39;: core_count} trn = lgb.Dataset(X, y) lgb_cv = lgb.cv(param, trn, num_boost_round = 1500, nfold = 3, stratified = False, early_stopping_rounds = 10, seed = seed) score = lgb_cv[&quot;rmse-mean&quot;][-1] return -score . # parameter bounds bounds_LGB = { &#39;bagging_fraction&#39;: (0.6, 1), &#39;bagging_freq&#39;: (1, 4), &#39;lambda_l1&#39;: (0, 3.0), &#39;lambda_l2&#39;: (0, 3.0), &#39;learning_rate&#39;: (0.01, 0.1), &#39;max_depth&#39;:(3,8), &#39;min_data_in_leaf&#39;: (5, 20), &#39;min_gain_to_split&#39;: (0, 1), &#39;min_sum_hessian_in_leaf&#39;: (0.01, 20), &#39;num_leaves&#39;: (5, 25), &#39;feature_fraction&#39;: (0.05, 1) } . seed = 42 df = pd.read_parquet(&quot;nyc_sample_train.parquet&quot;) X = df.copy() y = X.pop(&quot;fare_amount&quot;) . # optimizer LG_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state = seed) . # find the best hyperparameters LG_BO.maximize(init_points = 5, n_iter = 50) . | iter | target | baggin... | baggin... | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | min_da... | min_ga... | min_su... | num_le... | - | 1 | -3.878 | 0.7498 | 3.852 | 0.7454 | 1.796 | 0.4681 | 0.02404 | 3.29 | 17.99 | 0.6011 | 14.16 | 5.412 | | 2 | -3.855 | 0.988 | 3.497 | 0.2517 | 0.5455 | 0.5502 | 0.03738 | 5.624 | 11.48 | 0.2912 | 12.24 | 7.79 | | 3 | -3.833 | 0.7169 | 2.099 | 0.4833 | 2.356 | 0.599 | 0.05628 | 5.962 | 5.697 | 0.6075 | 3.419 | 6.301 | | 4 | -3.731 | 0.9796 | 3.897 | 0.818 | 0.9138 | 0.293 | 0.07158 | 5.201 | 6.831 | 0.4952 | 0.6974 | 23.19 | | 5 | -3.795 | 0.7035 | 2.988 | 0.3461 | 1.56 | 1.64 | 0.02664 | 7.848 | 16.63 | 0.9395 | 17.9 | 16.96 | | 6 | -4.28 | 0.6316 | 1.0 | 0.05 | 0.0 | 3.0 | 0.1 | 8.0 | 18.12 | 1.0 | 0.01 | 25.0 | | 7 | -3.815 | 0.8511 | 3.327 | 0.2452 | 1.442 | 0.6327 | 0.08836 | 6.227 | 6.279 | 0.3659 | 0.7767 | 24.7 | | 8 | -3.811 | 1.0 | 4.0 | 1.0 | 0.5456 | 0.0 | 0.03513 | 3.0 | 5.0 | 0.5616 | 1.382 | 18.08 | | 9 | -3.933 | 1.0 | 4.0 | 1.0 | 0.0 | 0.0 | 0.01 | 3.0 | 7.293 | 1.0 | 6.517 | 22.86 | | 10 | -3.715 | 1.0 | 4.0 | 1.0 | 2.476 | 0.0 | 0.1 | 8.0 | 7.369 | 0.0 | 0.01 | 19.15 | | 11 | -3.832 | 1.0 | 4.0 | 1.0 | 3.0 | 3.0 | 0.01 | 8.0 | 8.835 | 0.0 | 0.01 | 13.3 | | 12 | -3.735 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.1 | 8.0 | 5.0 | 1.0 | 0.01 | 19.17 | | 13 | -3.827 | 0.6 | 1.0 | 1.0 | 3.0 | 3.0 | 0.1 | 3.0 | 20.0 | 0.0 | 20.0 | 23.85 | | 14 | -4.273 | 0.6 | 1.0 | 0.05 | 3.0 | 3.0 | 0.1 | 8.0 | 7.674 | 0.0 | 20.0 | 16.49 | | 15 | -3.933 | 1.0 | 4.0 | 1.0 | 0.0 | 0.0 | 0.01 | 3.0 | 20.0 | 1.0 | 20.0 | 14.9 | | 16 | -4.989 | 1.0 | 4.0 | 0.05 | 3.0 | 0.0 | 0.01 | 8.0 | 20.0 | 1.0 | 15.16 | 22.47 | | 17 | -3.797 | 0.6 | 1.031 | 1.0 | 3.0 | 3.0 | 0.1 | 4.379 | 8.206 | 0.0 | 0.01 | 19.6 | | 18 | -3.792 | 0.6 | 4.0 | 1.0 | 3.0 | 3.0 | 0.1 | 8.0 | 5.0 | 0.0 | 4.282 | 18.72 | | 19 | -4.267 | 1.0 | 1.0 | 0.05 | 0.0 | 0.0 | 0.1 | 8.0 | 8.037 | 1.0 | 3.758 | 15.92 | | 20 | -3.813 | 0.6 | 4.0 | 1.0 | 3.0 | 3.0 | 0.01 | 6.159 | 5.0 | 1.0 | 0.01 | 20.66 | | 21 | -3.856 | 0.6 | 2.865 | 0.5243 | 0.2189 | 2.828 | 0.01274 | 8.0 | 16.59 | 1.0 | 17.32 | 11.54 | | 22 | -3.84 | 0.6 | 4.0 | 1.0 | 3.0 | 3.0 | 0.1 | 3.0 | 5.0 | 0.0 | 0.01 | 12.42 | | 23 | -3.811 | 0.6 | 4.0 | 1.0 | 0.0 | 3.0 | 0.01 | 8.0 | 9.051 | 0.0 | 0.01 | 21.33 | | 24 | -3.954 | 0.7067 | 4.0 | 1.0 | 3.0 | 3.0 | 0.01 | 3.203 | 11.12 | 0.0 | 0.01 | 6.119 | | 25 | -3.932 | 1.0 | 4.0 | 1.0 | 3.0 | 0.0 | 0.01 | 3.0 | 11.03 | 0.0 | 0.01 | 21.11 | | 26 | -3.983 | 0.6 | 4.0 | 1.0 | 3.0 | 2.971 | 0.01 | 3.0 | 5.0 | 0.0 | 9.472 | 5.0 | | 27 | -3.826 | 0.6 | 1.0 | 1.0 | 3.0 | 0.0 | 0.1 | 3.0 | 15.18 | 0.0 | 15.69 | 12.3 | | 28 | -3.786 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.1 | 3.0 | 13.18 | 0.0 | 19.45 | 6.494 | | 29 | -3.841 | 0.6 | 1.0 | 1.0 | 0.0 | 3.0 | 0.1 | 8.0 | 12.35 | 1.0 | 18.0 | 5.0 | | 30 | -3.865 | 0.6 | 4.0 | 1.0 | 0.0 | 0.0 | 0.06983 | 3.0 | 7.798 | 1.0 | 19.92 | 5.0 | | 31 | -3.79 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.1 | 6.897 | 18.7 | 0.0 | 20.0 | 5.0 | | 32 | -3.963 | 0.8491 | 3.073 | 0.1499 | 2.922 | 2.358 | 0.07925 | 4.056 | 18.72 | 0.8548 | 19.94 | 7.476 | | 33 | -3.782 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.1 | 8.0 | 16.57 | 0.0 | 13.73 | 5.0 | | 34 | -3.834 | 0.6 | 1.0 | 1.0 | 0.0 | 3.0 | 0.1 | 8.0 | 20.0 | 0.0 | 8.132 | 5.0 | | 35 | -3.751 | 1.0 | 1.0 | 1.0 | 3.0 | 0.0 | 0.1 | 8.0 | 18.25 | 0.0 | 10.86 | 9.799 | | 36 | -3.773 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.1 | 3.0 | 19.02 | 0.0 | 8.749 | 9.815 | | 37 | -4.271 | 1.0 | 1.0 | 0.05 | 3.0 | 0.0 | 0.1 | 5.383 | 15.75 | 1.0 | 7.404 | 5.795 | | 38 | -3.737 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.1 | 6.136 | 20.0 | 0.0 | 13.56 | 9.97 | | 39 | -3.825 | 0.6 | 3.931 | 1.0 | 0.0 | 3.0 | 0.1 | 7.734 | 20.0 | 0.0 | 10.28 | 10.79 | | 40 | -3.934 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.01 | 3.0 | 18.02 | 0.0 | 11.98 | 13.07 | | 41 | -3.895 | 0.6 | 4.0 | 1.0 | 3.0 | 0.0 | 0.01 | 8.0 | 20.0 | 0.0 | 14.44 | 8.194 | | 42 | -3.77 | 1.0 | 1.0 | 1.0 | 0.0 | 3.0 | 0.1 | 3.0 | 5.0 | 0.0 | 0.01 | 22.56 | | 43 | -3.841 | 0.6 | 4.0 | 1.0 | 0.0 | 3.0 | 0.1 | 3.0 | 20.0 | 0.0 | 2.512 | 11.95 | | 44 | -3.819 | 0.6 | 1.0 | 1.0 | 0.0 | 0.0 | 0.1 | 8.0 | 15.15 | 0.0 | 13.15 | 11.61 | | 45 | -3.799 | 0.6 | 1.0 | 1.0 | 0.0 | 0.0 | 0.1 | 8.0 | 20.0 | 0.0 | 6.499 | 12.16 | | 46 | -3.779 | 1.0 | 4.0 | 1.0 | 0.0 | 3.0 | 0.1 | 8.0 | 5.0 | 0.0 | 0.01 | 5.0 | | 47 | -3.979 | 1.0 | 4.0 | 1.0 | 3.0 | 0.0 | 0.01 | 8.0 | 12.85 | 0.0 | 20.0 | 5.0 | | 48 | -5.078 | 0.6 | 1.0 | 0.05 | 0.0 | 3.0 | 0.01 | 8.0 | 20.0 | 1.0 | 15.05 | 5.0 | | 49 | -3.741 | 1.0 | 2.33 | 1.0 | 0.7072 | 1.186 | 0.1 | 6.142 | 6.29 | 0.0 | 0.0376 | 20.51 | | 50 | -3.749 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.1 | 6.615 | 19.09 | 0.0 | 10.41 | 11.33 | | 51 | -3.738 | 1.0 | 3.279 | 1.0 | 1.481 | 0.0 | 0.1 | 5.232 | 17.54 | 0.0 | 12.46 | 10.42 | | 52 | -3.739 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.1 | 5.307 | 14.7 | 0.0 | 15.16 | 7.892 | | 53 | -3.707 | 1.0 | 4.0 | 1.0 | 1.791 | 3.0 | 0.1 | 8.0 | 5.0 | 0.0 | 0.01 | 16.65 | | 54 | -3.731 | 1.0 | 1.0 | 1.0 | 3.0 | 0.0 | 0.1 | 7.134 | 19.29 | 0.0 | 14.2 | 12.87 | | 55 | -3.832 | 0.6 | 3.801 | 1.0 | 3.0 | 0.0 | 0.1 | 5.359 | 20.0 | 1.0 | 9.467 | 12.58 | ============================================================================================================================================================= . # tuned hyperparameters LG_BO.max[&quot;params&quot;] . {&#39;bagging_fraction&#39;: 1.0, &#39;bagging_freq&#39;: 4.0, &#39;feature_fraction&#39;: 1.0, &#39;lambda_l1&#39;: 1.7911760022903485, &#39;lambda_l2&#39;: 3.0, &#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 8.0, &#39;min_data_in_leaf&#39;: 5.0, &#39;min_gain_to_split&#39;: 0.0, &#39;min_sum_hessian_in_leaf&#39;: 0.01, &#39;num_leaves&#39;: 16.653557814597693} . Now that the hyperparameter tuning is done, we can load the full training data and then train lightgbm on the full data using tuned hyperparameters. . Load full dataset . To load full dataset, we&#39;ll import chunks and apply all preprocessing on these chunks and then merge these chunks. . %%time chunksize = 5_000_000 df_list = [] for df_chunk in tqdm(pd.read_csv(f&quot;{data_dir}/train.csv&quot;, usecols = columns, dtype = dtypes, chunksize = chunksize)): df_chunk[&#39;pickup_datetime&#39;] = datetime_parser(df_chunk) df_chunk = preprocessor(df_chunk) df_list.append(df_chunk) . 12it [03:18, 16.51s/it] . CPU times: user 2min 25s, sys: 17.2 s, total: 2min 42s Wall time: 3min 18s . . full_df = pd.concat(df_list) full_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 54061209 entries, 0 to 55423855 Data columns (total 16 columns): # Column Dtype -- 0 fare_amount float32 1 pickup_longitude float32 2 pickup_latitude float32 3 dropoff_longitude float32 4 dropoff_latitude float32 5 passenger_count uint8 6 date uint8 7 month uint8 8 weekday uint8 9 year uint16 10 trip_dist float32 11 jfk_dist float32 12 lga_dist float32 13 ewr_dist float32 14 met_dist float32 15 wtc_dist float32 dtypes: float32(11), uint16(1), uint8(4) memory usage: 2.9 GB . We&#39;ll now delete the importing list to free memory and then save the full processed train data to .parquet format for faster reading in the future. . # delete the importing list del df_list # save the full processed train data full_df.to_parquet(&quot;full_training_df.parquet&quot;, index = None) . Final Training and Submission . We&#39;ll need to make some optimizations in order to train on full data, otherwise the system crashes because of low memory. . # separate target y = full_df.pop(&quot;fare_amount&quot;) . # create lightgbm dataset from the training data. full_lgb_df = lgb.Dataset(full_df, label = y, free_raw_data = True) . We&#39;ll save the lightgbm dataset into binary format and then load it again after exiting the system. This will free a memory and make sure we&#39;ll only have processed train data in the memory . # save the lightgb dataset to binary format which is much lighter (about 600 MB.) full_lgb_df.save_binary(&quot;full_train.bin&quot;) . [LightGBM] [Info] Saving data to binary file full_train.bin . &lt;lightgbm.basic.Dataset at 0x7fadd0803a50&gt; . # exit the system, which will force the notebook to restart os._exit(00) . # import the required libraries import lightgbm as lgb import pandas as pd import warnings import psutil # set parameters core_count = psutil.cpu_count(logical = False) seed = 42 params = {&#39;bagging_fraction&#39;: 1.0, &#39;bagging_freq&#39;: 4, &#39;feature_fraction&#39;: 1.0, &#39;lambda_l1&#39;: 1.7911760022903485, &#39;lambda_l2&#39;: 3.0, &#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 8, &#39;min_data_in_leaf&#39;: 5, &#39;min_gain_to_split&#39;: 0.0, &#39;min_sum_hessian_in_leaf&#39;: 0.01, &#39;num_leaves&#39;: 16, &#39;seed&#39;: seed, &#39;feature_fraction_seed&#39;: seed, &#39;bagging_seed&#39;: seed, &#39;drop_seed&#39;: seed, &#39;boosting_type&#39;: &#39;gbdt&#39;, &#39;metric&#39;: &#39;rmse&#39;, &#39;force_col_wise&#39;: True, &#39;num_threads&#39;: core_count, &#39;device&#39;: &#39;gpu&#39;} . # train on full data train = lgb.Dataset(&quot;full_train.bin&quot;) warnings.filterwarnings(&quot;ignore&quot;) full_bst = lgb.train(params, train, num_boost_round = 400, valid_sets = [train], verbose_eval = 25) . [LightGBM] [Info] Load from binary file full_train.bin [LightGBM] [Info] This is the GPU trainer!! [LightGBM] [Info] Total Bins 2617 [LightGBM] [Info] Number of data points in the train set: 54061209, number of used features: 15 [LightGBM] [Info] Using GPU Device: gfx90c, Vendor: Advanced Micro Devices, Inc. [LightGBM] [Info] Compiling OpenCL Kernel with 256 bins... [LightGBM] [Info] GPU programs have been built [LightGBM] [Info] Size of histogram bin entry: 8 [LightGBM] [Info] 15 dense feature groups (824.91 MB) transferred to GPU in 0.891994 secs. 0 sparse feature groups [LightGBM] [Info] Start training from score 11.324434 [25] training&#39;s rmse: 4.15862 [50] training&#39;s rmse: 3.91879 [75] training&#39;s rmse: 3.85214 [100] training&#39;s rmse: 3.80957 [125] training&#39;s rmse: 3.77661 [150] training&#39;s rmse: 3.74922 [175] training&#39;s rmse: 3.72905 [200] training&#39;s rmse: 3.71091 [225] training&#39;s rmse: 3.69806 [250] training&#39;s rmse: 3.68685 [275] training&#39;s rmse: 3.67519 [300] training&#39;s rmse: 3.66482 [325] training&#39;s rmse: 3.6549 [350] training&#39;s rmse: 3.6476 . # # Prediction and Submission test_df = pd.read_csv(&quot;preprocessed_test_df.csv&quot;, index_col = &quot;key&quot;) submission_df = pd.read_csv(data_dir + &quot;/sample_submission.csv&quot;, index_col = &quot;key&quot;) test_preds = full_bst.predict(test_df) submission_df[&quot;fare_amount&quot;] = test_preds submission_df.to_csv(&quot;nyc_full_tuned1.csv&quot;) . submission_df.head() . fare_amount . key . 2015-01-27 13:08:24.0000002 10.046500 | . 2015-01-27 13:08:24.0000003 10.233339 | . 2011-10-08 11:53:44.0000002 4.996559 | . 2012-12-01 21:12:12.0000002 8.939364 | . 2012-12-01 21:12:12.0000003 16.210700 | . . This submission gives us our best score yet of RMSE $ 3.21. This is our final submssion in this project and this lands us in the top 30 %. We can now save this model for future use and we can also analyse what can this model tell us about the importance of various features in the data. . # save model full_bst.save_model(&quot;nyc_full_tuned_model.bin&quot;) . # plot feature importance lgb.plot_importance(full_bst) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Feature importance&#39;}, xlabel=&#39;Feature importance&#39;, ylabel=&#39;Features&#39;&gt; . Observations . As we can see, the trip distance faeture that we added through feature engineering contributed the most in prediction of taxi fares. Following it are also features determining the location of taxi trips like pickup location features, distance from jfk airport and dropoff location features. Next in the importance list is the year of taxi trip, which could suggest that the fares probably increased with the years. . Summary and Conclusion . In this project, we worked on the problem of predicting taxi fares in new york city with the dataset provided by Google Cloud and Coursera. The involved dealing with multiple challenges, with handling big data one being the primary one. We imported only 1 percent of training data to select the ML model and to tune its hyperparameters. But before we could do so, we also analysed and visulaized the data, which also helped in the feature engineering. After preparing the data for training, we moved on to select ML model for full training. The initial results suggested that lightgbm worked the best on the problem. We then tuned the hyperparameters for lightgbm using bayesian optimization. After doing so, we were ready for training the model on the full data. We then imported the full training data and applied preprocessing on it. But before proceeding with the ML model training, we optimized the process by saving the binary format of precessed training data and freeing the memory by restarting the kernel. . The final training and the submission on Kaggle gave us the RMSE of 3.21. We also analysed the feature importance that the model found, and it suggested that the location based features such as trip distance which we calculated using haversin distance in feature engineering section, the pickup longitude and pickup latitude were the most important in making predictions. . Thanks for reading. .",
            "url": "https://ncitshubham.github.io/blogs/2022/03/28/nyc-taxi-fare-prediction.html",
            "relUrl": "/2022/03/28/nyc-taxi-fare-prediction.html",
            "date": " • Mar 28, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "EDA and Data Visualization of Zomato Bangalore Restuarants Dataset",
            "content": "Introduction . Zomato is an Indian multinational restaurant aggregator and food delivery company. In this project, we&#39;re going to study and analyze the Zomato Dataset shared by Himanshu Podder on Kaggle. This dataset contains information on restaurants in the city of Bengaluru, India. We can use this dataset to get an idea of different factors affecting the restaurants in different parts of the city and also answer questions like which type of food is most popular in the city, how does the location of the restuarant affects its rating on the Zomato platform, and what relation does the rating of the restaurant and the number of cuisines it offers has? . We will use the approach of Exploratory Data Analysis (EDA) for studying this data, which is used to analyse datasets to summarize their main characteristics, often using statistical graphics and other data visualization methods. EDA can help in seeing what the data can tell us beyond the formal modeling or hypothesis testing task.3 . The dataset can be used to answer a lot of questions but for this project, we are going to limit ourselves to 5-10 questions and then try to answer these questions. . Setup . Import the required libraries and get the file path. . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # Data Visualization import seaborn as sns # Data Visualization import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . /kaggle/input/zomato-bangalore-restaurants/zomato.csv . file_path = &quot;/kaggle/input/zomato-bangalore-restaurants/zomato.csv&quot; !ls -lh {file_path} . -rw-r--r-- 1 nobody nogroup 548M Feb 1 14:14 /kaggle/input/zomato-bangalore-restaurants/zomato.csv . The file size of the dataset is 548MB and it is safe to import the whole dataset at once. . # Read the csv file into a pandas DataFrame df = pd.read_csv(file_path, thousands = &#39;,&#39;) . Data Preparation and Cleaning . Premilinary Analysis . Evaluate the structure of the dataset . df.head() . url address name online_order book_table rate votes phone location rest_type dish_liked cuisines approx_cost(for two people) reviews_list menu_item listed_in(type) listed_in(city) . 0 https://www.zomato.com/bangalore/jalsa-banasha... | 942, 21st Main Road, 2nd Stage, Banashankari, ... | Jalsa | Yes | Yes | 4.1/5 | 775 | 080 42297555 r n+91 9743772233 | Banashankari | Casual Dining | Pasta, Lunch Buffet, Masala Papad, Paneer Laja... | North Indian, Mughlai, Chinese | 800.0 | [(&#39;Rated 4.0&#39;, &#39;RATED n A beautiful place to ... | [] | Buffet | Banashankari | . 1 https://www.zomato.com/bangalore/spice-elephan... | 2nd Floor, 80 Feet Road, Near Big Bazaar, 6th ... | Spice Elephant | Yes | No | 4.1/5 | 787 | 080 41714161 | Banashankari | Casual Dining | Momos, Lunch Buffet, Chocolate Nirvana, Thai G... | Chinese, North Indian, Thai | 800.0 | [(&#39;Rated 4.0&#39;, &#39;RATED n Had been here for din... | [] | Buffet | Banashankari | . 2 https://www.zomato.com/SanchurroBangalore?cont... | 1112, Next to KIMS Medical College, 17th Cross... | San Churro Cafe | Yes | No | 3.8/5 | 918 | +91 9663487993 | Banashankari | Cafe, Casual Dining | Churros, Cannelloni, Minestrone Soup, Hot Choc... | Cafe, Mexican, Italian | 800.0 | [(&#39;Rated 3.0&#39;, &quot;RATED n Ambience is not that ... | [] | Buffet | Banashankari | . 3 https://www.zomato.com/bangalore/addhuri-udupi... | 1st Floor, Annakuteera, 3rd Stage, Banashankar... | Addhuri Udupi Bhojana | No | No | 3.7/5 | 88 | +91 9620009302 | Banashankari | Quick Bites | Masala Dosa | South Indian, North Indian | 300.0 | [(&#39;Rated 4.0&#39;, &quot;RATED n Great food and proper... | [] | Buffet | Banashankari | . 4 https://www.zomato.com/bangalore/grand-village... | 10, 3rd Floor, Lakshmi Associates, Gandhi Baza... | Grand Village | No | No | 3.8/5 | 166 | +91 8026612447 r n+91 9901210005 | Basavanagudi | Casual Dining | Panipuri, Gol Gappe | North Indian, Rajasthani | 600.0 | [(&#39;Rated 4.0&#39;, &#39;RATED n Very good restaurant ... | [] | Buffet | Banashankari | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 51717 entries, 0 to 51716 Data columns (total 17 columns): # Column Non-Null Count Dtype -- -- 0 url 51717 non-null object 1 address 51717 non-null object 2 name 51717 non-null object 3 online_order 51717 non-null object 4 book_table 51717 non-null object 5 rate 43942 non-null object 6 votes 51717 non-null int64 7 phone 50509 non-null object 8 location 51696 non-null object 9 rest_type 51490 non-null object 10 dish_liked 23639 non-null object 11 cuisines 51672 non-null object 12 approx_cost(for two people) 51371 non-null float64 13 reviews_list 51717 non-null object 14 menu_item 51717 non-null object 15 listed_in(type) 51717 non-null object 16 listed_in(city) 51717 non-null object dtypes: float64(1), int64(1), object(15) memory usage: 6.7+ MB . Change the structure . We&#39;ll drop the columns which don&#39;t inform us much about the restuarants. . df.drop([&quot;url&quot;, &quot;name&quot;, &quot;phone&quot;, &quot;reviews_list&quot;, &quot;address&quot;, &quot;menu_item&quot;], axis = 1, inplace = True) df.columns . Index([&#39;online_order&#39;, &#39;book_table&#39;, &#39;rate&#39;, &#39;votes&#39;, &#39;location&#39;, &#39;rest_type&#39;, &#39;dish_liked&#39;, &#39;cuisines&#39;, &#39;approx_cost(for two people)&#39;, &#39;listed_in(type)&#39;, &#39;listed_in(city)&#39;], dtype=&#39;object&#39;) . We&#39;ll also rename some of the columns. . df.rename(mapper = {&quot;listed_in(type)&quot;: &quot;type&quot;, &quot;approx_cost(for two people)&quot;: &quot;cost_for_two_people&quot;, &quot;rate&quot;: &quot;rating&quot;}, axis = 1, inplace = True) df.columns . Index([&#39;online_order&#39;, &#39;book_table&#39;, &#39;rating&#39;, &#39;votes&#39;, &#39;location&#39;, &#39;rest_type&#39;, &#39;dish_liked&#39;, &#39;cuisines&#39;, &#39;cost_for_two_people&#39;, &#39;type&#39;, &#39;listed_in(city)&#39;], dtype=&#39;object&#39;) . Correcting datatypes . Now we&#39;ll look at the dtypes of the DataFrame and see if that needs any change. . df.dtypes . online_order object book_table object rating object votes int64 location object rest_type object dish_liked object cuisines object cost_for_two_people float64 type object listed_in(city) object dtype: object . We need to change the rating column to numeric dtype. . # All distinct values in the `rating` column df.rating.unique() . array([&#39;4.1/5&#39;, &#39;3.8/5&#39;, &#39;3.7/5&#39;, &#39;3.6/5&#39;, &#39;4.6/5&#39;, &#39;4.0/5&#39;, &#39;4.2/5&#39;, &#39;3.9/5&#39;, &#39;3.1/5&#39;, &#39;3.0/5&#39;, &#39;3.2/5&#39;, &#39;3.3/5&#39;, &#39;2.8/5&#39;, &#39;4.4/5&#39;, &#39;4.3/5&#39;, &#39;NEW&#39;, &#39;2.9/5&#39;, &#39;3.5/5&#39;, nan, &#39;2.6/5&#39;, &#39;3.8 /5&#39;, &#39;3.4/5&#39;, &#39;4.5/5&#39;, &#39;2.5/5&#39;, &#39;2.7/5&#39;, &#39;4.7/5&#39;, &#39;2.4/5&#39;, &#39;2.2/5&#39;, &#39;2.3/5&#39;, &#39;3.4 /5&#39;, &#39;-&#39;, &#39;3.6 /5&#39;, &#39;4.8/5&#39;, &#39;3.9 /5&#39;, &#39;4.2 /5&#39;, &#39;4.0 /5&#39;, &#39;4.1 /5&#39;, &#39;3.7 /5&#39;, &#39;3.1 /5&#39;, &#39;2.9 /5&#39;, &#39;3.3 /5&#39;, &#39;2.8 /5&#39;, &#39;3.5 /5&#39;, &#39;2.7 /5&#39;, &#39;2.5 /5&#39;, &#39;3.2 /5&#39;, &#39;2.6 /5&#39;, &#39;4.5 /5&#39;, &#39;4.3 /5&#39;, &#39;4.4 /5&#39;, &#39;4.9/5&#39;, &#39;2.1/5&#39;, &#39;2.0/5&#39;, &#39;1.8/5&#39;, &#39;4.6 /5&#39;, &#39;4.9 /5&#39;, &#39;3.0 /5&#39;, &#39;4.8 /5&#39;, &#39;2.3 /5&#39;, &#39;4.7 /5&#39;, &#39;2.4 /5&#39;, &#39;2.1 /5&#39;, &#39;2.2 /5&#39;, &#39;2.0 /5&#39;, &#39;1.8 /5&#39;], dtype=object) . # Remove the non-desired values from the rating column df = df.loc[df.rating != &quot;NEW&quot;] df = df.loc[df.rating != &quot;-&quot;] # Select the first 3 characters and convert the column to numeric df.rating = pd.to_numeric(df.rating.str[:3]) . df.rating.head() . 0 4.1 1 4.1 2 3.8 3 3.7 4 3.8 Name: rating, dtype: float64 . Deal with null values . # Number of null values df.isnull().sum().sort_values(ascending = False) . dish_liked 25948 rating 7775 cost_for_two_people 341 rest_type 225 cuisines 45 location 21 online_order 0 book_table 0 votes 0 type 0 listed_in(city) 0 dtype: int64 . It appears that in all the columns with null values, absence of values neither indicates the value of zero nor informs us on something useful. Thus, it&#39;s better to drop the rows with null values. In the dish_liked column, because the null values account for about half of the data, it&#39;s better to drop the whole column. . df.dropna(subset = [&quot;location&quot;, &quot;rating&quot;, &quot;rest_type&quot;, &quot;cuisines&quot;, &quot;cost_for_two_people&quot;], inplace = True) df.drop([&quot;dish_liked&quot;], axis = 1, inplace = True) df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 41263 entries, 0 to 51716 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 online_order 41263 non-null object 1 book_table 41263 non-null object 2 rating 41263 non-null float64 3 votes 41263 non-null int64 4 location 41263 non-null object 5 rest_type 41263 non-null object 6 cuisines 41263 non-null object 7 cost_for_two_people 41263 non-null float64 8 type 41263 non-null object 9 listed_in(city) 41263 non-null object dtypes: float64(2), int64(1), object(7) memory usage: 3.5+ MB . df.head() . online_order book_table rating votes location rest_type cuisines cost_for_two_people type listed_in(city) . 0 Yes | Yes | 4.1 | 775 | Banashankari | Casual Dining | North Indian, Mughlai, Chinese | 800.0 | Buffet | Banashankari | . 1 Yes | No | 4.1 | 787 | Banashankari | Casual Dining | Chinese, North Indian, Thai | 800.0 | Buffet | Banashankari | . 2 Yes | No | 3.8 | 918 | Banashankari | Cafe, Casual Dining | Cafe, Mexican, Italian | 800.0 | Buffet | Banashankari | . 3 No | No | 3.7 | 88 | Banashankari | Quick Bites | South Indian, North Indian | 300.0 | Buffet | Banashankari | . 4 No | No | 3.8 | 166 | Basavanagudi | Casual Dining | North Indian, Rajasthani | 600.0 | Buffet | Banashankari | . Extras . We&#39;ll also convert the values in rest_type and cuisines columns to lists. . df.cuisines = df.cuisines.str.split(&quot;,&quot;) df.rest_type = df.rest_type.str.split(&quot;,&quot;) . df.head() . online_order book_table rating votes location rest_type cuisines cost_for_two_people type listed_in(city) . 0 Yes | Yes | 4.1 | 775 | Banashankari | [Casual Dining] | [North Indian, Mughlai, Chinese] | 800.0 | Buffet | Banashankari | . 1 Yes | No | 4.1 | 787 | Banashankari | [Casual Dining] | [Chinese, North Indian, Thai] | 800.0 | Buffet | Banashankari | . 2 Yes | No | 3.8 | 918 | Banashankari | [Cafe, Casual Dining] | [Cafe, Mexican, Italian] | 800.0 | Buffet | Banashankari | . 3 No | No | 3.7 | 88 | Banashankari | [Quick Bites] | [South Indian, North Indian] | 300.0 | Buffet | Banashankari | . 4 No | No | 3.8 | 166 | Basavanagudi | [Casual Dining] | [North Indian, Rajasthani] | 600.0 | Buffet | Banashankari | . Exploratory Analysis and Visualization . With these columns in mind, we will try to answer the following questions from the dataset: . What locations are most popular for restaurants in Benagluru? | Which locations have the best rated restaurants? | What relation does the rating and number of votes that a restaurant receives have? What about table booking and online order facility? | Is a restaurant which offers online order facility rated better? | Are restaurants offering expensive food rated better? Does a table booking facility make a difference? | Does the number of cuisines that a restaurant provides have a relation to the rating it recieves? | How do Casual Dining and Fine Dining restaurants differ in their rating? | How do Casual Dining and Fine Dining restaurants differ in their cost_for_two_people? | What are the number of different types of restaurants? | Q1. What locations are most popular for restaurants in Benagluru? . popular_locations = df.location.value_counts().head(15) popular_locations . BTM 3879 Koramangala 5th Block 2297 HSR 1993 Indiranagar 1800 JP Nagar 1710 Jayanagar 1634 Whitefield 1568 Marathahalli 1410 Bannerghatta Road 1226 Koramangala 7th Block 1055 Koramangala 6th Block 1054 Brigade Road 1052 Bellandur 997 Sarjapur Road 854 Koramangala 1st Block 852 Name: location, dtype: int64 . plt.figure(figsize = (10, 8)) sns.barplot(x = popular_locations, y = popular_locations.index) . &lt;AxesSubplot:xlabel=&#39;location&#39;&gt; . The 5 most popular locations for restaurants are BTM, Koramangala 5th Block, HSR, Indiranagar, and JP Nagar, with BTM boasting of nearly 4000 eateries. . Q2. Which locations have the best rated restaurants? . (We&#39;ll select only those locations which have a minimum of 50 restaurants.) . # groupby location and get the count of each location along with the average rating of restaurants in that location. location_rating = df.groupby(by = [&quot;location&quot;])[&quot;rating&quot;].agg([&quot;count&quot;, &quot;mean&quot;]) location_rating.head() . count mean . location . BTM 3879 | 3.571410 | . Banashankari 744 | 3.649866 | . Banaswadi 468 | 3.496368 | . Bannerghatta Road 1226 | 3.509869 | . Basavanagudi 595 | 3.671092 | . # select the locations with 50 minimumn eateries and then sort them by their rating. rated_locations = location_rating.loc[location_rating[&quot;count&quot;] &gt;= 50].sort_values(by = &quot;mean&quot;, ascending = False) # select the top 20 locations. top20_rated_locations = rated_locations[:20] top20_rated_locations.head() . count mean . location . Lavelle Road 481 | 4.141788 | . Koramangala 3rd Block 191 | 4.020419 | . St. Marks Road 343 | 4.017201 | . Koramangala 5th Block 2297 | 4.006661 | . Church Street 546 | 3.992125 | . # plot the observations plt.figure(figsize = (15, 5)) sns.barplot(x = top20_rated_locations.index, y = top20_rated_locations[&quot;mean&quot;]) plt.xticks(rotation = 45) plt.ylim(3.5, 4.3) plt.show() . These are the top 20 locations in Bengaluru based on the eatries&#39; ratings. The average rating in these locations range from 4.14 and 3.8. . Q3. What relation does the rating and number of votes that a restaurant receives have? What about table booking and online order facility? . # Relationship between `rating`, `votes` and `book_table` plt.figure(figsize = (8, 5)) sns.scatterplot(x = &quot;rating&quot;, y = &quot;votes&quot;, hue = &quot;book_table&quot;, data = df, s = 40) . &lt;AxesSubplot:xlabel=&#39;rating&#39;, ylabel=&#39;votes&#39;&gt; . The number of votes look to be directly correlated with the rating of the restaurant, and they look to increase exponentially with the rating after a critical point. This is to be expected because better restaurants would atrract more customers and thus more votes. . Also, the restaurants which don&#39;t provide booking facility are clustered in low ratings and less number of votes. In other words, more popular restaurants with high ratings are more likely to offer table booking facility, which can also be seen the following graph. . sns.boxplot(x = &quot;book_table&quot;, y = &quot;rating&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;book_table&#39;, ylabel=&#39;rating&#39;&gt; . Now, we can look at the scatterplot for rating, votes and online_order . # Relationship between `rating`, `votes` and `online_order` plt.figure(figsize = (8, 5)) sns.scatterplot(x = &quot;rating&quot;, y = &quot;votes&quot;, hue = &quot;online_order&quot;, data = df, s = 40) . &lt;AxesSubplot:xlabel=&#39;rating&#39;, ylabel=&#39;votes&#39;&gt; . The data points for online order facility are scattered in the graph, and thus the data doesn&#39;t reveal any relationship between these variables. But, it may be worth exploring its relationship with the votes and the rating individually, which we&#39;ll do in the following sections. . Q4. Is a restaurant which offers online order facility rated better? . sns.violinplot(x = &quot;online_order&quot;, y = &quot;rating&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;online_order&#39;, ylabel=&#39;rating&#39;&gt; . As discussed in the previous question, the rating doesn&#39;t seem to be any different between the restaurants which offer online order facility and which don&#39;t. . Q5. Are restaurants offering expensive food rated better? Does a table booking facility make a difference? . plt.figure(figsize = (8, 5)) sns.scatterplot(x = &quot;rating&quot;, y = &quot;cost_for_two_people&quot;, data = df, hue = &#39;book_table&#39;) . &lt;AxesSubplot:xlabel=&#39;rating&#39;, ylabel=&#39;cost_for_two_people&#39;&gt; . Although there are hints of an exponential relationship between the cost for two people and the rating of a restaurant, most of the data is clustered in low cost, and thus because of lack of data points for expensive restaurants, the data is inconclusive for this relations. But we can study book table facility individuallly with the cost for two people, where restaurants which offer book_table facility seem to be more expensive than restaurants which don&#39;t. . sns.boxplot(x = &quot;book_table&quot;, y = &quot;cost_for_two_people&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;book_table&#39;, ylabel=&#39;cost_for_two_people&#39;&gt; . This graph also supports the idea that table booking is correlated with the cost for two people. . Q6. Does the number of cuisines that a restaurant provides have a relation to the rating it recieves? . The cuisines column shows the all the cuisines that a restaurant offers. We can add column to the DataFrame to store the number of cuisines that a restaurant offers. . df[&quot;no_of_cuisines&quot;] = df.cuisines.str.len() df[&quot;no_of_cuisines&quot;].head() . 0 3 1 3 2 3 3 2 4 2 Name: no_of_cuisines, dtype: int64 . plt.figure(figsize = (8, 5)) sns.stripplot(x = &quot;no_of_cuisines&quot;, y = &quot;rating&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;no_of_cuisines&#39;, ylabel=&#39;rating&#39;&gt; . The rating seems to become more concentrated towards mean as the no. of cuisines that a restaurant offers goes up. But, this could also be a artifact of low no. of restaurants offering higher no. of cuisines. In general, the mean of rating also seems to go up with the increase in no. of cuisines, but the graph is inconclusive. We&#39;ll explore this more in the boxplot. . sns.boxplot(x = &quot;no_of_cuisines&quot;, y = &quot;rating&quot;, data = df) . &lt;AxesSubplot:xlabel=&#39;no_of_cuisines&#39;, ylabel=&#39;rating&#39;&gt; . This graph reflects the relationship better and does support the idea that restaurants offering more no of cuisines are usually rated better. . Q7. How do Casual Dining and Fine Dining restaurants differ in their cost_for_two_people? . #Look at the dataframe df.head() . online_order book_table rating votes location rest_type cuisines cost_for_two_people type listed_in(city) no_of_cuisines . 0 Yes | Yes | 4.1 | 775 | Banashankari | [Casual Dining] | [North Indian, Mughlai, Chinese] | 800.0 | Buffet | Banashankari | 3 | . 1 Yes | No | 4.1 | 787 | Banashankari | [Casual Dining] | [Chinese, North Indian, Thai] | 800.0 | Buffet | Banashankari | 3 | . 2 Yes | No | 3.8 | 918 | Banashankari | [Cafe, Casual Dining] | [Cafe, Mexican, Italian] | 800.0 | Buffet | Banashankari | 3 | . 3 No | No | 3.7 | 88 | Banashankari | [Quick Bites] | [South Indian, North Indian] | 300.0 | Buffet | Banashankari | 2 | . 4 No | No | 3.8 | 166 | Basavanagudi | [Casual Dining] | [North Indian, Rajasthani] | 600.0 | Buffet | Banashankari | 2 | . We&#39;ll need to explode the rest_type column to extract information. . # extract information from `rest_type` column rest_type_exploded = df.explode(column = &quot;rest_type&quot;) rest_type_exploded[&quot;rest_type&quot;] = rest_type_exploded[&quot;rest_type&quot;].str.strip() rest_type_exploded.head() . online_order book_table rating votes location rest_type cuisines cost_for_two_people type listed_in(city) no_of_cuisines . 0 Yes | Yes | 4.1 | 775 | Banashankari | Casual Dining | [North Indian, Mughlai, Chinese] | 800.0 | Buffet | Banashankari | 3 | . 1 Yes | No | 4.1 | 787 | Banashankari | Casual Dining | [Chinese, North Indian, Thai] | 800.0 | Buffet | Banashankari | 3 | . 2 Yes | No | 3.8 | 918 | Banashankari | Cafe | [Cafe, Mexican, Italian] | 800.0 | Buffet | Banashankari | 3 | . 2 Yes | No | 3.8 | 918 | Banashankari | Casual Dining | [Cafe, Mexican, Italian] | 800.0 | Buffet | Banashankari | 3 | . 3 No | No | 3.7 | 88 | Banashankari | Quick Bites | [South Indian, North Indian] | 300.0 | Buffet | Banashankari | 2 | . # separate data for casual dining restaurants and fine dining restaurants. fine_dining_rest = rest_type_exploded.loc[rest_type_exploded.rest_type == &quot;Fine Dining&quot;] casual_dining_rest = rest_type_exploded.loc[rest_type_exploded.rest_type == &quot;Casual Dining&quot;] . # plot the data sns.kdeplot(fine_dining_rest.cost_for_two_people, fill = True) sns.kdeplot(casual_dining_rest.cost_for_two_people, fill = True) plt.legend([&quot;Fine Dining&quot;, &quot;Casual Dining&quot;]) plt.show() . As expected, fine dining restaurants are much more expensive than casual dining restaurants. . Q8. How do Casual Dining and Fine Dining restaurants differ in their rating? . sns.kdeplot(fine_dining_rest.rating, fill = True) sns.kdeplot(casual_dining_rest.rating, fill = True) plt.legend([&quot;Fine Dining&quot;, &quot;Casual Dining&quot;]) plt.show() . Fine dining restaurants are usually rated better and their ratings show much less variance than the ratings of casual dining restaurants. . Q9. What are the five most common types of restaurants? . # Five most common types of restaurants most_common_types_of_restaurants = rest_type_exploded.rest_type.value_counts() most_common_types_of_restaurants.head() . Quick Bites 15144 Casual Dining 12188 Cafe 4604 Delivery 2946 Dessert Parlor 2695 Name: rest_type, dtype: int64 . sns.barplot(x = most_common_types_of_restaurants.head(), y = most_common_types_of_restaurants.index[:5]) plt.show() . Q10. What are the top 5 rated restaurants in type and no_of_cuisines combined? . In other words, which type and no_of_cuisines combinations gather the highest ratings? . # group and extract data for different `types` and `no of cuisines` rating_data = df.groupby(by = [&quot;type&quot;, &quot;no_of_cuisines&quot;])[&quot;rating&quot;].agg(&quot;mean&quot;) rating_data.head() . type no_of_cuisines Buffet 1 3.995402 2 3.903509 3 3.911111 4 4.063212 5 4.217284 Name: rating, dtype: float64 . Now, we&#39;ll make a 2D datarame out of this multiindexed pandas Series. . rating_data_df = rating_data.unstack() rating_data_df.head() . no_of_cuisines 1 2 3 4 5 6 7 8 . type . Buffet 3.995402 | 3.903509 | 3.911111 | 4.063212 | 4.217284 | 3.926667 | 3.500000 | 4.062500 | . Cafes 3.643697 | 3.760366 | 3.941109 | 3.938434 | 4.015942 | 4.090698 | 4.156757 | 4.084615 | . Delivery 3.593535 | 3.629772 | 3.654311 | 3.739603 | 3.817625 | 3.901439 | 3.999333 | 3.801149 | . Desserts 3.698520 | 3.772140 | 3.755556 | 3.934932 | 4.125610 | 4.084211 | 3.920000 | 4.060000 | . Dine-out 3.613442 | 3.603628 | 3.687592 | 3.825194 | 3.903406 | 4.006091 | 4.035433 | 3.982051 | . # plot the data plt.figure(figsize = (9, 7)) fig = sns.heatmap(data = rating_data_df, annot = True, cmap = &quot;rocket_r&quot;) fig.set(xlabel = &quot;No. of cuisines&quot;, ylabel = &quot;Type&quot;) plt.show() . From the plot, Pubs and bars which offer more than 3 cuisines are all rated high. Similarly, Drinks &amp; nightlife restaurants with multiple cuisines are also rated really high. . We can get the top 5 combinations from the rating_data Series. . # top 5 combinations for `type` and `no_of_cuisines` rating_data.sort_values(ascending = False).head() . type no_of_cuisines Drinks &amp; nightlife 7 4.464706 Pubs and bars 7 4.455556 8 4.400000 Drinks &amp; nightlife 6 4.264865 8 4.250000 Name: rating, dtype: float64 . Q11. What is the relationship between the type and cost_for_two_people? . plt.figure(figsize = (8, 6)) sns.boxplot(x = &quot;type&quot;, y = &quot;cost_for_two_people&quot;, data = df) plt.xticks(rotation = 45) plt.show() . First thing to note is that there are quite a few outliers in the data, almost all of them offering much more expensive food from the rest of the distribution. Also buffet, drinks &amp; nighlife, and pubs are much more expensive than eateries of the type desserts and delivery. . Summary and Conclusion . Many questions could be asked and explored from the Zomato Dataset, but here we tried to answer 11 of them. We studied all the restaurants in Bengaluru, who have registered on Zomato, and tried to explore multiple variables&#39; relationship with the restaurants&#39; ratings. We also studied what factors go along with the food cost for two people in these restaurants. . There are two important things to note here before making any conclusions. First, all the analysis we did might apply only to restaurants registered on Zomato and other similar online platforms, and might differ significantly if we explore the food industry offline. Second really important thing is all the relationships that we studied are correlational in nature. This project thus does not establish causal relationships, although it might suggest some and can be taken as an inspiration to conduct actual experimental studies to explore the variables discussed here. Keeping in mind this, we can look at what we actually did establish in this EDA of Zomato Dataset. . The most popular places for restaurants in Benagluru are BTM, Koramangala 5th Block, HSR, Indiranagar, and JP Nagar, with BTM boasting of nearly 4000 eateries. | The top 5 locations according to avg rating of restaurants are Lavelle Road, Koramangala 3rd Block, St. Marks Road, Koramangala 5th Block and Church Street. | Restaurants with higher ratings have generally received more votes than the restaurants with lower rating and they are more likely to offer table booking facility. | Also, restaurants offering table booking facility are also generally more expensive. | Restaurants offering more no. of cuisines are also on average rated better. | Fine dining restaurants are much more expensive than casual dining restaurants and they are also usually rated better with much less variance in the ratings. | Quick Bites and Casual Dining restaurants but are the most popular types of restaurant in Bengaluru on Zomato. | Pubs and bars which offer more than 3 cuisines are all rated high. Similarly, Drinks &amp; nightlife restaurants with multiple cuisines are also rated really highly. | Buffet, drinks &amp; nighlife, and pubs are much more expensive than eateries of the type desserts and delivery. | For the other questions we asked, the data was more or less inconclusive. We may need more extensive data to answer those questions. | . Apart from these inferences, many more interesting relationships can be studied and should be explored from this data. . Thanks for reading. .",
            "url": "https://ncitshubham.github.io/blogs/2021/11/23/eda-and-data-vis-of-zomato-dataset.html",
            "relUrl": "/2021/11/23/eda-and-data-vis-of-zomato-dataset.html",
            "date": " • Nov 23, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Scraping Data Science Jobs on in.indeed.com",
            "content": "Introduction . Indeed.com is a worldwide employment website for job listings. Globally, it hosts millions of job listings on thousands of jobs. In this project, we are interested in the &#39;Data Science&#39; related job listings on https://in.indeed.com/ . Thus, we&#39;ll scrape the website for this information and save that to a csv file for future use. In order to do this, we&#39;ll use the following tools: . Python as the programming language | Requests library for downloading the webpage contents | BeautifulSoup library for finding and accessing the relevant information from the downloaded webpage. | Numpy library for handling missing values. | Pandas library for saving the accessed information to a csv file. | . Here are the steps we&#39;ll follow: . We&#39;ll scrape first 30 pages from https://in.indeed.com/jobs?q=data%20science | We&#39;ll get a list of all 15 jobs on each page. | For each job, we&#39;ll grab the Job Title, Salary, Location, Company Name, and Company Rating. | We&#39;ll save all of the information in a csv file in the following format: . Title,Salary,Location,Company Name,Company Rating Data science,&quot;₹35,000 - ₹45,000 a month&quot;,&quot;Mumbai, Maharashtra&quot;,V Digitech Sevices Data Science ( 2 - 8 Yrs) - Remote,&quot;₹8,00,000 - ₹20,00,000 a year&quot;,Remote,ProGrad Data Science Internship,&quot;₹10,000 a month&quot;,&quot;Gurgaon, Haryana&quot;,Zigram . | . Initial Setup . Import the required libraries . import requests from bs4 import BeautifulSoup import pandas as pd from numpy import nan . Set up base URL and the user-agent. . # The base_url is grabbed after searching &#39;Data Science&#39; on in.indeed.com # The start value in the base_url will increment by 10 to access each following page. base_url = &quot;https://in.indeed.com/jobs?q=data%20science&amp;start={}&quot; header = {&quot;User-Agent&quot;: &quot;Mozilla/5.0&quot;} . Create a dictionary to save all information. . jobs = {&quot;Job Title&quot;: [], &quot;Salary&quot;: [], &quot;Location&quot;: [], &quot;Company Name&quot;: [], &quot;Company Rating&quot;: []} . Scrape the search result webpage. . Download webpage and create a BeautifulSoup object . def get_soup(url): &#39;&#39;&#39; This function will download the webpage for the url supplied as argument and return the BeautifulSoup object for the webpage which can be used to grab required information for the webpage. &#39;&#39;&#39; response = requests.get(url, &quot;html.parser&quot;, headers = header) if response.status_code != 200: raise Exception(&#39;Failed to load page {}&#39;.format(url)) soup = BeautifulSoup(response.text) return soup . Example for get_soup . soup = get_soup(base_url.format(10)) type(soup) . bs4.BeautifulSoup . Create a transform function . Now, we&#39;ll create a transform function to grab the list of jobs from the result webpage. . To do this, we&#39;ll pick td tags with the class: resultContent . . def transform(soup): # find all the job listings on the webpage. jobs_tags = soup.find_all(&quot;td&quot;, class_ = &quot;resultContent&quot;) # for each job, call helper functions to grab information about the job # and save that to jobs dictionary. for job in jobs_tags: jobs[&quot;Job Title&quot;].append(get_job_title(job)) jobs[&quot;Salary&quot;].append(get_job_salary(job)) jobs[&quot;Location&quot;].append(get_company_location(job)) jobs[&quot;Company Name&quot;].append(get_company_name(job)) jobs[&quot;Company Rating&quot;].append(get_company_rating(job)) . Example for finding the job tags . job_tags = soup.find_all(&quot;td&quot;, class_ = &quot;resultContent&quot;) print(len(job_tags)) . 15 . print(job_tags[0].text) . newData Analyst/ScientistSopra Steria3.5Noida, Uttar Pradesh . Create helper functions . Create helper functions to grab job information for each job and store that in the jobs dictionary. . First grab Job Title. . def get_job_title(job): &#39;&#39;&#39; Function to grab the job title. Because some job titles have a prefix new in their job titles, this function will automatically detect this prefix and return the title sans &#39;new&#39; in the job title. &#39;&#39;&#39; title = job.find(class_ = &quot;jobTitle&quot;).text if title[:3] == &quot;new&quot;: return title[3:] else: return title get_job_title(job_tags[0]) . &#39;Data Analyst/Scientist&#39; . Now, we&#39;ll grab the job salary, if the listing has one. . def get_job_salary(job): salary = job.find(&quot;div&quot;, class_ = &quot;salary-snippet&quot;) if salary: return salary.text else: return nan get_job_salary(job_tags[1]) . &#39;₹500 an hour&#39; . Similarly, we&#39;ll grab the company name, location, and its rating. . def get_company_name(job): &#39;&#39;&#39; Returns the company name for the supp&#39;&#39;&#39; return job.find(class_ = &quot;companyName&quot;).text def get_company_location(job): &#39;&#39;&#39; Returns the company location for the supplied job tag &#39;&#39;&#39; return job.find(class_ = &quot;companyLocation&quot;).text def get_company_rating(job): &#39;&#39;&#39; Returns the company rating for the supplied job tag &#39;&#39;&#39; rating = job.find(class_ = &quot;ratingNumber&quot;) if rating: return float(rating.text) else: return nan # Example print(get_company_name(job_tags[0]), get_company_location(job_tags[0]), get_company_rating(job_tags[0]), sep = &quot; n&quot;) . Sopra Steria Noida, Uttar Pradesh 3.5 . Putting it all together . We&#39;ll use a for loop to loop through 30 search result pages. Within this loop, we can apply the get_soup function to download these pages and the transform function to parse through all job listings from these pages and save the information in the jobs dictionary. We&#39;ll then use this dictionary to create a pandas DataFrame, which can then be saved to a csv file. . for page in range(0, 310, 10): print(f&quot;Scraping page {page}...&quot;) soup = get_soup(base_url.format(page)) transform(soup) . Scraping page 0... Scraping page 10... Scraping page 20... Scraping page 30... Scraping page 40... Scraping page 50... Scraping page 60... Scraping page 70... Scraping page 80... Scraping page 90... Scraping page 100... Scraping page 110... Scraping page 120... Scraping page 130... Scraping page 140... Scraping page 150... Scraping page 160... Scraping page 170... Scraping page 180... Scraping page 190... Scraping page 200... Scraping page 210... Scraping page 220... Scraping page 230... Scraping page 240... Scraping page 250... Scraping page 260... Scraping page 270... Scraping page 280... Scraping page 290... Scraping page 300... . # create a pandas DataFrame of the scraped data jobs_df = pd.DataFrame(jobs) jobs_df.head() . Job Title Salary Location Company Name Company Rating . 0 Technology Analyst: Data Science | Machine Lea... | NaN | Bengaluru, Karnataka | Infosys Limited | 3.9 | . 1 Analyst-Data Science | NaN | Gurgaon, Haryana+2 locations | Amex | NaN | . 2 Junior Data Scientist Data Science Chennai, India | NaN | Tamil Nadu | Applied Data Finance | NaN | . 3 Data Engineer – EPH | NaN | India | Kyndryl | 3.4 | . 4 Data Science Analyst | NaN | India | Helius Technologies | NaN | . # save data to a csv file jobs_df.to_csv(&quot;Data_Science_jobs_from_indeed.com.csv&quot;, index = None, encoding = &quot;utf-8&quot;) . Summary . This was a short project, where we looked into how job listings can be scraped from Indeed.com. We craped 30 pages of job listings with tags Data Science. This gave us a total of 450 job listings with the details like the job title, salary, company, location, etc. We then saved this scraped data into a csv file for future use. .",
            "url": "https://ncitshubham.github.io/blogs/2021/10/19/scraping-indeed.com-for-data-science-jobs.html",
            "relUrl": "/2021/10/19/scraping-indeed.com-for-data-science-jobs.html",
            "date": " • Oct 19, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Exploratory Data Analysis of Road Accidents in USA",
            "content": "Introduction . Every year 1.3 million people die as a result of a road traffic crash around the world. And 20 - 50 million people suffer non-fatal injuries, with many incurring a disability as a result of their injury.1 In USA alone, there were 33,244 fatal motor vehicle crashes in 2019 in which 36,096 deaths occurred.2 . In this project, we are going to study the &#39;US Accidents&#39; dataset provided by Sobhan Moosavi on https://www.kaggle.com. We can analyse this data to gain some really interesting insights about road accidents in USA, such as the impact of environmental stimuli on accidental occurance, the change in the occurance of accidents with change in months, or which 5 states have the highest (or lowest) number of accidents. [Note: This Dataset covers 49 mainland states of the USA (excluding Alaska) for the time period: February 2016 to Dec 2020] . We will use the approach of Exploratory Data Analysis (EDA) for studying this data, which is used to analyse datasets to summarize their main characteristics, often using statistical graphics and other data visualization methods. EDA can help in seeing what the data can tell us beyond the formal modeling or hypothesis testing task.3 . Because this dataset is huge, with dozens of features, it can be used to answer a lot of questions. For this project, we are going to limit ourselves to 5-10 questions and then try to answer these questions. . Setup . Import the necessary libraries and get the file_path for the dataset. . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # data visualization import seaborn as sns # data visualization import os for dirname, _, filenames in os.walk(&#39;US_Accidents_Dec20_updated.csv/&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . US_Accidents_Dec20_updated.csv/US_Accidents_Dec20_updated.csv . # Read the file file_path = &quot;US_Accidents_Dec20_updated.csv/US_Accidents_Dec20_updated.csv&quot; us_accidents = pd.read_csv(file_path) . pd.set_option(&quot;display.max_columns&quot;, None) us_accidents.head() . ID Severity Start_Time End_Time Start_Lat Start_Lng End_Lat End_Lng Distance(mi) Description Number Street Side City County State Zipcode Country Timezone Airport_Code Weather_Timestamp Temperature(F) Wind_Chill(F) Humidity(%) Pressure(in) Visibility(mi) Wind_Direction Wind_Speed(mph) Precipitation(in) Weather_Condition Amenity Bump Crossing Give_Way Junction No_Exit Railway Roundabout Station Stop Traffic_Calming Traffic_Signal Turning_Loop Sunrise_Sunset Civil_Twilight Nautical_Twilight Astronomical_Twilight . 0 A-2716600 | 3 | 2016-02-08 00:37:08 | 2016-02-08 06:37:08 | 40.10891 | -83.09286 | 40.11206 | -83.03187 | 3.230 | Between Sawmill Rd/Exit 20 and OH-315/Olentang... | NaN | Outerbelt E | R | Dublin | Franklin | OH | 43017 | US | US/Eastern | KOSU | 2016-02-08 00:53:00 | 42.1 | 36.1 | 58.0 | 29.76 | 10.0 | SW | 10.4 | 0.00 | Light Rain | False | False | False | False | False | False | False | False | False | False | False | False | False | Night | Night | Night | Night | . 1 A-2716601 | 2 | 2016-02-08 05:56:20 | 2016-02-08 11:56:20 | 39.86542 | -84.06280 | 39.86501 | -84.04873 | 0.747 | At OH-4/OH-235/Exit 41 - Accident. | NaN | I-70 E | R | Dayton | Montgomery | OH | 45424 | US | US/Eastern | KFFO | 2016-02-08 05:58:00 | 36.9 | NaN | 91.0 | 29.68 | 10.0 | Calm | NaN | 0.02 | Light Rain | False | False | False | False | False | False | False | False | False | False | False | False | False | Night | Night | Night | Night | . 2 A-2716602 | 2 | 2016-02-08 06:15:39 | 2016-02-08 12:15:39 | 39.10266 | -84.52468 | 39.10209 | -84.52396 | 0.055 | At I-71/US-50/Exit 1 - Accident. | NaN | I-75 S | R | Cincinnati | Hamilton | OH | 45203 | US | US/Eastern | KLUK | 2016-02-08 05:53:00 | 36.0 | NaN | 97.0 | 29.70 | 10.0 | Calm | NaN | 0.02 | Overcast | False | False | False | False | True | False | False | False | False | False | False | False | False | Night | Night | Night | Day | . 3 A-2716603 | 2 | 2016-02-08 06:15:39 | 2016-02-08 12:15:39 | 39.10148 | -84.52341 | 39.09841 | -84.52241 | 0.219 | At I-71/US-50/Exit 1 - Accident. | NaN | US-50 E | R | Cincinnati | Hamilton | OH | 45202 | US | US/Eastern | KLUK | 2016-02-08 05:53:00 | 36.0 | NaN | 97.0 | 29.70 | 10.0 | Calm | NaN | 0.02 | Overcast | False | False | False | False | True | False | False | False | False | False | False | False | False | Night | Night | Night | Day | . 4 A-2716604 | 2 | 2016-02-08 06:51:45 | 2016-02-08 12:51:45 | 41.06213 | -81.53784 | 41.06217 | -81.53547 | 0.123 | At Dart Ave/Exit 21 - Accident. | NaN | I-77 N | R | Akron | Summit | OH | 44311 | US | US/Eastern | KAKR | 2016-02-08 06:54:00 | 39.0 | NaN | 55.0 | 29.65 | 10.0 | Calm | NaN | NaN | Overcast | False | False | False | False | False | False | False | False | False | False | False | False | False | Night | Night | Day | Day | . Data Preparation and Cleaning . Premilinary analysis of dataset . us_accidents.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1516064 entries, 0 to 1516063 Data columns (total 47 columns): # Column Non-Null Count Dtype -- -- 0 ID 1516064 non-null object 1 Severity 1516064 non-null int64 2 Start_Time 1516064 non-null object 3 End_Time 1516064 non-null object 4 Start_Lat 1516064 non-null float64 5 Start_Lng 1516064 non-null float64 6 End_Lat 1516064 non-null float64 7 End_Lng 1516064 non-null float64 8 Distance(mi) 1516064 non-null float64 9 Description 1516064 non-null object 10 Number 469969 non-null float64 11 Street 1516064 non-null object 12 Side 1516064 non-null object 13 City 1515981 non-null object 14 County 1516064 non-null object 15 State 1516064 non-null object 16 Zipcode 1515129 non-null object 17 Country 1516064 non-null object 18 Timezone 1513762 non-null object 19 Airport_Code 1511816 non-null object 20 Weather_Timestamp 1485800 non-null object 21 Temperature(F) 1473031 non-null float64 22 Wind_Chill(F) 1066748 non-null float64 23 Humidity(%) 1470555 non-null float64 24 Pressure(in) 1479790 non-null float64 25 Visibility(mi) 1471853 non-null float64 26 Wind_Direction 1474206 non-null object 27 Wind_Speed(mph) 1387202 non-null float64 28 Precipitation(in) 1005515 non-null float64 29 Weather_Condition 1472057 non-null object 30 Amenity 1516064 non-null bool 31 Bump 1516064 non-null bool 32 Crossing 1516064 non-null bool 33 Give_Way 1516064 non-null bool 34 Junction 1516064 non-null bool 35 No_Exit 1516064 non-null bool 36 Railway 1516064 non-null bool 37 Roundabout 1516064 non-null bool 38 Station 1516064 non-null bool 39 Stop 1516064 non-null bool 40 Traffic_Calming 1516064 non-null bool 41 Traffic_Signal 1516064 non-null bool 42 Turning_Loop 1516064 non-null bool 43 Sunrise_Sunset 1515981 non-null object 44 Civil_Twilight 1515981 non-null object 45 Nautical_Twilight 1515981 non-null object 46 Astronomical_Twilight 1515981 non-null object dtypes: bool(13), float64(13), int64(1), object(20) memory usage: 412.1+ MB . # basic statistics us_accidents.describe() . Severity Start_Lat Start_Lng End_Lat End_Lng Distance(mi) Number Temperature(F) Wind_Chill(F) Humidity(%) Pressure(in) Visibility(mi) Wind_Speed(mph) Precipitation(in) . count 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 4.699690e+05 | 1.473031e+06 | 1.066748e+06 | 1.470555e+06 | 1.479790e+06 | 1.471853e+06 | 1.387202e+06 | 1.005515e+06 | . mean 2.238630e+00 | 3.690056e+01 | -9.859919e+01 | 3.690061e+01 | -9.859901e+01 | 5.872617e-01 | 8.907533e+03 | 5.958460e+01 | 5.510976e+01 | 6.465960e+01 | 2.955495e+01 | 9.131755e+00 | 7.630812e+00 | 8.477855e-03 | . std 6.081481e-01 | 5.165653e+00 | 1.849602e+01 | 5.165629e+00 | 1.849590e+01 | 1.632659e+00 | 2.242190e+04 | 1.827316e+01 | 2.112735e+01 | 2.325986e+01 | 1.016756e+00 | 2.889112e+00 | 5.637364e+00 | 1.293168e-01 | . min 1.000000e+00 | 2.457022e+01 | -1.244976e+02 | 2.457011e+01 | -1.244978e+02 | 0.000000e+00 | 0.000000e+00 | -8.900000e+01 | -8.900000e+01 | 1.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 25% 2.000000e+00 | 3.385422e+01 | -1.182076e+02 | 3.385420e+01 | -1.182077e+02 | 0.000000e+00 | 1.212000e+03 | 4.700000e+01 | 4.080000e+01 | 4.800000e+01 | 2.944000e+01 | 1.000000e+01 | 4.600000e+00 | 0.000000e+00 | . 50% 2.000000e+00 | 3.735113e+01 | -9.438100e+01 | 3.735134e+01 | -9.437987e+01 | 1.780000e-01 | 4.000000e+03 | 6.100000e+01 | 5.700000e+01 | 6.800000e+01 | 2.988000e+01 | 1.000000e+01 | 7.000000e+00 | 0.000000e+00 | . 75% 2.000000e+00 | 4.072593e+01 | -8.087469e+01 | 4.072593e+01 | -8.087449e+01 | 5.940000e-01 | 1.010000e+04 | 7.300000e+01 | 7.100000e+01 | 8.400000e+01 | 3.004000e+01 | 1.000000e+01 | 1.040000e+01 | 0.000000e+00 | . max 4.000000e+00 | 4.900058e+01 | -6.711317e+01 | 4.907500e+01 | -6.710924e+01 | 1.551860e+02 | 9.999997e+06 | 1.706000e+02 | 1.130000e+02 | 1.000000e+02 | 5.804000e+01 | 1.400000e+02 | 9.840000e+02 | 2.400000e+01 | . Number of null values in the dataset, column vise . null_val_cols = us_accidents.isnull().sum() null_val_cols = null_val_cols[null_val_cols &gt; 0].sort_values(ascending = False) null_val_cols . Number 1046095 Precipitation(in) 510549 Wind_Chill(F) 449316 Wind_Speed(mph) 128862 Humidity(%) 45509 Visibility(mi) 44211 Weather_Condition 44007 Temperature(F) 43033 Wind_Direction 41858 Pressure(in) 36274 Weather_Timestamp 30264 Airport_Code 4248 Timezone 2302 Zipcode 935 City 83 Sunrise_Sunset 83 Civil_Twilight 83 Nautical_Twilight 83 Astronomical_Twilight 83 dtype: int64 . # plot percentage of missing values missing_percentage = null_val_cols / len(us_accidents) * 100 missing_percentage.plot(kind = &#39;barh&#39;, figsize = (14, 8)) plt.xlabel(&quot;Percentage of missing values&quot;) plt.ylabel(&quot;Columns&quot;) plt.show() . Observations . The Number column represents the street number in address record. | It has over a million null values. | . Because we already have the location data from the Start_Lat, Start_Lng, County and State features, we can choose to drop this column from the dataset. With regards to columns representing weather conditions, we can choose to drop the columns having a significant number of null values, and impute other columns to the mean values from the same State and month, which should represent similar weather conditions. . us_accidents.drop(labels = [&quot;Number&quot;, &quot;Precipitation(in)&quot;], axis = 1, inplace = True) us_accidents.shape . (1516064, 45) . We&#39;ll also convert the Start_Time column to datetime dtype, so that we can use use it to apply datetime functions later on. . us_accidents.Start_Time = us_accidents.Start_Time.astype(&quot;datetime64&quot;) . Exploratory Analysis and Visualization . With these columns in mind, we will try to answer the following questions from the dataset: . Are there more accidents in warmer or colder areas? | Which 5 states have the highest number of accidents? | Among the top 100 cities in number of accidents, which states do they belong to most frequently. | What time of the day are accidents most frequent in? | Which days of the week have the most accidents? | Which months have the most accidents? | What is the trend of accidents year over year (decreasing/increasing?) | How does accident severity change with change in precipitation? | We can start with plotting longitude and lattitude data to get a sense of where is the data concentrated . lat, long = us_accidents.Start_Lat, us_accidents.Start_Lng . # plot locations of accidents plt.figure(figsize = (10, 6)) sns.scatterplot(x = long, y = lat, s = 0.5) . &lt;AxesSubplot:xlabel=&#39;Start_Lng&#39;, ylabel=&#39;Start_Lat&#39;&gt; . It appears that accidents are more concentrated near the coasts, which are more populated areas of US. There seems to be a sharp decline in the mid - mid-western US. Apart from lack of accidents in those areas, it could also be a signal of poor data collection in those states. . Q1. Are there more accidents in warmer or colder areas? . First, we&#39;ll plot a histogram for the temperature data. . us_accidents[&quot;Temperature(F)&quot;].plot(kind = &quot;hist&quot;, logy = True) plt.xlabel(&quot;Temperature(F)&quot;) plt.show() . print(f&#39;&#39;&#39;Mean: {us_accidents[&quot;Temperature(F)&quot;].mean()} Skewness: {us_accidents[&quot;Temperature(F)&quot;].skew()} Kurtosis: {us_accidents[&quot;Temperature(F)&quot;].kurtosis()}&#39;&#39;&#39;) . Mean: 58.5816432896377 Skewness: -0.30533266112446805 Kurtosis: -0.13261978426888454 . Observations . Data is normally distributed with very low skewness and kurtosis values. | Thus, the data doesn&#39;t support the idea that warmer areas have more accidents than colder ones or vice versa. | . Q2. Which 5 states have the highest number of accidents? . accidents_by_states = us_accidents.State.value_counts() print(&quot;The top 5 states in terms of number of accidents are:&quot;) accidents_by_states.head() . The top 5 states in terms of number of accidents are: . CA 448833 FL 153007 OR 87484 TX 75142 NY 60974 Name: State, dtype: int64 . plt.figure(figsize = (16, 5)) sns.barplot(x = accidents_by_states.index, y = accidents_by_states.values) . &lt;AxesSubplot:&gt; . Q3. Among the top 100 cities in number of accidents, which states do they belong to most frequently. . city_accidents = us_accidents.City.value_counts() city_accidents . Los Angeles 39984 Miami 36233 Charlotte 22203 Houston 20843 Dallas 19497 ... Manzanita 1 West Brooklyn 1 Garfield Heights 1 Belding 1 American Fork-Pleasant Grove 1 Name: City, Length: 10657, dtype: int64 . city_states = us_accidents.groupby(&quot;City&quot;)[&quot;State&quot;].aggregate(pd.Series.mode) city_states . City Aaronsburg PA Abbeville SC Abbotsford WI Abbottstown PA Aberdeen MD .. Zortman MT Zumbro Falls MN Zumbrota MN Zuni VA Zwingle IA Name: State, Length: 8769, dtype: object . top_100_cities = pd.concat([city_accidents, city_states], axis = 1)[:100] top_100_cities . City State . Los Angeles 27760 | CA | . Miami 26831 | FL | . Orlando 10772 | FL | . Dallas 10522 | TX | . Charlotte 10312 | NC | . ... ... | ... | . Flint 1465 | MI | . Hollywood 1431 | FL | . Eugene 1427 | OR | . Silver Spring 1425 | MD | . Birmingham 1422 | AL | . 100 rows × 2 columns . #Most frequent states in the top 100 cities in the number of accidents most_freq_states = top_100_cities.State.value_counts() most_freq_states . CA 35 FL 13 TX 5 NY 4 OR 4 MI 3 VA 3 LA 3 PA 3 SC 2 MO 2 UT 2 AZ 2 TN 2 MN 2 NC 2 OH 2 OK 1 NJ 1 MD 1 KY 1 IN 1 CO 1 DC 1 WA 1 IL 1 GA 1 AL 1 Name: State, dtype: int64 . # plot most_freq_states.plot(kind = &quot;bar&quot;, figsize = (15,5)) plt.xlabel(&quot;States&quot;) plt.ylabel(&quot;Frequency in top 100 cities by number of accidents&quot;) plt.title(&quot;Most frequent states in the top 100 cities in the number of accidents&quot;) plt.show() . Q4. What time of the day are accidents most frequent in? . Do more accidents tend to occur at a particular time of the day? . # plot us_accidents.Start_Time.dt.hour.plot(kind = &#39;hist&#39;, bins = 24) plt.xlabel(&quot;Hour of the day&quot;) plt.show() . Observations . It appears that accidents tend to occur more in the morning between 7-9 and in the afternoon between 13-17. Office hours could be the reason behind this trend. We can examine this by separating the data for weekdays and weekends. . # plot weekdays = us_accidents.Start_Time.dt.dayofweek &lt; 5 us_accidents.loc[weekdays].Start_Time.dt.hour.plot(kind = &#39;hist&#39;, bins = 24) plt.xlabel(&quot;Hour of the day&quot;) plt.title(&quot;Accidents frequency of Weekdays&quot;) plt.show() . # plot data for weekends us_accidents.loc[~weekdays].Start_Time.dt.hour.plot(kind = &#39;hist&#39;, bins = 24) plt.xlabel(&quot;Hour of the day&quot;) plt.title(&quot;Accidents frequency of Weekends&quot;) plt.show() . Observations . On weekends, there seems to be a breakaway from the pattern that we saw earlier, which supports the idea that the trend we saw earlier could be due to the traffic resulting from office timings, which are usually closed on weekends. | On weekends, accidents tend to occur more during daylight, which should be due to more traffic during those hours. | . Q5. Which days of the week have the most accidents? . sns.histplot(us_accidents.Start_Time.dt.dayofweek, bins = 7) plt.xlabel(&quot;Day of Week&quot;) plt.show() . Observations . Accidents occur more on weekdays and there is a sharp drop in their count on the weekends. | . Q6. Which months have the most accidents? . sns.histplot(us_accidents.Start_Time.dt.month, bins = 12) plt.xlabel(&quot;Month&quot;) plt.show() . Observations . Later months of the year appear to witness more accidents. | . This cannot be due to temperature(winter season) because the count drops suddenly in the starting months of the year. Due to there being no other apparent cause, it demands further analysis. . We can start by looking at the trend year over year. . fig, axes = plt.subplots(2, 3, figsize = (15,10), ) year = 2016 for r in range(2): for c in range(3): if year &lt; 2021: year_data = us_accidents.loc[us_accidents.Start_Time.dt.year == year] sns.histplot(year_data.Start_Time.dt.month, bins = 12, ax = axes[r, c]) axes[r, c].set_title(year) axes[r, c].set_xlabel(&quot;Month of the year&quot;) year += 1 . Observations . Probably because the data was started being collected in the year 2016, the starting months have a much lower number of datapoints. | Also, in the year 2020, because of the coronavirus pandemic lockdown restrictions, there was a suddent drop in the middle of the year. And as the restrictions eased, more accidents started to occur. | . But this data requires further more analysis on the month wise trends. . Q7. What is the trend of accidents year over year (decreasing/increasing?) . us_accidents.Start_Time.dt.year.plot(kind = &#39;hist&#39;, bins = 5, title = &quot;Year wise trend&quot;, xticks = np.arange(2016, 2021), figsize = (7, 5)) plt.show() . Observations . The number of accidents year over year looks to be increasing. | . This can be attributed to better data collection in the later years. Thus, it need further analysis. . Summary and Conclusion . Many questions could be asked and explored, but here we analysed this dataset to answer 7 questions about road accidents in USA. To answer these questions, we first did some basic data preparation, and then went on to analyse the data. It is important to take note that the findings from the data analysis are correlational in nature and do not establish a causal relationship in any way. These findings are: . The number of accidents don&#39;t differ much between warmer and colder temperatures. | The top 5 states in number of accidents are California, Florida, Oregon, Texas and New York. | These 5 states are also the most frequent states in the top 100 cities by number of road accidents. | The number of accidents look to be highly correlated with the office hours. | This idea is also supported by the fact that weekdays see more accidents than weekends. | Although the number of accidents look to be increasing with years, but this could be attributed to better data collection in the later years. | . Thanks for reading. .",
            "url": "https://ncitshubham.github.io/blogs/2021/06/01/exploratory-data-analysis-of-road-accidents-in-usa.html",
            "relUrl": "/2021/06/01/exploratory-data-analysis-of-road-accidents-in-usa.html",
            "date": " • Jun 1, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Scraping Top Repositories for Topics on GitHub",
            "content": "GitHub is an increasingly popular programming resource used for code sharing. It&#39;s a social networking site for programmers that many companies and organizations use to facilitate project management and collaboration. According to statistics collected in August 2021, it was the most prominent source code host, with over 60 million new repositories created in 2020 and boasting over 67 million total developers. . All the projects on Github are stored as repositories. These repositories can get upvotes which are stored as stars. The stars that a repository gets can give us a guage of how popular the repository is. We can further filter all the repositores on GitHub by the topic they ascribe to. The list of topics is available here. . Thus, we&#39;ll scrape GitHub for the top repocistories on each topic and then save that to a csv file for future use. In order to do this, we&#39;ll use the following tools: . Python as the programming language | Requests library for downloading the webpage contents | BeautifulSoup library for finding and accessing the relevant information from the downloaded webpage. | Pandas library for saving the accessed information to a csv file. | . Introduction about GitHub and the problem statement | . Here are the steps we&#39;ll follow: . We&#39;re going to scrape https://github.com/topics | We&#39;ll get a list of topics. For each topic, we&#39;ll get topic title, topic page URL and topic description | For each topic, we&#39;ll get the top 30 repositories in the topic from the topic page | For each repository, we&#39;ll grab the repo name, username, stars and repo URL | For each topic we&#39;ll create a CSV file in the following format: | . Repo Name,Username,Stars,Repo URL three.js,mrdoob,69700,https://github.com/mrdoob/three.js libgdx,libgdx,18300,https://github.com/libgdx/libgdx . Setup . Import the required libraries . import os import requests from bs4 import BeautifulSoup import pandas as pd . Set up URLs and the user-agent. . topics_url = &quot;https://github.com/topics&quot; base_url = &#39;https://github.com&#39; header = {&quot;User-Agent&quot;: &quot;Mozilla/5.0&quot;} . Create variables to store scraped information. . topic_titles = [] topic_desc = [] topic_URL = [] . Scrape the list of topics. . Download the topics webpage and create a BeautifulSoup object . Let&#39;s write a function to download the page. . def get_soup(url): &#39;&#39;&#39; This function will download the webpage for the url supplied as argument and return the BeautifulSoup object for the webpage which can be used to grab required information for the webpage. &#39;&#39;&#39; response = requests.get(url, &quot;html.parser&quot;, headers = header) if response.status_code != 200: raise Exception(&#39;Failed to load page {}&#39;.format(url)) soup = BeautifulSoup(response.text) return soup . # Example soup = get_soup(topics_url) type(soup) . bs4.BeautifulSoup . Create a transform function . Let&#39;s create some helper functions to parse information from the page. . Get topic titles . To get topic titles, we can pick p tags with the class &quot;f3 lh-condensed mb-0 mt-1 Link--primary&quot; . ![My Image alt text](https://i.imgur.com/OnzIdyP.png) # finding all topic titles def get_topic_titles(soup): selection_class = &#39;f3 lh-condensed mb-0 mt-1 Link--primary&#39; topic_title_tags = soup.find_all(&#39;p&#39;, {&#39;class&#39;: selection_class}) topic_titles = [] for tag in topic_title_tags: topic_titles.append(tag.text) return topic_titles . # Example titles = get_topic_titles(soup) len(titles) . 30 . titles[:5] . [&#39;3D&#39;, &#39;Ajax&#39;, &#39;Algorithm&#39;, &#39;Amp&#39;, &#39;Android&#39;] . This is the list of topics on page number 1. We will today scrape information for topics only on this page. In the future, we can scrape information from other pages as well by changing the page number in the url. Now we&#39;ll find the topic descriptions similarly. . Get topic descriptions . # finding all topics descriptions def get_topic_descs(soup): desc_selector = &#39;f5 color-fg-muted mb-0 mt-1&#39; topic_desc_tags = soup.find_all(&#39;p&#39;, {&#39;class&#39;: desc_selector}) topic_descs = [] for tag in topic_desc_tags: topic_descs.append(tag.text.strip()) return topic_descs . # Example topics_descs = get_topic_descs(soup) len(topics_descs) . 30 . topics_descs[:5] . [&#39;3D modeling is the process of virtually developing the surface and structure of a 3D object.&#39;, &#39;Ajax is a technique for creating interactive web applications.&#39;, &#39;Algorithms are self-contained sequences that carry out a variety of tasks.&#39;, &#39;Amp is a non-blocking concurrency library for PHP.&#39;, &#39;Android is an operating system built by Google designed for mobile devices.&#39;] . Similary, we&#39;ll find the topic urls. . Get topic URLs . def get_topic_urls(soup): topic_link_tags = soup.find_all(&#39;a&#39;, {&#39;class&#39;: &#39;no-underline flex-1 d-flex flex-column&#39;}) topic_urls = [] for tag in topic_link_tags: topic_urls.append(base_url + tag[&#39;href&#39;]) return topic_urls . # Example topic_urls = get_topic_urls(soup) len(topic_urls) . 30 . topic_urls[:5] . [&#39;https://github.com/topics/3d&#39;, &#39;https://github.com/topics/ajax&#39;, &#39;https://github.com/topics/algorithm&#39;, &#39;https://github.com/topics/amphp&#39;, &#39;https://github.com/topics/android&#39;] . Save all information . We&#39;ll put together all this information into a single function and then save the scraped information into a pandas DataFrame. . def scrape_topics(): topics_url = &#39;https://github.com/topics&#39; soup = get_soup(topics_url) topics_dict = { &#39;Title&#39;: get_topic_titles(soup), &#39;Description&#39;: get_topic_descs(soup), &#39;URL&#39;: get_topic_urls(soup) } return pd.DataFrame(topics_dict) . topics_df = scrape_topics() topics_df.head() . Title Description URL . 0 3D | 3D modeling is the process of virtually develo... | https://github.com/topics/3d | . 1 Ajax | Ajax is a technique for creating interactive w... | https://github.com/topics/ajax | . 2 Algorithm | Algorithms are self-contained sequences that c... | https://github.com/topics/algorithm | . 3 Amp | Amp is a non-blocking concurrency library for ... | https://github.com/topics/amphp | . 4 Android | Android is an operating system built by Google... | https://github.com/topics/android | . Scraping for top 30 repos for each topic . Now that we have the topics with their titles, descriptions and url, we can access each topic url to grab information about the top 30 repositories from that topic individually and then save the scraped information for each topic as a separate csv file. . Each topic page looks like this . . From this page, we&#39;ll grab information about the top 30 repositories based on their popularity as measured by the number of stars. The repositories are already sorted by popularity by default, so we can grab 30 of them from the first page on each topic itself. We&#39;ll begin by writing a function to download each topic page and create its BeautifulSoup object. . Download each topic page and create a BeautifulSoup Object . def get_topic_page(topic_url): # Download the page response = requests.get(topic_url, &quot;html.parser&quot;, headers = header) # Check successful response if response.status_code != 200: raise Exception(&#39;Failed to load page {}&#39;.format(topic_url)) # Parse using Beautiful soup topic_soup = BeautifulSoup(response.text) return topic_soup . page = get_topic_page(&#39;https://github.com/topics/3d&#39;) . Transform the topic Beautiful Object . Get all the required information about a repository . All the information that we need about a repository is given under a div tag with class d-flex flex-justify-between my-3. So we will make a function which takes in the content of each repository from these tags as arguement. It will then grab and return the required information from the content. . def get_repo_info(repo): # returns all the required info about a repository info = repo.find(&#39;h3&#39;, {&#39;class&#39;: &#39;f3 color-fg-muted text-normal lh-condensed&#39;}).find_all(&#39;a&#39;) username = info[0].text.strip() repo_name = info[1].text.strip() repo_url = base_url + info[0][&#39;href&#39;].strip() stars = repo.find(&#39;span&#39;, {&#39;id&#39;: &#39;repo-stars-counter-star&#39;}).text.strip() return username, repo_name, stars, repo_url . # Example repo_contents = page.find_all(&#39;div&#39;, {&#39;class&#39;: &#39;d-flex flex-justify-between my-3&#39;}) get_repo_info(repo_contents[0]) . (&#39;mrdoob&#39;, &#39;three.js&#39;, &#39;80.6k&#39;, &#39;https://github.com/mrdoob&#39;) . Here we can see that the function returns the information about the first repository from the topic page. The top repository in this case is &#39;three.js&#39; with 80.6k stars. . Grab the information from top 30 repos under a topic. . Now, we&#39;ll write a function to grab information about repositories within a topic. It will take in a topic soup and return a pandas DataFrame on the top 30 repos in that topic. . def get_topic_repos(topic_soup): div_selection_class = &#39;d-flex flex-justify-between my-3&#39; repo_tags = topic_soup.find_all(&#39;div&#39;, {&#39;class&#39;: div_selection_class}) topic_repos_dict = { &#39;username&#39;: [], &#39;repo_name&#39;: [], &#39;stars&#39;: [],&#39;repo_url&#39;: []} # Get repo info for i in range(len(repo_tags)): username, repo_name, stars, repo_url = get_repo_info(repo_tags[i]) topic_repos_dict[&#39;username&#39;].append(username) topic_repos_dict[&#39;repo_name&#39;].append(repo_name) topic_repos_dict[&#39;stars&#39;].append(stars) topic_repos_dict[&#39;repo_url&#39;].append(repo_url) return pd.DataFrame(topic_repos_dict) . # Example get_topic_repos(page) . username repo_name stars repo_url . 0 mrdoob | three.js | 80.6k | https://github.com/mrdoob | . 1 libgdx | libgdx | 19.8k | https://github.com/libgdx | . 2 pmndrs | react-three-fiber | 17.4k | https://github.com/pmndrs | . 3 BabylonJS | Babylon.js | 16.2k | https://github.com/BabylonJS | . 4 aframevr | aframe | 14k | https://github.com/aframevr | . 5 ssloy | tinyrenderer | 13.3k | https://github.com/ssloy | . 6 lettier | 3d-game-shaders-for-beginners | 12.5k | https://github.com/lettier | . 7 FreeCAD | FreeCAD | 11k | https://github.com/FreeCAD | . 8 metafizzy | zdog | 9.1k | https://github.com/metafizzy | . 9 CesiumGS | cesium | 8.5k | https://github.com/CesiumGS | . 10 timzhang642 | 3D-Machine-Learning | 7.8k | https://github.com/timzhang642 | . 11 a1studmuffin | SpaceshipGenerator | 7.1k | https://github.com/a1studmuffin | . 12 isl-org | Open3D | 6.4k | https://github.com/isl-org | . 13 blender | blender | 5.2k | https://github.com/blender | . 14 domlysz | BlenderGIS | 5k | https://github.com/domlysz | . 15 spritejs | spritejs | 4.8k | https://github.com/spritejs | . 16 openscad | openscad | 4.7k | https://github.com/openscad | . 17 tensorspace-team | tensorspace | 4.6k | https://github.com/tensorspace-team | . 18 jagenjo | webglstudio.js | 4.6k | https://github.com/jagenjo | . 19 YadiraF | PRNet | 4.6k | https://github.com/YadiraF | . 20 AaronJackson | vrn | 4.4k | https://github.com/AaronJackson | . 21 google | model-viewer | 4.1k | https://github.com/google | . 22 ssloy | tinyraytracer | 4.1k | https://github.com/ssloy | . 23 mosra | magnum | 3.9k | https://github.com/mosra | . 24 FyroxEngine | Fyrox | 3.5k | https://github.com/FyroxEngine | . 25 gfxfundamentals | webgl-fundamentals | 3.5k | https://github.com/gfxfundamentals | . 26 tengbao | vanta | 3.3k | https://github.com/tengbao | . 27 cleardusk | 3DDFA | 3.2k | https://github.com/cleardusk | . 28 jasonlong | isometric-contributions | 3.1k | https://github.com/jasonlong | . 29 cnr-isti-vclab | meshlab | 2.9k | https://github.com/cnr-isti-vclab | . As we can see, the function has returned a pandas DataFrame of the top 30 repos from the topic &#39;3d&#39;. Now, we&#39;ll make function to save this DataFrame as a csv file if we haven&#39;t already created a file on that topic. . Save topic file . def scrape_topic(topic_url, path): if os.path.exists(path): print(&quot;The file {} already exists. Skipping...&quot;.format(path)) return topic_df = get_topic_repos(get_topic_page(topic_url)) topic_df.to_csv(path, index=None) . Putting it all together . We have a funciton to get the list of topics | We have a function to create a CSV file for scraped repos from a topics page | Let&#39;s create a function to put them together | . def scrape_topics_repos(): print(&#39;Scraping list of topics&#39;) topics_df = scrape_topics() os.makedirs(&#39;data&#39;, exist_ok=True) for index, row in topics_df.iterrows(): print(f&quot;Scraping top repositories for {row[&#39;Title&#39;]}&quot;) scrape_topic(row[&#39;URL&#39;], f&quot;data/{row[&#39;Title&#39;]}.csv&quot;) . Let&#39;s run it to scrape the top repos for the all the topics on the first page of https://github.com/topics . scrape_topics_repos() . Scraping list of topics Scraping top repositories for 3D Scraping top repositories for Ajax Scraping top repositories for Algorithm Scraping top repositories for Amp Scraping top repositories for Android Scraping top repositories for Angular Scraping top repositories for Ansible Scraping top repositories for API Scraping top repositories for Arduino Scraping top repositories for ASP.NET Scraping top repositories for Atom Scraping top repositories for Awesome Lists Scraping top repositories for Amazon Web Services Scraping top repositories for Azure Scraping top repositories for Babel Scraping top repositories for Bash Scraping top repositories for Bitcoin Scraping top repositories for Bootstrap Scraping top repositories for Bot Scraping top repositories for C Scraping top repositories for Chrome Scraping top repositories for Chrome extension Scraping top repositories for Command line interface Scraping top repositories for Clojure Scraping top repositories for Code quality Scraping top repositories for Code review Scraping top repositories for Compiler Scraping top repositories for Continuous integration Scraping top repositories for COVID-19 Scraping top repositories for C++ . Summary and Conclusion . As we can see, we have successfully scraped top 30 repositories for 30 topics. And we have saved the information on these top 30 repositories as a csv file for each topic separately. The information that we have scraped for each repository is its title, owner username, star count and its url. . We have scraped repositories for only 30 topics today. This was the number of topics available on the page 1 of https://github.com/topics. But it is easy top scrape more topics. What we just need to do is change the page number in the url https://github.com/topics?page={i} where &#39;i&#39; is the page number. This way, we can scrape info on top repos for all the topics of GitHub. .",
            "url": "https://ncitshubham.github.io/blogs/2021/05/10/Scrape_GitHub_topic_repos.html",
            "relUrl": "/2021/05/10/Scrape_GitHub_topic_repos.html",
            "date": " • May 10, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a Data Analyst and a Software Developer in Python. .",
          "url": "https://ncitshubham.github.io/blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ncitshubham.github.io/blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}