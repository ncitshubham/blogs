{
  
    
        "post0": {
            "title": "Title",
            "content": "print(&quot;hellooooo&quot;) . hellooooo . import os os.getcwd() . &#39;/kaggle/working&#39; . import seaborn as sns . sns.dogplot() .",
            "url": "https://ncitshubham.github.io/blogs/2022/02/12/notebook01e419c9fc.html",
            "relUrl": "/2022/02/12/notebook01e419c9fc.html",
            "date": " • Feb 12, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Markdown_test",
            "content": "print(&quot;hellooooo&quot;) . hellooooo . import os os.getcwd() . &#39;C: Users ncits Downloads&#39; . .",
            "url": "https://ncitshubham.github.io/blogs/2022/01/15/markdown_test.html",
            "relUrl": "/2022/01/15/markdown_test.html",
            "date": " • Jan 15, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Scraping Data Science Jobs on in.indeed.com",
            "content": "Indeed.com is a worldwide employment website for job listings. Globally, it hosts millions of job listings on thousands of jobs. In this project, we are interested in the &#39;Data Science&#39; related job listings on https://in.indeed.com/ . Thus, we&#39;ll scrape the website for this information and save that to a csv file for future use. In order to do this, we&#39;ll use the following tools: . Python as the programming language | Requests library for downloading the webpage contents | BeautifulSoup library for finding and accessing the relevant information from the downloaded webpage. | Numpy library for handling missing values. | Pandas library for saving the accessed information to a csv file. | . Here are the steps we&#39;ll follow: . We&#39;ll scrape first 10 pages from https://in.indeed.com/jobs?q=data%20science | We&#39;ll get a list of all 15 jobs on each page. | For each job, we&#39;ll grab the Job Title, Salary, Location, Company Name, and Company Rating. | We&#39;ll save all of the information in a csv file in the following format: . Title,Salary,Location,Company Name,Company Rating Data science,&quot;₹35,000 - ₹45,000 a month&quot;,&quot;Mumbai, Maharashtra&quot;,V Digitech Sevices Data Science ( 2 - 8 Yrs) - Remote,&quot;₹8,00,000 - ₹20,00,000 a year&quot;,Remote,ProGrad Data Science Internship,&quot;₹10,000 a month&quot;,&quot;Gurgaon, Haryana&quot;,Zigram . | . Initial Setup . Import the required libraries . import requests from bs4 import BeautifulSoup import pandas as pd from numpy import nan . Set up base URL and the user-agent. . # The start value in the base_url will increment by 10 to access each following page. base_url = &quot;https://in.indeed.com/jobs?q=data%20science&amp;start={}&quot; header = {&quot;User-Agent&quot;: &quot;Mozilla/5.0&quot;} . Create a dictionary to save all information. . jobs = {&quot;Job Title&quot;: [], &quot;Salary&quot;: [], &quot;Location&quot;: [], &quot;Company Name&quot;: [], &quot;Company Rating&quot;: []} . Scrape the search result webpage. . Download webpage and create a BeautifulSoup object . def get_soup(url): &#39;&#39;&#39; This function will download the webpage for the url supplied as argument and return the BeautifulSoup object for the webpage which can be used to grab required information for the webpage. &#39;&#39;&#39; response = requests.get(url, &quot;html.parser&quot;, headers = header) if response.status_code != 200: raise Exception(&#39;Failed to load page {}&#39;.format(url)) soup = BeautifulSoup(response.text) return soup . Example for get_soup . soup = get_soup(base_url.format(10)) type(soup) . bs4.BeautifulSoup . Create a transform function . Now, we&#39;ll create a transform function to grab the list of jobs from the result webpage. . To do this, we&#39;ll pick td tags with the class: resultContent . . def transform(soup): # find all the job listings on the webpage. jobs_tags = soup.find_all(&quot;td&quot;, class_ = &quot;resultContent&quot;) # for each job, call helper functions to grab information about the job # and save that to jobs dictionary. for job in jobs_tags: jobs[&quot;Job Title&quot;].append(get_job_title(job)) jobs[&quot;Salary&quot;].append(get_job_salary(job)) jobs[&quot;Location&quot;].append(get_company_location(job)) jobs[&quot;Company Name&quot;].append(get_company_name(job)) jobs[&quot;Company Rating&quot;].append(get_company_rating(job)) . Example for finding the job tags . job_tags = soup.find_all(&quot;td&quot;, class_ = &quot;resultContent&quot;) print(len(job_tags)) . 15 . print(job_tags[0].text) . newData Scientist IIMasterCard4.1Gurgaon, Haryana . Create helper functions . Create helper functions to grab job information for each job and store that in the jobs dictionary. . First grab Job Title. . def get_job_title(job): &#39;&#39;&#39; Function to grab the job title. Because some job titles have a prefix new in their job titles, this function will automatically detect this prefix and return the title sans &#39;new&#39; in the job title. &#39;&#39;&#39; title = job.find(class_ = &quot;jobTitle&quot;).text if title[:3] == &quot;new&quot;: return title[3:] else: return title get_job_title(job_tags[0]) . &#39;Data Scientist II&#39; . Now, we&#39;ll grab the job salary, if the listing has one. . def get_job_salary(job): salary = job.find(&quot;div&quot;, class_ = &quot;salary-snippet&quot;) if salary: return salary.text else: return nan get_job_salary(job_tags[1]) . nan . Similarly, we&#39;ll grab the company name, location, and its rating. . def get_company_name(job): &#39;&#39;&#39; Returns the company name for the supp&#39;&#39;&#39; return job.find(class_ = &quot;companyName&quot;).text def get_company_location(job): &#39;&#39;&#39; Returns the company location for the supplied job tag &#39;&#39;&#39; return job.find(class_ = &quot;companyLocation&quot;).text def get_company_rating(job): &#39;&#39;&#39; Returns the company rating for the supplied job tag &#39;&#39;&#39; rating = job.find(class_ = &quot;ratingNumber&quot;) if rating: return float(rating.text) else: return nan print(get_company_name(job_tags[0]), get_company_location(job_tags[0]), get_company_rating(job_tags[0]), sep = &quot; n&quot;) . MasterCard Gurgaon, Haryana 4.1 . Putting it all together . We&#39;ll use a for loop to loop through 10 search result pages. Within this loop, we can apply the get_soup function to download these pages and the transform function to parse through all job listings from these pages and save the information in the jobs dictionary. We&#39;ll then use this dictionary to create a pandas DataFrame, which can then be saved to a csv file. . for page in range(0, 110, 10): print(f&quot;Scraping page {page}...&quot;) soup = get_soup(base_url.format(page)) transform(soup) . Scraping page 0... Scraping page 10... Scraping page 20... Scraping page 30... Scraping page 40... Scraping page 50... Scraping page 60... Scraping page 70... Scraping page 80... Scraping page 90... Scraping page 100... . jobs_df = pd.DataFrame(jobs) . jobs_df.head() . Job Title Salary Location Company Name Company Rating . 0 Analyst-Data Science | NaN | Bengaluru, Karnataka+1 location | Amex | 4.1 | . 1 Data Science | ₹12,00,000 - ₹18,00,000 a year | Bengaluru, Karnataka | Onward Group | NaN | . 2 Data Scientist - ML | NaN | Bengaluru, Karnataka | JPMorgan Chase Bank, N.A. | 3.9 | . 3 Data Scientist | NaN | Pune, Maharashtra+1 location | Maersk | 4.1 | . 4 Data Engineer | NaN | Gurgaon, Haryana+3 locations | NatWest Group | 3.3 | . jobs_df.to_csv(&quot;Data_Science_jobs_from_indeed.com.csv&quot;, index = None, encoding = &quot;utf-8&quot;) . !pip install jovian --upgrade --quiet . import jovian . jovian.commit(project=&quot;scraping-in-indeed-com-for-data-science-jobs&quot;) .",
            "url": "https://ncitshubham.github.io/blogs/2021/11/09/scraping-indeed.com-for-data-science-jobs-Copy.html",
            "relUrl": "/2021/11/09/scraping-indeed.com-for-data-science-jobs-Copy.html",
            "date": " • Nov 9, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "wha fakdsfj  alsfjasl",
            "content": "Scraping Data Science Jobs on in.indeed.com . Introduction . Indeed.com is a worldwide employment website for job listings. Globally, it hosts millions of job listings on thousands of jobs. In this project, we are interested in the &#39;Data Science&#39; related job listings on https://in.indeed.com/ . Thus, we&#39;ll scrape the website for this information and save that to a csv file for future use. In order to do this, we&#39;ll use the following tools: . Python as the programming language | Requests library for downloading the webpage contents | BeautifulSoup library for finding and accessing the relevant information from the downloaded webpage. | Numpy library for handling missing values. | Pandas library for saving the accessed information to a csv file. | . Here are the steps we&#39;ll follow: . We&#39;ll scrape first 10 pages from https://in.indeed.com/jobs?q=data%20science | We&#39;ll get a list of all 15 jobs on each page. | For each job, we&#39;ll grab the Job Title, Salary, Location, Company Name, and Company Rating. | We&#39;ll save all of the information in a csv file in the following format: . Title,Salary,Location,Company Name,Company Rating Data science,&quot;₹35,000 - ₹45,000 a month&quot;,&quot;Mumbai, Maharashtra&quot;,V Digitech Sevices Data Science ( 2 - 8 Yrs) - Remote,&quot;₹8,00,000 - ₹20,00,000 a year&quot;,Remote,ProGrad Data Science Internship,&quot;₹10,000 a month&quot;,&quot;Gurgaon, Haryana&quot;,Zigram . | . Initial Setup . Import the required libraries . import requests from bs4 import BeautifulSoup import pandas as pd from numpy import nan . Set up base URL and the user-agent. . # The start value in the base_url will increment by 10 to access each following page. base_url = &quot;https://in.indeed.com/jobs?q=data%20science&amp;start={}&quot; header = {&quot;User-Agent&quot;: &quot;Mozilla/5.0&quot;} . Create a dictionary to save all information. . jobs = {&quot;Job Title&quot;: [], &quot;Salary&quot;: [], &quot;Location&quot;: [], &quot;Company Name&quot;: [], &quot;Company Rating&quot;: []} . Scrape the search result webpage. . Download webpage and create a BeautifulSoup object . def get_soup(url): &#39;&#39;&#39; This function will download the webpage for the url supplied as argument and return the BeautifulSoup object for the webpage which can be used to grab required information for the webpage. &#39;&#39;&#39; response = requests.get(url, &quot;html.parser&quot;, headers = header) if response.status_code != 200: raise Exception(&#39;Failed to load page {}&#39;.format(url)) soup = BeautifulSoup(response.text) return soup . Example for get_soup . soup = get_soup(base_url.format(10)) type(soup) . bs4.BeautifulSoup . Create a transform function . Now, we&#39;ll create a transform function to grab the list of jobs from the result webpage. . To do this, we&#39;ll pick td tags with the class: resultContent . . def transform(soup): # find all the job listings on the webpage. jobs_tags = soup.find_all(&quot;td&quot;, class_ = &quot;resultContent&quot;) # for each job, call helper functions to grab information about the job # and save that to jobs dictionary. for job in jobs_tags: jobs[&quot;Job Title&quot;].append(get_job_title(job)) jobs[&quot;Salary&quot;].append(get_job_salary(job)) jobs[&quot;Location&quot;].append(get_company_location(job)) jobs[&quot;Company Name&quot;].append(get_company_name(job)) jobs[&quot;Company Rating&quot;].append(get_company_rating(job)) . Example for finding the job tags . job_tags = soup.find_all(&quot;td&quot;, class_ = &quot;resultContent&quot;) print(len(job_tags)) . 15 . print(job_tags[0].text) . newData Scientist IIMasterCard4.1Gurgaon, Haryana . Create helper functions . Create helper functions to grab job information for each job and store that in the jobs dictionary. . First grab Job Title. . def get_job_title(job): &#39;&#39;&#39; Function to grab the job title. Because some job titles have a prefix new in their job titles, this function will automatically detect this prefix and return the title sans &#39;new&#39; in the job title. &#39;&#39;&#39; title = job.find(class_ = &quot;jobTitle&quot;).text if title[:3] == &quot;new&quot;: return title[3:] else: return title get_job_title(job_tags[0]) . &#39;Data Scientist II&#39; . Now, we&#39;ll grab the job salary, if the listing has one. . def get_job_salary(job): salary = job.find(&quot;div&quot;, class_ = &quot;salary-snippet&quot;) if salary: return salary.text else: return nan get_job_salary(job_tags[1]) . nan . Similarly, we&#39;ll grab the company name, location, and its rating. . def get_company_name(job): &#39;&#39;&#39; Returns the company name for the supp&#39;&#39;&#39; return job.find(class_ = &quot;companyName&quot;).text def get_company_location(job): &#39;&#39;&#39; Returns the company location for the supplied job tag &#39;&#39;&#39; return job.find(class_ = &quot;companyLocation&quot;).text def get_company_rating(job): &#39;&#39;&#39; Returns the company rating for the supplied job tag &#39;&#39;&#39; rating = job.find(class_ = &quot;ratingNumber&quot;) if rating: return float(rating.text) else: return nan print(get_company_name(job_tags[0]), get_company_location(job_tags[0]), get_company_rating(job_tags[0]), sep = &quot; n&quot;) . MasterCard Gurgaon, Haryana 4.1 . Putting it all together . We&#39;ll use a for loop to loop through 10 search result pages. Within this loop, we can apply the get_soup function to download these pages and the transform function to parse through all job listings from these pages and save the information in the jobs dictionary. We&#39;ll then use this dictionary to create a pandas DataFrame, which can then be saved to a csv file. . for page in range(0, 110, 10): print(f&quot;Scraping page {page}...&quot;) soup = get_soup(base_url.format(page)) transform(soup) . Scraping page 0... Scraping page 10... Scraping page 20... Scraping page 30... Scraping page 40... Scraping page 50... Scraping page 60... Scraping page 70... Scraping page 80... Scraping page 90... Scraping page 100... . jobs_df = pd.DataFrame(jobs) . jobs_df.head() . Job Title Salary Location Company Name Company Rating . 0 Analyst-Data Science | NaN | Bengaluru, Karnataka+1 location | Amex | 4.1 | . 1 Data Science | ₹12,00,000 - ₹18,00,000 a year | Bengaluru, Karnataka | Onward Group | NaN | . 2 Data Scientist - ML | NaN | Bengaluru, Karnataka | JPMorgan Chase Bank, N.A. | 3.9 | . 3 Data Scientist | NaN | Pune, Maharashtra+1 location | Maersk | 4.1 | . 4 Data Engineer | NaN | Gurgaon, Haryana+3 locations | NatWest Group | 3.3 | . jobs_df.to_csv(&quot;Data_Science_jobs_from_indeed.com.csv&quot;, index = None, encoding = &quot;utf-8&quot;) .",
            "url": "https://ncitshubham.github.io/blogs/2021/11/09/scraping-indeed.com-for-data-science-jobs-Copy-(1).html",
            "relUrl": "/2021/11/09/scraping-indeed.com-for-data-science-jobs-Copy-(1).html",
            "date": " • Nov 9, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "EDA of Stroke Dataset and Prediction of Strokes using Selected ML Algorithms",
            "content": "Introduction . According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths. . In this project, we&#39;ll try to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. To do this, we&#39;ll use the Stroke Prediction Dataset provided by fedesoriano on Kaggle. . Each row in the dataset provides relavant information about the patient like age, smoking status, gender, heart disease, bmi, work type and in the end whether the patient suffered a stroke. This last parameter will be our target, which we&#39;ll try to predict using information from the other columns. . The steps that we&#39;ll take: . Setup and import the dataset | Perform basic EDA and prepare the dataset for training | Train and evaluate a baseline model | Train multiple ML models and make predictions. | Evaluate and compare their performance. | . Setup . import numpy as np # Linear algebra import pandas as pd # Data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # Data Visualizatin import seaborn as sns # Data Visualization from imblearn.over_sampling import SMOTE # Oversampling imbalanced classes from sklearn.impute import SimpleImputer, MissingIndicator # Handle missing values from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate, GridSearchCV, StratifiedKFold from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, RocCurveDisplay, PrecisionRecallDisplay, confusion_matrix, ConfusionMatrixDisplay # Metrics from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, FunctionTransformer from imblearn.pipeline import make_pipeline from sklearn.compose import make_column_transformer # Models for prediciton from sklearn.dummy import DummyClassifier from sklearn.linear_model import LogisticRegression, RidgeClassifier from sklearn.svm import SVC from sklearn.naive_bayes import GaussianNB from sklearn.neighbors import KNeighborsClassifier from sklearn.ensemble import RandomForestClassifier from xgboost import XGBClassifier # ignore warnings import warnings warnings.filterwarnings(&quot;ignore&quot;) # get the file path import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . /kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv . file_path = &quot;/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv&quot; . !ls -lh {file_path} . -rw-r--r-- 1 nobody nogroup 310K Oct 14 16:31 /kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv . !head {file_path} . id,gender,age,hypertension,heart_disease,ever_married,work_type,Residence_type,avg_glucose_level,bmi,smoking_status,stroke 9046,Male,67,0,1,Yes,Private,Urban,228.69,36.6,formerly smoked,1 51676,Female,61,0,0,Yes,Self-employed,Rural,202.21,N/A,never smoked,1 31112,Male,80,0,1,Yes,Private,Rural,105.92,32.5,never smoked,1 60182,Female,49,0,0,Yes,Private,Urban,171.23,34.4,smokes,1 1665,Female,79,1,0,Yes,Self-employed,Rural,174.12,24,never smoked,1 56669,Male,81,0,0,Yes,Private,Urban,186.21,29,formerly smoked,1 53882,Male,74,1,1,Yes,Private,Rural,70.09,27.4,never smoked,1 10434,Female,69,0,0,No,Private,Urban,94.39,22.8,never smoked,1 27419,Female,59,0,0,Yes,Private,Rural,76.15,N/A,Unknown,1 . Observations . File size is 310 KB, thus it is safe to import the whole dataset. | The delimiters are , | id is the index column | stroke is the prediction class in the last column | . df = pd.read_csv(file_path, index_col = [&quot;id&quot;]) df.head() . gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status stroke . id . 9046 Male | 67.0 | 0 | 1 | Yes | Private | Urban | 228.69 | 36.6 | formerly smoked | 1 | . 51676 Female | 61.0 | 0 | 0 | Yes | Self-employed | Rural | 202.21 | NaN | never smoked | 1 | . 31112 Male | 80.0 | 0 | 1 | Yes | Private | Rural | 105.92 | 32.5 | never smoked | 1 | . 60182 Female | 49.0 | 0 | 0 | Yes | Private | Urban | 171.23 | 34.4 | smokes | 1 | . 1665 Female | 79.0 | 1 | 0 | Yes | Self-employed | Rural | 174.12 | 24.0 | never smoked | 1 | . Basic EDA and Data Preparation . First, it is really important to separate the test data from the train data at this point, so that the transformers and models cannot learn from the test data itself. Before making a split, it is worth looking at the distribution of prediction class, to see if there is an imbalance and whether we will need to stratify the split. . stroke_val_count = df.stroke.value_counts() print(f&quot;Value Count in the prediction class - Stroke: n{stroke_val_count} n n&quot;) sns.barplot(x = stroke_val_count.index, y = stroke_val_count) plt.show() . Value Count in the prediction class - Stroke: 0 4861 1 249 Name: stroke, dtype: int64 . As we can see, the prediction class is highly imbalanced. Therefore, we&#39;ll need to stratify the split. Also, after making the split, it would be worth to generate artificial data in the training dataset to help the ML models distinguish better between the two categories of prediction class. . X = df.copy() y = X.pop(&quot;stroke&quot;) # split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 42) . We&#39;ll perform EDA on the dataset and use the observations that we make for building a data preparation pipeline as the last step. . Basic Inspection . X_train.head() . gender age hypertension heart_disease ever_married work_type Residence_type avg_glucose_level bmi smoking_status . id . 25283 Female | 48.0 | 0 | 0 | Yes | Private | Urban | 69.21 | 33.1 | never smoked | . 43734 Male | 15.0 | 0 | 0 | No | Private | Rural | 122.25 | 21.0 | never smoked | . 47113 Female | 67.0 | 0 | 0 | Yes | Self-employed | Rural | 110.42 | 24.9 | never smoked | . 56996 Male | 44.0 | 0 | 0 | Yes | Private | Urban | 65.41 | 24.8 | smokes | . 26325 Male | 14.0 | 0 | 0 | No | Govt_job | Urban | 82.34 | 31.6 | Unknown | . X_train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 4088 entries, 25283 to 31836 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 gender 4088 non-null object 1 age 4088 non-null float64 2 hypertension 4088 non-null int64 3 heart_disease 4088 non-null int64 4 ever_married 4088 non-null object 5 work_type 4088 non-null object 6 Residence_type 4088 non-null object 7 avg_glucose_level 4088 non-null float64 8 bmi 3918 non-null float64 9 smoking_status 4088 non-null object dtypes: float64(3), int64(2), object(5) memory usage: 351.3+ KB . Observations . There are 4088 entries in the train dataset. | There are total 10 features which we can use to predict the occurance of stroke. | There are some categorical features like gender, work_type, Residence_type of dtype object, which we&#39;ll need to be One Hot Encoded. | Numerical features will need to be scaled. | There are some missing values in the bmi column. | . Null values . X_train.isnull().sum() . gender 0 age 0 hypertension 0 heart_disease 0 ever_married 0 work_type 0 Residence_type 0 avg_glucose_level 0 bmi 170 smoking_status 0 dtype: int64 . plt.figure(figsize = (10, 8)) sns.heatmap(X_train.isnull(), cmap = &quot;rocket&quot;, yticklabels = &quot;&quot;, cbar = None) . &lt;AxesSubplot:ylabel=&#39;id&#39;&gt; . null_bmi = y_train.loc[X_train.bmi.isna()] not_null_bmi = y_train.loc[X_train.bmi.notnull()] print(f&quot;&quot;&quot;Non null bmi values:- Stroke-no stroke ratio: {null_bmi.sum()/len(null_bmi)} Null bmi values:- Stroke-no stroke ratio: {not_null_bmi.sum()/len(not_null_bmi)} &quot;&quot;&quot;) . Non null bmi values:- Stroke-no stroke ratio: 0.21764705882352942 Null bmi values:- Stroke-no stroke ratio: 0.04134762633996937 . Observations . Although Null values look to be evenly distributed in the heatmap, the ratio for occurance of stroke is significatly different in the entries with null bmi values. Thus, instead of dropping the null values, it would be better to impute the null values with median bmi value, and also encode the presence of null values in a separate column. This may help in better prediction of stroke. . Categorical columns . First it is worth inspecting the categories and their distributions in all categorical columns. . cat_cols = [&#39;gender&#39;, &#39;hypertension&#39;, &#39;heart_disease&#39;, &#39;ever_married&#39;, &#39;work_type&#39;, &#39;Residence_type&#39;, &#39;smoking_status&#39;] print(&quot;Value Counts of all categorical columns... n&quot;) for i, col in enumerate(cat_cols): val_count = X_train[col].value_counts() print(f&quot;Values:- n{val_count} n n&quot;) plt.figure(i) sns.barplot(x = val_count.index, y = val_count) . Value Counts of all categorical columns... Values:- Female 2395 Male 1692 Other 1 Name: gender, dtype: int64 Values:- 0 3691 1 397 Name: hypertension, dtype: int64 Values:- 0 3867 1 221 Name: heart_disease, dtype: int64 Values:- Yes 2700 No 1388 Name: ever_married, dtype: int64 Values:- Private 2332 Self-employed 667 children 554 Govt_job 522 Never_worked 13 Name: work_type, dtype: int64 Values:- Urban 2069 Rural 2019 Name: Residence_type, dtype: int64 Values:- never smoked 1501 Unknown 1247 formerly smoked 714 smokes 626 Name: smoking_status, dtype: int64 . Observation . The categories in all the categorical features look ok, albeit most of the categories are unevenly distributed. Thus, we&#39;ll just one hot encode these columns. . Numerical Columns . num_cols = [&quot;age&quot;, &quot;avg_glucose_level&quot;, &quot;bmi&quot;] X_train[num_cols].describe() . age avg_glucose_level bmi . count 4088.000000 | 4088.000000 | 3918.000000 | . mean 43.353288 | 106.317167 | 28.922180 | . std 22.596816 | 45.259652 | 7.928378 | . min 0.080000 | 55.120000 | 10.300000 | . 25% 26.000000 | 77.312500 | 23.600000 | . 50% 45.000000 | 91.945000 | 28.000000 | . 75% 61.000000 | 114.197500 | 33.100000 | . max 82.000000 | 271.740000 | 97.600000 | . Observations . Age appears to be slightly negatively skewed | avg_glucose_level appears to be positively skewed | min age of 0.08 suggests that age is stored in fractions, which needs further inspection. | . for i, col in enumerate(num_cols): plt.figure(i) sns.violinplot(x = X_train[col]) plt.legend([f&quot;skewness: {X_train[col].skew():.2f} nkurtosis: {X_train[col].kurtosis():.2f}&quot;]) . Observations . age is very slightly negatively skewed with negative kurtosis. | avg_glucose_level and bmi are positively skewed with sharp peaks(positive kurtosis) | . Although some ML models assume normal distribution of data, they can work fine with data with small skewness and kurtosis values. Therefore, we&#39;ll just scale this data using MinMaxScaler. . print(sorted(X_train.age.unique())) . [0.08, 0.24, 0.32, 0.4, 0.48, 0.56, 0.64, 0.72, 0.8, 0.88, 1.0, 1.08, 1.16, 1.24, 1.32, 1.4, 1.48, 1.56, 1.64, 1.72, 1.8, 1.88, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0] . Observations . Only values smaller than 2 are stored as floats. Thus, we can change the whole column to int datatype to make this feature uniform. . Build Column Transformer . Now, we can use combine all the observations that we made to build a data preparation pipeline with a column transformer. We&#39;ll use the transformer to: . impute missing values | change age column dtype to int | add missing value indicator | onehotencode categorical columns | scale numerical features | . age_col = [&quot;age&quot;] num_cols_without_age = [&quot;avg_glucose_level&quot;, &quot;bmi&quot;] cat_cols = [&quot;gender&quot;, &quot;ever_married&quot;, &quot;work_type&quot;, &quot;Residence_type&quot;, &quot;smoking_status&quot;] # Scale the data scaler = MinMaxScaler() # handle missing values in `bmi` imputer = SimpleImputer(strategy = &quot;median&quot;) # change dtype of `age` def to_int(x): return pd.DataFrame(x).astype(int) int_tr = FunctionTransformer(to_int) . age_transformer = make_pipeline(int_tr, scaler) # build transformer for numeric cols without age num_transformer = make_pipeline(imputer, scaler) # missing_indicator miss_ind = MissingIndicator() # build transformer for categorical cols cat_transformer = OneHotEncoder(drop = &quot;first&quot;, handle_unknown = &quot;ignore&quot;, sparse = False) # combine transformers to make a single column transformer ct = make_column_transformer((age_transformer, age_col), (num_transformer, num_cols_without_age), (miss_ind, num_cols_without_age), (cat_transformer, cat_cols), remainder = &quot;passthrough&quot;) . ct.fit_transform(X_train).shape . (4088, 17) . SMOTE . We&#39;ll use imblearn&#39;s SMOTE over-sampling to balance the data. We&#39;ll implement this within the final pipeline with the training model. . sm = SMOTE(random_state = 42) sm . SMOTE(random_state=42) . Train and Evaluate a Baseline Model . A baseline model which doesn&#39;t use any of the features to make predictions will give a baseline score, that the future ML models should at least beat. This score will help to identify errors in model training and evaluation if the models perform worse than the baseline score. . base_model = DummyClassifier(strategy = &#39;prior&#39;) base_pipe = make_pipeline(ct, sm, base_model) # specify KFold strategy cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42) . Baseline scores . cross_validate(base_pipe, X_train, y_train, cv = cv, n_jobs = -1, scoring = [&quot;accuracy&quot;, &quot;recall&quot;, &quot;roc_auc&quot;, &quot;f1&quot;]) . {&#39;fit_time&#39;: array([0.0513134 , 0.0490582 , 0.05048561, 0.04655719, 0.0471642 ]), &#39;score_time&#39;: array([0.04595423, 0.04224229, 0.04374504, 0.02858949, 0.04048371]), &#39;test_accuracy&#39;: array([0.95110024, 0.95110024, 0.95110024, 0.95226438, 0.95104039]), &#39;test_recall&#39;: array([0., 0., 0., 0., 0.]), &#39;test_roc_auc&#39;: array([0.5, 0.5, 0.5, 0.5, 0.5]), &#39;test_f1&#39;: array([0., 0., 0., 0., 0.])} . The model accuracy comes out to be really great at about 0.95. Although, it would generally be a good score to achieve. in this domain, a good accuracy score can be decieving. We can actually confirm this by looking at the test recall scores. All the test recall scores are 0, which means that the model failed to catch even a single True Positive. This is what we almost never want in the medical sphere. If the testing isn&#39;t prohibitively expensive or risky for the patient, which in this case it isn&#39;t, the test should aim for a high recall score and not a high accuracy score. That is why we&#39;ll judge our models using the roc_auc score primarily, which shows the relation between the True Positive Rate(TPR/recall) and False Positive Rate(FPR) of the model. For this baseline model, the roc_auc score comes out to be 0.5, which means the model isn&#39;t able to distinguish between the prediction classes at any of the thresholds, which we would expect from a dummy model. We can also look at the roc_auc_curve of the model. . base_preds_prob = cross_val_predict(base_pipe, X_train, y_train, cv = cv, n_jobs = -1, method = &quot;predict_proba&quot;)[:, 1] RocCurveDisplay.from_predictions(y_train, base_preds_prob) . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7fc4d14678d0&gt; . Model Selection . In this section, we&#39;ll train multiple ML models typically used for binary classification and compare their performance using the scores from cross_validate . models = {&quot;Logistic_Regression&quot;: LogisticRegression(random_state = 42), &quot;Ridge_Classification&quot;: RidgeClassifier(random_state = 42), &quot;SVC&quot;: SVC(random_state = 42), &quot;GaussianNB&quot;: GaussianNB(), &quot;KNClassifier&quot;: KNeighborsClassifier(n_neighbors = 5), &quot;RandomForestClassifier&quot;: RandomForestClassifier(max_depth = 10, n_jobs = -1, random_state = 42), &quot;XGBClassifier&quot;: XGBClassifier(n_estimators = 50, learning_rate = 0.03, n_jobs = -1, objective = &quot;binary:logistic&quot;, eval_metric = &quot;auc&quot;, tree_method = &quot;hist&quot;, random_state = 42)} . scores = {} for model_name, model in models.items(): model_pipe = make_pipeline(ct, sm, model) cross_val = cross_validate(model_pipe, X_train, y_train, cv = cv, scoring = [&quot;accuracy&quot;, &quot;recall&quot;, &quot;precision&quot;, &quot;roc_auc&quot;, &quot;f1&quot;]) del cross_val[&quot;fit_time&quot;] del cross_val[&quot;score_time&quot;] print(model_name + &quot; : n&quot;) for score_name, score_vals in cross_val.items(): score_mean = score_vals.mean() score_std = score_vals.std() cross_val[score_name] = score_mean print(f&quot;{score_name}: Mean: {(score_mean * 100):.2f} % Std: {(score_std * 100):.2f} n&quot;) print(&quot; n&quot;, &quot;-&quot; * 50, &quot; n&quot;) scores[model_name] = cross_val . Logistic_Regression : test_accuracy: Mean: 75.42 % Std: 1.51 test_recall: Mean: 78.91 % Std: 2.90 test_precision: Mean: 14.06 % Std: 0.93 test_roc_auc: Mean: 84.92 % Std: 0.96 test_f1: Mean: 23.86 % Std: 1.40 -- Ridge_Classification : test_accuracy: Mean: 73.95 % Std: 1.57 test_recall: Mean: 79.41 % Std: 3.60 test_precision: Mean: 13.41 % Std: 0.94 test_roc_auc: Mean: 85.04 % Std: 1.38 test_f1: Mean: 22.93 % Std: 1.47 -- SVC : test_accuracy: Mean: 81.97 % Std: 1.27 test_recall: Mean: 45.26 % Std: 10.97 test_precision: Mean: 12.36 % Std: 1.46 test_roc_auc: Mean: 76.44 % Std: 3.34 test_f1: Mean: 19.36 % Std: 2.79 -- GaussianNB : test_accuracy: Mean: 25.37 % Std: 4.74 test_recall: Mean: 99.50 % Std: 1.00 test_precision: Mean: 6.12 % Std: 0.43 test_roc_auc: Mean: 82.68 % Std: 1.54 test_f1: Mean: 11.53 % Std: 0.76 -- KNClassifier : test_accuracy: Mean: 83.76 % Std: 1.21 test_recall: Mean: 36.19 % Std: 3.84 test_precision: Mean: 11.84 % Std: 0.47 test_roc_auc: Mean: 65.28 % Std: 0.76 test_f1: Mean: 17.80 % Std: 0.71 -- RandomForestClassifier : test_accuracy: Mean: 83.41 % Std: 1.46 test_recall: Mean: 40.69 % Std: 5.24 test_precision: Mean: 12.65 % Std: 0.85 test_roc_auc: Mean: 80.27 % Std: 0.91 test_f1: Mean: 19.25 % Std: 1.29 -- XGBClassifier : test_accuracy: Mean: 77.28 % Std: 1.95 test_recall: Mean: 63.27 % Std: 6.57 test_precision: Mean: 12.89 % Std: 1.56 test_roc_auc: Mean: 80.95 % Std: 2.15 test_f1: Mean: 21.39 % Std: 2.45 -- . scores_df = pd.DataFrame.from_dict(scores, orient = &#39;index&#39;) scores_df.sort_values(by = &quot;test_roc_auc&quot;, ascending = False) . test_accuracy test_recall test_precision test_roc_auc test_f1 . Ridge_Classification 0.739481 | 0.794103 | 0.134058 | 0.850447 | 0.229299 | . Logistic_Regression 0.754158 | 0.789103 | 0.140608 | 0.849226 | 0.238578 | . GaussianNB 0.253661 | 0.995000 | 0.061211 | 0.826782 | 0.115301 | . XGBClassifier 0.772752 | 0.632692 | 0.128876 | 0.809516 | 0.213944 | . RandomForestClassifier 0.834148 | 0.406923 | 0.126488 | 0.802683 | 0.192495 | . SVC 0.819717 | 0.452564 | 0.123650 | 0.764393 | 0.193636 | . KNClassifier 0.837574 | 0.361923 | 0.118366 | 0.652839 | 0.177978 | . From the scores we get, linear models although lacking in accuracy porformed way much better than the others in recall, auc_roc, and f1 metrics. Their roc_auc score of above 80s but low precision score suggests that they can do well with threshold tuning. The tree models like RandomForestClassifier and XGBClassifier also achieved satisfactory performance with high accuracy and moderate roc_auc scores. KNClassifier and SVC performed the worst and may require some hyperparameter tuning. But time would be better spent tuning the linear models to get even better performance than to tune the other ML models. . Model Tuning . We&#39;ll train LogisticRegression and RidgeClassifier ML models with GridSearchCV to find the best model and its parameters. We can then train this model on the full train dataset and then make predictions on the test dataset in the next section. . models_and_params = {&quot;LogisticRegression&quot;: [LogisticRegression(random_state = 42), {&quot;logisticregression__class_weight&quot;: [{0:1, 1: 1}, {0:1, 1:3}]}], &quot;RidgeClassification&quot;: [RidgeClassifier(random_state = 42), {&quot;ridgeclassifier__alpha&quot;: [1, 2, 3], &quot;ridgeclassifier__class_weight&quot;: [{0:1, 1: 1}, {0:1, 1:3}]}]} . tuning_scores = [] for model_name in models_and_params: model, params = models_and_params[model_name] model_pipe = make_pipeline(ct, sm, model) grid_cv = GridSearchCV(model_pipe, params, scoring = [&quot;accuracy&quot;, &quot;recall&quot;, &quot;precision&quot;, &quot;roc_auc&quot;, &quot;f1&quot;], n_jobs = -1, cv = cv, refit = False) grid_cv.fit(X_train, y_train) tuning_scores.append(grid_cv) . logistic_regression_grid_result = pd.DataFrame.from_dict(tuning_scores[0].cv_results_) logistic_regression_grid_result = logistic_regression_grid_result[[ &quot;param_logisticregression__class_weight&quot;, &quot;mean_test_accuracy&quot;, &quot;mean_test_precision&quot;, &quot;mean_test_recall&quot;, &quot;mean_test_roc_auc&quot;, &quot;mean_test_f1&quot;]] logistic_regression_grid_result.sort_values(by = &quot;mean_test_roc_auc&quot;, ascending = False) . param_logisticregression__class_weight mean_test_accuracy mean_test_precision mean_test_recall mean_test_roc_auc mean_test_f1 . 0 {0: 1, 1: 1} | 0.754158 | 0.140608 | 0.789103 | 0.849226 | 0.238578 | . 1 {0: 1, 1: 3} | 0.610319 | 0.104987 | 0.929744 | 0.847900 | 0.188645 | . ridge_classifier_grid_result = pd.DataFrame.from_dict(tuning_scores[1].cv_results_) ridge_classifier_grid_result = ridge_classifier_grid_result[[ &quot;param_ridgeclassifier__alpha&quot;, &quot;param_ridgeclassifier__class_weight&quot;, &quot;mean_test_accuracy&quot;, &quot;mean_test_precision&quot;, &quot;mean_test_recall&quot;, &quot;mean_test_roc_auc&quot;, &quot;mean_test_f1&quot;]] ridge_classifier_grid_result.sort_values(by = &quot;mean_test_roc_auc&quot;, ascending = False) . param_ridgeclassifier__alpha param_ridgeclassifier__class_weight mean_test_accuracy mean_test_precision mean_test_recall mean_test_roc_auc mean_test_f1 . 4 3 | {0: 1, 1: 1} | 0.738258 | 0.133504 | 0.794103 | 0.850666 | 0.228482 | . 2 2 | {0: 1, 1: 1} | 0.739237 | 0.133963 | 0.794103 | 0.850492 | 0.229157 | . 0 1 | {0: 1, 1: 1} | 0.739481 | 0.134058 | 0.794103 | 0.850447 | 0.229299 | . 5 3 | {0: 1, 1: 3} | 0.582188 | 0.101112 | 0.959872 | 0.844802 | 0.182931 | . 3 2 | {0: 1, 1: 3} | 0.582432 | 0.101167 | 0.959872 | 0.844769 | 0.183022 | . 1 1 | {0: 1, 1: 3} | 0.583411 | 0.100952 | 0.954872 | 0.844712 | 0.182581 | . From these grid search, we have found that both the logistic regression and the ridge regression models are really close in performance on their roc_auc score which is the primary metric, and that the increasing alpha values in ridge classifier does help in improving the scores. Changing class weights improves test recall a lot but because of tradeoff between test recall and test precision, test precision takes a sligh hit, along with a large hit on test accuracy. Now, we have to make a call on what parameter is more important for us between test precision and test recall. with the roc_auc, the primary metric still high, we choose to prefer better test precision and test accuracy. . From this, we have identified the ML model - RidgeClassifier with the default class weghts 1:1 and alpha value of 3. Now, we will train this final model and make predictions on the test dataset and then evaluate those predictions. . Final Model and Predictions . We can now train a RidgeClassifier with the default parameters with a imblearn Pipeline and then make predictions on the test dataset. . ridge = RidgeClassifier(random_state = 42, alpha = 3) ridge_pipe = make_pipeline(ct, sm, ridge) ridge_pipe.fit(X_train, y_train) . Pipeline(steps=[(&#39;columntransformer&#39;, ColumnTransformer(remainder=&#39;passthrough&#39;, transformers=[(&#39;pipeline-1&#39;, Pipeline(steps=[(&#39;functiontransformer&#39;, FunctionTransformer(func=&lt;function to_int at 0x7fc4d14a3200&gt;)), (&#39;minmaxscaler&#39;, MinMaxScaler())]), [&#39;age&#39;]), (&#39;pipeline-2&#39;, Pipeline(steps=[(&#39;simpleimputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;minmaxscaler&#39;, MinM... [&#39;avg_glucose_level&#39;, &#39;bmi&#39;]), (&#39;missingindicator&#39;, MissingIndicator(), [&#39;avg_glucose_level&#39;, &#39;bmi&#39;]), (&#39;onehotencoder&#39;, OneHotEncoder(drop=&#39;first&#39;, handle_unknown=&#39;ignore&#39;, sparse=False), [&#39;gender&#39;, &#39;ever_married&#39;, &#39;work_type&#39;, &#39;Residence_type&#39;, &#39;smoking_status&#39;])])), (&#39;smote&#39;, SMOTE(random_state=42)), (&#39;ridgeclassifier&#39;, RidgeClassifier(alpha=3, random_state=42))]) . preds = ridge_pipe.predict(X_test) ConfusionMatrixDisplay.from_predictions(y_test, preds) . &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fc48bf1dc10&gt; . RocCurveDisplay.from_estimator(ridge_pipe, X_test, y_test, name = &quot;RidgeClassifier&quot;) print(&quot;Accuracy Score:&quot;, accuracy_score(y_test, preds)) print(&quot;Precision Score:&quot;, precision_score(y_test, preds)) print(&quot;Recall Score:&quot;, recall_score(y_test, preds)) . Accuracy Score: 0.7465753424657534 Precision Score: 0.1384083044982699 Recall Score: 0.8 . The classifier has predicted 723 True Negatives and 40 True Positives. There are 259 misclassifications too out of which 249 are False Positives. Considering that the predictions class was highly imbalanced to begin with, the model has worked really well in making correct classifications. Its AUC_ROC score on the test dataset is 0.85, similar to what we found on the train dataset. With more data, we can improve this score further. . Summary and Conclusion . In this project, we used the Stroke Dataset available on Kaggle to predict whether a patient would suffer from a stroke. First, we prepared the data for training and test by splitting it using train_test_split. Then we explored the data and understood where it needed some cleaning and preparation. With this knowledge, we developed imblearn&#39;s Pipelines to clean the data. We then explored multiple ML models and studied their performance through multiple metrics, primarily focusing on roc_auc scores. This choice of metrics was made with the knowledge that in the medicinal domain, the correct knowledge of True Positives is much more valuable than the wrong knowledge on False Positives. We chose 2 linear models from this step for further model tuning and selection. RidgeClassifier turned out to be performing the best with its default parameters. We then trained this model and made predictions on the test data that we got from the split earlier. On the predictions, we achieved respectable scores on recall - 0.8 and on precision - 0.14. .",
            "url": "https://ncitshubham.github.io/blogs/2021/07/23/stroke-prediction-auc-roc-85-accuracy-75.html",
            "relUrl": "/2021/07/23/stroke-prediction-auc-roc-85-accuracy-75.html",
            "date": " • Jul 23, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Predicting Mushroom Edibility",
            "content": "Introduction . Setup . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # data visualization import seaborn as sns # data visualization from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV from sklearn.preprocessing import OneHotEncoder, LabelEncoder from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay, precision_score, recall_score, accuracy_score, f1_score from sklearn.pipeline import make_pipeline from sklearn.dummy import DummyClassifier from sklearn.linear_model import LogisticRegression, RidgeClassifier from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier from xgboost import XGBClassifier from sklearn.naive_bayes import GaussianNB from sklearn.svm import SVC import warnings warnings.filterwarnings(&quot;ignore&quot;) import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . /kaggle/input/mushroom-classification/mushrooms.csv . file_dir = &quot;/kaggle/input/mushroom-classification/mushrooms.csv&quot; . !ls -lh {file_dir} . -rw-r--r-- 1 nobody nogroup 366K Nov 21 08:43 /kaggle/input/mushroom-classification/mushrooms.csv . !head {file_dir} . class,cap-shape,cap-surface,cap-color,bruises,odor,gill-attachment,gill-spacing,gill-size,gill-color,stalk-shape,stalk-root,stalk-surface-above-ring,stalk-surface-below-ring,stalk-color-above-ring,stalk-color-below-ring,veil-type,veil-color,ring-number,ring-type,spore-print-color,population,habitat p,x,s,n,t,p,f,c,n,k,e,e,s,s,w,w,p,w,o,p,k,s,u e,x,s,y,t,a,f,c,b,k,e,c,s,s,w,w,p,w,o,p,n,n,g e,b,s,w,t,l,f,c,b,n,e,c,s,s,w,w,p,w,o,p,n,n,m p,x,y,w,t,p,f,c,n,n,e,e,s,s,w,w,p,w,o,p,k,s,u e,x,s,g,f,n,f,w,b,k,t,e,s,s,w,w,p,w,o,e,n,a,g e,x,y,y,t,a,f,c,b,n,e,c,s,s,w,w,p,w,o,p,k,n,g e,b,s,w,t,a,f,c,b,g,e,c,s,s,w,w,p,w,o,p,k,n,m e,b,y,w,t,l,f,c,b,n,e,c,s,s,w,w,p,w,o,p,n,s,m p,x,y,w,t,p,f,c,n,p,e,e,s,s,w,w,p,w,o,p,k,v,g . Observations . The dataset file size is 366 KB. | It will be safe to import the whole dataset. | The prediction class is the first column. | There appears to be no index column | . df = pd.read_csv(file_dir) # view all columns pd.set_option(&quot;display.max_columns&quot;, None) df.head() . class cap-shape cap-surface cap-color bruises odor gill-attachment gill-spacing gill-size gill-color stalk-shape stalk-root stalk-surface-above-ring stalk-surface-below-ring stalk-color-above-ring stalk-color-below-ring veil-type veil-color ring-number ring-type spore-print-color population habitat . 0 p | x | s | n | t | p | f | c | n | k | e | e | s | s | w | w | p | w | o | p | k | s | u | . 1 e | x | s | y | t | a | f | c | b | k | e | c | s | s | w | w | p | w | o | p | n | n | g | . 2 e | b | s | w | t | l | f | c | b | n | e | c | s | s | w | w | p | w | o | p | n | n | m | . 3 p | x | y | w | t | p | f | c | n | n | e | e | s | s | w | w | p | w | o | p | k | s | u | . 4 e | x | s | g | f | n | f | w | b | k | t | e | s | s | w | w | p | w | o | e | n | a | g | . EDA and Data Preparation . X = df.copy() y = X.pop(&quot;class&quot;) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) . class_vc = df[&quot;class&quot;].value_counts() class_vc . e 4208 p 3916 Name: class, dtype: int64 . sns.barplot(x = class_vc.index, y = class_vc) . &lt;AxesSubplot:ylabel=&#39;class&#39;&gt; . The class counts are not much imbalanced. . Distribution of features . for i, cols in enumerate(df): feature_vc = df[cols].value_counts() print(feature_vc, &quot; n_________ n&quot;) plt.figure(i) sns.barplot(x = feature_vc.index, y = feature_vc) . e 4208 p 3916 Name: class, dtype: int64 _________ x 3656 f 3152 k 828 b 452 s 32 c 4 Name: cap-shape, dtype: int64 _________ y 3244 s 2556 f 2320 g 4 Name: cap-surface, dtype: int64 _________ n 2284 g 1840 e 1500 y 1072 w 1040 b 168 p 144 c 44 u 16 r 16 Name: cap-color, dtype: int64 _________ f 4748 t 3376 Name: bruises, dtype: int64 _________ n 3528 f 2160 y 576 s 576 a 400 l 400 p 256 c 192 m 36 Name: odor, dtype: int64 _________ f 7914 a 210 Name: gill-attachment, dtype: int64 _________ c 6812 w 1312 Name: gill-spacing, dtype: int64 _________ b 5612 n 2512 Name: gill-size, dtype: int64 _________ b 1728 p 1492 w 1202 n 1048 g 752 h 732 u 492 k 408 e 96 y 86 o 64 r 24 Name: gill-color, dtype: int64 _________ t 4608 e 3516 Name: stalk-shape, dtype: int64 _________ b 3776 ? 2480 e 1120 c 556 r 192 Name: stalk-root, dtype: int64 _________ s 5176 k 2372 f 552 y 24 Name: stalk-surface-above-ring, dtype: int64 _________ s 4936 k 2304 f 600 y 284 Name: stalk-surface-below-ring, dtype: int64 _________ w 4464 p 1872 g 576 n 448 b 432 o 192 e 96 c 36 y 8 Name: stalk-color-above-ring, dtype: int64 _________ w 4384 p 1872 g 576 n 512 b 432 o 192 e 96 c 36 y 24 Name: stalk-color-below-ring, dtype: int64 _________ p 8124 Name: veil-type, dtype: int64 _________ w 7924 n 96 o 96 y 8 Name: veil-color, dtype: int64 _________ o 7488 t 600 n 36 Name: ring-number, dtype: int64 _________ p 3968 e 2776 l 1296 f 48 n 36 Name: ring-type, dtype: int64 _________ w 2388 n 1968 k 1872 h 1632 r 72 u 48 o 48 y 48 b 48 Name: spore-print-color, dtype: int64 _________ v 4040 y 1712 s 1248 n 400 a 384 c 340 Name: population, dtype: int64 _________ d 3148 g 2148 p 1144 l 832 u 368 m 292 w 192 Name: habitat, dtype: int64 _________ . df[&quot;veil-type&quot;].unique() . array([&#39;p&#39;], dtype=object) . df.isnull().sum() . class 0 cap-shape 0 cap-surface 0 cap-color 0 bruises 0 odor 0 gill-attachment 0 gill-spacing 0 gill-size 0 gill-color 0 stalk-shape 0 stalk-root 0 stalk-surface-above-ring 0 stalk-surface-below-ring 0 stalk-color-above-ring 0 stalk-color-below-ring 0 veil-type 0 veil-color 0 ring-number 0 ring-type 0 spore-print-color 0 population 0 habitat 0 dtype: int64 . We also didn&#39;t need stratification.&quot;HistGradientBoosterClassifier&quot;: HistGradientBoostingClassifier(learning_rate = 0.05, random_state = 42, scoring=) . Train Baseline Model and Evaluate Results . ohe = OneHotEncoder(drop = &quot;first&quot;, handle_unknown = &quot;ignore&quot;, sparse = False) ohe_train_data = ohe.fit_transform(X_train) . le = LabelEncoder() y_train_le = le.fit_transform(y_train) . y_test_le = le.transform(y_test) . base_model = DummyClassifier(random_state = 42, strategy = &quot;most_frequent&quot;) base_cross_val = cross_validate(base_model, ohe_train_data, y_train_le, scoring = [&quot;accuracy&quot;, &quot;precision&quot;, &quot;recall&quot;, &quot;f1&quot;, &quot;roc_auc&quot;]) . base_cross_val . {&#39;fit_time&#39;: array([0.00459409, 0.00267696, 0.00175118, 0.00171733, 0.00184631]), &#39;score_time&#39;: array([0.00764799, 0.00651956, 0.00638986, 0.00660157, 0.00642753]), &#39;test_accuracy&#39;: array([0.51461538, 0.51461538, 0.51384615, 0.51384615, 0.51424172]), &#39;test_precision&#39;: array([0., 0., 0., 0., 0.]), &#39;test_recall&#39;: array([0., 0., 0., 0., 0.]), &#39;test_f1&#39;: array([0., 0., 0., 0., 0.]), &#39;test_roc_auc&#39;: array([0.5, 0.5, 0.5, 0.5, 0.5])} . Model Selection . models = {&quot;LogisticRegression&quot;: LogisticRegression(random_state = 42), &quot;RidgeClassification&quot;: RidgeClassifier(random_state = 42), &quot;GaussianNB&quot;: GaussianNB(), &quot;RandomForestClassifier&quot;: RandomForestClassifier(n_estimators = 70, random_state = 42), &quot;XGBClassifier&quot;: XGBClassifier(n_estimators = 70, objective = &quot;binary:logistic&quot;, learning_rate = 0.05, n_jobs = -1, scoring = &quot;auc&quot;, random_state = 42)} . model_scores = {} for model_name, model in models.items(): cross_val = cross_validate(model, ohe_train_data, y_train_le, n_jobs = -1, scoring = [&quot;accuracy&quot;, &quot;precision&quot;, &quot;recall&quot;, &quot;f1&quot;, &quot;roc_auc&quot;]) del cross_val[&quot;fit_time&quot;] del cross_val[&quot;score_time&quot;] model_scores[model_name] = cross_val . model_scores_df = pd.DataFrame.from_dict(model_scores) for col in model_scores_df: model_scores_df[col] = model_scores_df[col].mean() . model_scores_df . LogisticRegression RidgeClassification GaussianNB RandomForestClassifier XGBClassifier . test_accuracy 1.000000 | 1.000000 | 0.963843 | 1.0 | 1.0 | . test_precision 0.998741 | 0.998741 | 0.958774 | 1.0 | 1.0 | . test_recall 1.000000 | 1.000000 | 0.963281 | 1.0 | 1.0 | . test_f1 1.000000 | 1.000000 | 0.947865 | 1.0 | 1.0 | . test_roc_auc 0.999370 | 0.999370 | 0.948100 | 1.0 | 1.0 | . Train Final Model and make predictions . forest_clas = RandomForestClassifier(random_state = 42) ohe = OneHotEncoder(drop = &quot;first&quot;, handle_unknown = &quot;ignore&quot;, sparse = False) forest_pipe = make_pipeline(ohe, forest_clas) forest_pipe.fit(X_train, y_train_le) . Pipeline(steps=[(&#39;onehotencoder&#39;, OneHotEncoder(drop=&#39;first&#39;, handle_unknown=&#39;ignore&#39;, sparse=False)), (&#39;randomforestclassifier&#39;, RandomForestClassifier(random_state=42))]) . preds = forest_pipe.predict(X_test) preds_in = le.inverse_transform(preds) preds_in . array([&#39;e&#39;, &#39;p&#39;, &#39;e&#39;, ..., &#39;e&#39;, &#39;e&#39;, &#39;p&#39;], dtype=object) . ConfusionMatrixDisplay.from_predictions(y_test, preds_in) . &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fb2bcf36690&gt; . accuracy_score(y_test, preds_in) . 1.0 . recall_score(y_test_le, preds) . 1.0 . f1_score(y_test_le, preds) . 1.0 . RocCurveDisplay.from_estimator(forest_pipe, X_test, y_test_le) . &lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7fb2bcf51050&gt; . Summary and Conclusion .",
            "url": "https://ncitshubham.github.io/blogs/2021/06/02/classifying-mushrooms-on-edibility.html",
            "relUrl": "/2021/06/02/classifying-mushrooms-on-edibility.html",
            "date": " • Jun 2, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Exploratory Data Analysis of Road Accidents in USA",
            "content": "Introduction . Every year 1.3 million people die as a result of a road traffic crash around the world. And 20 - 50 million people suffer non-fatal injuries, with many incurring a disability as a result of their injury.1 In USA alone, there were 33,244 fatal motor vehicle crashes in 2019 in which 36,096 deaths occurred.2 . In this project, we are going to study the &#39;US Accidents&#39; dataset provided by Sobhan Moosavi on https://www.kaggle.com. We can analyse this data to gain some really interesting insights about road accidents in USA, such as the impact of environmental stimuli on accidental occurance, the change in the occurance of accidents with change in months, or which 5 states have the highest (or lowest) number of accidents. [Note: This Dataset covers 49 mainland states of the USA (excluding Alaska) for the time period: February 2016 to Dec 2020] . We will use the approach of Exploratory Data Analysis (EDA) for studying this data, which is used to analyse datasets to summarize their main characteristics, often using statistical graphics and other data visualization methods. EDA can help in seeing what the data can tell us beyond the formal modeling or hypothesis testing task.3 . Because this dataset is huge, with dozens of features, it can be used to answer a lot of questions. For this project, we are going to limit ourselves to 5-10 questions and then try to answer these questions. . Setup . Import the necessary libraries and get the file_path for the dataset. . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # data visualization import seaborn as sns # data visualization # Input data files are available in the read-only &quot;../input/&quot; directory # The following code will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;US_Accidents_Dec20_updated.csv/&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . US_Accidents_Dec20_updated.csv/US_Accidents_Dec20_updated.csv . Data Preparation and Cleaning . file_path = &quot;US_Accidents_Dec20_updated.csv/US_Accidents_Dec20_updated.csv&quot; us_accidents = pd.read_csv(file_path) . pd.set_option(&quot;display.max_columns&quot;, None) us_accidents.head() . ID Severity Start_Time End_Time Start_Lat Start_Lng End_Lat End_Lng Distance(mi) Description Number Street Side City County State Zipcode Country Timezone Airport_Code Weather_Timestamp Temperature(F) Wind_Chill(F) Humidity(%) Pressure(in) Visibility(mi) Wind_Direction Wind_Speed(mph) Precipitation(in) Weather_Condition Amenity Bump Crossing Give_Way Junction No_Exit Railway Roundabout Station Stop Traffic_Calming Traffic_Signal Turning_Loop Sunrise_Sunset Civil_Twilight Nautical_Twilight Astronomical_Twilight . 0 A-2716600 | 3 | 2016-02-08 00:37:08 | 2016-02-08 06:37:08 | 40.10891 | -83.09286 | 40.11206 | -83.03187 | 3.230 | Between Sawmill Rd/Exit 20 and OH-315/Olentang... | NaN | Outerbelt E | R | Dublin | Franklin | OH | 43017 | US | US/Eastern | KOSU | 2016-02-08 00:53:00 | 42.1 | 36.1 | 58.0 | 29.76 | 10.0 | SW | 10.4 | 0.00 | Light Rain | False | False | False | False | False | False | False | False | False | False | False | False | False | Night | Night | Night | Night | . 1 A-2716601 | 2 | 2016-02-08 05:56:20 | 2016-02-08 11:56:20 | 39.86542 | -84.06280 | 39.86501 | -84.04873 | 0.747 | At OH-4/OH-235/Exit 41 - Accident. | NaN | I-70 E | R | Dayton | Montgomery | OH | 45424 | US | US/Eastern | KFFO | 2016-02-08 05:58:00 | 36.9 | NaN | 91.0 | 29.68 | 10.0 | Calm | NaN | 0.02 | Light Rain | False | False | False | False | False | False | False | False | False | False | False | False | False | Night | Night | Night | Night | . 2 A-2716602 | 2 | 2016-02-08 06:15:39 | 2016-02-08 12:15:39 | 39.10266 | -84.52468 | 39.10209 | -84.52396 | 0.055 | At I-71/US-50/Exit 1 - Accident. | NaN | I-75 S | R | Cincinnati | Hamilton | OH | 45203 | US | US/Eastern | KLUK | 2016-02-08 05:53:00 | 36.0 | NaN | 97.0 | 29.70 | 10.0 | Calm | NaN | 0.02 | Overcast | False | False | False | False | True | False | False | False | False | False | False | False | False | Night | Night | Night | Day | . 3 A-2716603 | 2 | 2016-02-08 06:15:39 | 2016-02-08 12:15:39 | 39.10148 | -84.52341 | 39.09841 | -84.52241 | 0.219 | At I-71/US-50/Exit 1 - Accident. | NaN | US-50 E | R | Cincinnati | Hamilton | OH | 45202 | US | US/Eastern | KLUK | 2016-02-08 05:53:00 | 36.0 | NaN | 97.0 | 29.70 | 10.0 | Calm | NaN | 0.02 | Overcast | False | False | False | False | True | False | False | False | False | False | False | False | False | Night | Night | Night | Day | . 4 A-2716604 | 2 | 2016-02-08 06:51:45 | 2016-02-08 12:51:45 | 41.06213 | -81.53784 | 41.06217 | -81.53547 | 0.123 | At Dart Ave/Exit 21 - Accident. | NaN | I-77 N | R | Akron | Summit | OH | 44311 | US | US/Eastern | KAKR | 2016-02-08 06:54:00 | 39.0 | NaN | 55.0 | 29.65 | 10.0 | Calm | NaN | NaN | Overcast | False | False | False | False | False | False | False | False | False | False | False | False | False | Night | Night | Day | Day | . Premilinary analysis of dataset . us_accidents.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1516064 entries, 0 to 1516063 Data columns (total 47 columns): # Column Non-Null Count Dtype -- -- 0 ID 1516064 non-null object 1 Severity 1516064 non-null int64 2 Start_Time 1516064 non-null object 3 End_Time 1516064 non-null object 4 Start_Lat 1516064 non-null float64 5 Start_Lng 1516064 non-null float64 6 End_Lat 1516064 non-null float64 7 End_Lng 1516064 non-null float64 8 Distance(mi) 1516064 non-null float64 9 Description 1516064 non-null object 10 Number 469969 non-null float64 11 Street 1516064 non-null object 12 Side 1516064 non-null object 13 City 1515981 non-null object 14 County 1516064 non-null object 15 State 1516064 non-null object 16 Zipcode 1515129 non-null object 17 Country 1516064 non-null object 18 Timezone 1513762 non-null object 19 Airport_Code 1511816 non-null object 20 Weather_Timestamp 1485800 non-null object 21 Temperature(F) 1473031 non-null float64 22 Wind_Chill(F) 1066748 non-null float64 23 Humidity(%) 1470555 non-null float64 24 Pressure(in) 1479790 non-null float64 25 Visibility(mi) 1471853 non-null float64 26 Wind_Direction 1474206 non-null object 27 Wind_Speed(mph) 1387202 non-null float64 28 Precipitation(in) 1005515 non-null float64 29 Weather_Condition 1472057 non-null object 30 Amenity 1516064 non-null bool 31 Bump 1516064 non-null bool 32 Crossing 1516064 non-null bool 33 Give_Way 1516064 non-null bool 34 Junction 1516064 non-null bool 35 No_Exit 1516064 non-null bool 36 Railway 1516064 non-null bool 37 Roundabout 1516064 non-null bool 38 Station 1516064 non-null bool 39 Stop 1516064 non-null bool 40 Traffic_Calming 1516064 non-null bool 41 Traffic_Signal 1516064 non-null bool 42 Turning_Loop 1516064 non-null bool 43 Sunrise_Sunset 1515981 non-null object 44 Civil_Twilight 1515981 non-null object 45 Nautical_Twilight 1515981 non-null object 46 Astronomical_Twilight 1515981 non-null object dtypes: bool(13), float64(13), int64(1), object(20) memory usage: 412.1+ MB . us_accidents.describe() . Severity Start_Lat Start_Lng End_Lat End_Lng Distance(mi) Number Temperature(F) Wind_Chill(F) Humidity(%) Pressure(in) Visibility(mi) Wind_Speed(mph) Precipitation(in) . count 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 4.699690e+05 | 1.473031e+06 | 1.066748e+06 | 1.470555e+06 | 1.479790e+06 | 1.471853e+06 | 1.387202e+06 | 1.005515e+06 | . mean 2.238630e+00 | 3.690056e+01 | -9.859919e+01 | 3.690061e+01 | -9.859901e+01 | 5.872617e-01 | 8.907533e+03 | 5.958460e+01 | 5.510976e+01 | 6.465960e+01 | 2.955495e+01 | 9.131755e+00 | 7.630812e+00 | 8.477855e-03 | . std 6.081481e-01 | 5.165653e+00 | 1.849602e+01 | 5.165629e+00 | 1.849590e+01 | 1.632659e+00 | 2.242190e+04 | 1.827316e+01 | 2.112735e+01 | 2.325986e+01 | 1.016756e+00 | 2.889112e+00 | 5.637364e+00 | 1.293168e-01 | . min 1.000000e+00 | 2.457022e+01 | -1.244976e+02 | 2.457011e+01 | -1.244978e+02 | 0.000000e+00 | 0.000000e+00 | -8.900000e+01 | -8.900000e+01 | 1.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 25% 2.000000e+00 | 3.385422e+01 | -1.182076e+02 | 3.385420e+01 | -1.182077e+02 | 0.000000e+00 | 1.212000e+03 | 4.700000e+01 | 4.080000e+01 | 4.800000e+01 | 2.944000e+01 | 1.000000e+01 | 4.600000e+00 | 0.000000e+00 | . 50% 2.000000e+00 | 3.735113e+01 | -9.438100e+01 | 3.735134e+01 | -9.437987e+01 | 1.780000e-01 | 4.000000e+03 | 6.100000e+01 | 5.700000e+01 | 6.800000e+01 | 2.988000e+01 | 1.000000e+01 | 7.000000e+00 | 0.000000e+00 | . 75% 2.000000e+00 | 4.072593e+01 | -8.087469e+01 | 4.072593e+01 | -8.087449e+01 | 5.940000e-01 | 1.010000e+04 | 7.300000e+01 | 7.100000e+01 | 8.400000e+01 | 3.004000e+01 | 1.000000e+01 | 1.040000e+01 | 0.000000e+00 | . max 4.000000e+00 | 4.900058e+01 | -6.711317e+01 | 4.907500e+01 | -6.710924e+01 | 1.551860e+02 | 9.999997e+06 | 1.706000e+02 | 1.130000e+02 | 1.000000e+02 | 5.804000e+01 | 1.400000e+02 | 9.840000e+02 | 2.400000e+01 | . Number of null values in the dataset, column vise . null_val_cols = us_accidents.isnull().sum() null_val_cols = null_val_cols[null_val_cols &gt; 0].sort_values(ascending = False) null_val_cols . Number 1046095 Precipitation(in) 510549 Wind_Chill(F) 449316 Wind_Speed(mph) 128862 Humidity(%) 45509 Visibility(mi) 44211 Weather_Condition 44007 Temperature(F) 43033 Wind_Direction 41858 Pressure(in) 36274 Weather_Timestamp 30264 Airport_Code 4248 Timezone 2302 Zipcode 935 City 83 Sunrise_Sunset 83 Civil_Twilight 83 Nautical_Twilight 83 Astronomical_Twilight 83 dtype: int64 . missing_percentage = null_val_cols / len(us_accidents) * 100 missing_percentage.plot(kind = &#39;barh&#39;, figsize = (14, 8)) plt.xlabel(&quot;Percentage of missing values&quot;) plt.ylabel(&quot;Columns&quot;) plt.show() . The Number column represents the street number in address record. It has over a million null values. Because we already have the location data from the Start_Lat, Start_Lng, County and State features, we can choose to drop this column from the dataset. With regards to columns representing weather conditions, we can choose to drop the columns having a significant number of null values, and impute other columns to the mean values from the same State and month, which should represent similar weather conditions. . us_accidents.drop(labels = [&quot;Number&quot;, &quot;Precipitation(in)&quot;], axis = 1, inplace = True) us_accidents.shape . (1516064, 45) . We&#39;ll also convert the Start_Time column to datetime dtype, so that we can use use it to apply datetime functions later on. . us_accidents.Start_Time = us_accidents.Start_Time.astype(&quot;datetime64&quot;) . Exploratory Analysis and Visualization . With these columns in mind, we will try to answer the following questions from the dataset: . Are there more accidents in warmer or colder areas? | Which 5 states have the highest number of accidents? | Among the top 100 cities in number of accidents, which states do they belong to most frequently. | What time of the day are accidents most frequent in? | Which days of the week have the most accidents? | Which months have the most accidents? | What is the trend of accidents year over year (decreasing/increasing?) | How does accident severity change with change in precipitation? | We can start with plotting longitude and lattitude data to get a sense of where is the data concentrated . lat, long = us_accidents.Start_Lat, us_accidents.Start_Lng . plt.figure(figsize = (10, 6)) sns.scatterplot(x = long, y = lat, s = 0.5) . &lt;AxesSubplot:xlabel=&#39;Start_Lng&#39;, ylabel=&#39;Start_Lat&#39;&gt; . It appears that accidents are more concentrated near the coasts, which are more populated areas of US. There seems to be a sharp decline in the mid - mid-western US. Apart from lack of accidents in those areas, it could also be a signal of poor data collection in those states. . Q1. Are there more accidents in warmer or colder areas? . First, we&#39;ll plot a histogram for the temperature data. . us_accidents[&quot;Temperature(F)&quot;].plot(kind = &quot;hist&quot;, logy = True) plt.xlabel(&quot;Temperature(F)&quot;) plt.show() . print(f&#39;&#39;&#39;Mean: {us_accidents[&quot;Temperature(F)&quot;].mean()} Skewness: {us_accidents[&quot;Temperature(F)&quot;].skew()} Kurtosis: {us_accidents[&quot;Temperature(F)&quot;].kurtosis()}&#39;&#39;&#39;) . Mean: 58.5816432896377 Skewness: -0.30533266112446805 Kurtosis: -0.13261978426888454 . We can see that the data is normally distributed with very low skewness and kurtosis values. Thus, the data doesn&#39;t support the idea that warmer areas have more accidents than colder ones or vice versa. . Q2. Which 5 states have the highest number of accidents? . accidents_by_states = us_accidents.State.value_counts() accidents_by_states[:5] . CA 291196 FL 105691 OR 60876 MN 41114 NY 38290 Name: State, dtype: int64 . plt.figure(figsize = (16, 5)) sns.barplot(x = accidents_by_states.index, y = accidents_by_states.values) . &lt;AxesSubplot:&gt; . Q3. Among the top 100 cities in number of accidents, which states do they belong to most frequently. . city_accidents = us_accidents.City.value_counts() city_accidents . Los Angeles 27760 Miami 26831 Orlando 10772 Dallas 10522 Charlotte 10312 ... Wilkeson 1 Culloden 1 Crestview Hills 1 Dinosaur 1 American Fork-Pleasant Grove 1 Name: City, Length: 8769, dtype: int64 . city_states = us_accidents.groupby(&quot;City&quot;)[&quot;State&quot;].aggregate(pd.Series.mode) city_states . City Aaronsburg PA Abbeville SC Abbotsford WI Abbottstown PA Aberdeen MD .. Zortman MT Zumbro Falls MN Zumbrota MN Zuni VA Zwingle IA Name: State, Length: 8769, dtype: object . top_100_cities = pd.concat([city_accidents, city_states], axis = 1)[:100] top_100_cities . City State . Los Angeles 27760 | CA | . Miami 26831 | FL | . Orlando 10772 | FL | . Dallas 10522 | TX | . Charlotte 10312 | NC | . ... ... | ... | . Flint 1465 | MI | . Hollywood 1431 | FL | . Eugene 1427 | OR | . Silver Spring 1425 | MD | . Birmingham 1422 | AL | . 100 rows × 2 columns . most_freq_states = top_100_cities.State.value_counts() most_freq_states . CA 35 FL 13 TX 5 NY 4 OR 4 MI 3 VA 3 LA 3 PA 3 SC 2 MO 2 UT 2 AZ 2 TN 2 MN 2 NC 2 OH 2 OK 1 NJ 1 MD 1 KY 1 IN 1 CO 1 DC 1 WA 1 IL 1 GA 1 AL 1 Name: State, dtype: int64 . most_freq_states.plot(kind = &quot;bar&quot;, figsize = (15,5)) plt.xlabel(&quot;States&quot;) plt.ylabel(&quot;Frequency in top 100 cities by number of accidents&quot;) plt.title(&quot;Most frequent states in the top 100 cities in the number of accidents&quot;) plt.show() . Q4. What time of the day are accidents most frequent in? . Do more accidents tend to occur at a particular time of the day? . us_accidents.Start_Time.dt.hour.plot(kind = &#39;hist&#39;, bins = 24) plt.xlabel(&quot;Hour of the day&quot;) plt.show() . It appears that accidents tend to occur more in the morning between 7-9 and in the afternoon between 13-17. Office hours could be the reason behind this trend. We can examine this by separating the data for weekdays and weekends. . weekdays = us_accidents.Start_Time.dt.dayofweek &lt; 5 us_accidents.loc[weekdays].Start_Time.dt.hour.plot(kind = &#39;hist&#39;, bins = 24) plt.xlabel(&quot;Hour of the day&quot;) plt.title(&quot;Accidents frequency of Weekdays&quot;) plt.show() . us_accidents.loc[~weekdays].Start_Time.dt.hour.plot(kind = &#39;hist&#39;, bins = 24) plt.xlabel(&quot;Hour of the day&quot;) plt.title(&quot;Accidents frequency of Weekends&quot;) plt.show() . On weekends, there seems to be a breakaway from the pattern that we saw earlier, which supports the idea that the trend we saw earlier could be due to the traffic resulting from office timings, which are usually closed on weekends. On weekends, accidents tend to occur more during daylight, which should be due to more traffic during those hours. . Q5. Which days of the week have the most accidents? . sns.histplot(us_accidents.Start_Time.dt.dayofweek, bins = 7) plt.xlabel(&quot;Day of Week&quot;) plt.show() . Accidents occur more on weekdays and there is a sharp drop in their count on the weekends. . Q6. Which months have the most accidents? . sns.histplot(us_accidents.Start_Time.dt.month, bins = 12) plt.xlabel(&quot;Month&quot;) plt.show() . Later months of the year appear to witness more accidents. This cannot be due to temperature(winter season) because the count drops suddenly in the starting months of the year. Due to there being no other apparent cause, it demands further analysis. . We can start by looking at the trend year over year. . fig, axes = plt.subplots(2, 3, figsize = (15,10), ) year = 2016 for r in range(2): for c in range(3): if year &lt; 2021: year_data = us_accidents.loc[us_accidents.Start_Time.dt.year == year] sns.histplot(year_data.Start_Time.dt.month, bins = 12, ax = axes[r, c]) axes[r, c].set_title(year) axes[r, c].set_xlabel(&quot;Month of the year&quot;) year += 1 . We can see that probably because the data was started being collected in the year 2016, the starting months have a much lower number of datapoints. Also, in the year 2020, because of the coronavirus pandemic lockdown restrictions, there was a suddent drop in the middle of the year. And as the restrictions eased, more accidents started to occur. But this data requires further more analysis on the month wise trends. . Q7. What is the trend of accidents year over year (decreasing/increasing?) . us_accidents.Start_Time.dt.year.plot(kind = &#39;hist&#39;, bins = 5, title = &quot;Year wise trend&quot;, xticks = np.arange(2016, 2021), figsize = (7, 5)) plt.show() . The number of accidents year over year looks to be increasing. This can be attributed to better data collection in the later years. Thus, it need further analysis. . Summary and Conclusion .",
            "url": "https://ncitshubham.github.io/blogs/2021/04/16/exploratory-data-analysis-of-road-accidents-in-usa.html",
            "relUrl": "/2021/04/16/exploratory-data-analysis-of-road-accidents-in-usa.html",
            "date": " • Apr 16, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Exploratory Data Analysis of Road Accidents in USA",
            "content": "Introduction . Every year 1.3 million people die as a result of a road traffic crash around the world. And 20 - 50 million people suffer non-fatal injuries, with many incurring a disability as a result of their injury.1 In USA alone, there were 33,244 fatal motor vehicle crashes in 2019 in which 36,096 deaths occurred.2 . In this project, we are going to study the &#39;US Accidents&#39; dataset provided by Sobhan Moosavi on https://www.kaggle.com. We can analyse this data to gain some really interesting insights about road accidents in USA, such as the impact of environmental stimuli on accidental occurance, the change in the occurance of accidents with change in months, or which 5 states have the highest (or lowest) number of accidents. [Note: This Dataset covers 49 mainland states of the USA (excluding Alaska) for the time period: February 2016 to Dec 2020] . We will use the approach of Exploratory Data Analysis (EDA) for studying this data, which is used to analyse datasets to summarize their main characteristics, often using statistical graphics and other data visualization methods. EDA can help in seeing what the data can tell us beyond the formal modeling or hypothesis testing task.3 . Because this dataset is huge, with dozens of features, it can be used to answer a lot of questions. For this project, we are going to limit ourselves to 5-10 questions and then try to answer these questions. . Setup . Import the necessary libraries and get the file_path for the dataset. . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # data visualization import seaborn as sns # data visualization # Input data files are available in the read-only &quot;../input/&quot; directory # The following code will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;US_Accidents_Dec20_updated.csv/&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . US_Accidents_Dec20_updated.csv/US_Accidents_Dec20_updated.csv . Data Preparation and Cleaning . file_path = &quot;US_Accidents_Dec20_updated.csv/US_Accidents_Dec20_updated.csv&quot; us_accidents = pd.read_csv(file_path) . pd.set_option(&quot;display.max_columns&quot;, None) us_accidents.head() . ID Severity Start_Time End_Time Start_Lat Start_Lng End_Lat End_Lng Distance(mi) Description Number Street Side City County State Zipcode Country Timezone Airport_Code Weather_Timestamp Temperature(F) Wind_Chill(F) Humidity(%) Pressure(in) Visibility(mi) Wind_Direction Wind_Speed(mph) Precipitation(in) Weather_Condition Amenity Bump Crossing Give_Way Junction No_Exit Railway Roundabout Station Stop Traffic_Calming Traffic_Signal Turning_Loop Sunrise_Sunset Civil_Twilight Nautical_Twilight Astronomical_Twilight . 0 A-2716600 | 3 | 2016-02-08 00:37:08 | 2016-02-08 06:37:08 | 40.10891 | -83.09286 | 40.11206 | -83.03187 | 3.230 | Between Sawmill Rd/Exit 20 and OH-315/Olentang... | NaN | Outerbelt E | R | Dublin | Franklin | OH | 43017 | US | US/Eastern | KOSU | 2016-02-08 00:53:00 | 42.1 | 36.1 | 58.0 | 29.76 | 10.0 | SW | 10.4 | 0.00 | Light Rain | False | False | False | False | False | False | False | False | False | False | False | False | False | Night | Night | Night | Night | . 1 A-2716601 | 2 | 2016-02-08 05:56:20 | 2016-02-08 11:56:20 | 39.86542 | -84.06280 | 39.86501 | -84.04873 | 0.747 | At OH-4/OH-235/Exit 41 - Accident. | NaN | I-70 E | R | Dayton | Montgomery | OH | 45424 | US | US/Eastern | KFFO | 2016-02-08 05:58:00 | 36.9 | NaN | 91.0 | 29.68 | 10.0 | Calm | NaN | 0.02 | Light Rain | False | False | False | False | False | False | False | False | False | False | False | False | False | Night | Night | Night | Night | . 2 A-2716602 | 2 | 2016-02-08 06:15:39 | 2016-02-08 12:15:39 | 39.10266 | -84.52468 | 39.10209 | -84.52396 | 0.055 | At I-71/US-50/Exit 1 - Accident. | NaN | I-75 S | R | Cincinnati | Hamilton | OH | 45203 | US | US/Eastern | KLUK | 2016-02-08 05:53:00 | 36.0 | NaN | 97.0 | 29.70 | 10.0 | Calm | NaN | 0.02 | Overcast | False | False | False | False | True | False | False | False | False | False | False | False | False | Night | Night | Night | Day | . 3 A-2716603 | 2 | 2016-02-08 06:15:39 | 2016-02-08 12:15:39 | 39.10148 | -84.52341 | 39.09841 | -84.52241 | 0.219 | At I-71/US-50/Exit 1 - Accident. | NaN | US-50 E | R | Cincinnati | Hamilton | OH | 45202 | US | US/Eastern | KLUK | 2016-02-08 05:53:00 | 36.0 | NaN | 97.0 | 29.70 | 10.0 | Calm | NaN | 0.02 | Overcast | False | False | False | False | True | False | False | False | False | False | False | False | False | Night | Night | Night | Day | . 4 A-2716604 | 2 | 2016-02-08 06:51:45 | 2016-02-08 12:51:45 | 41.06213 | -81.53784 | 41.06217 | -81.53547 | 0.123 | At Dart Ave/Exit 21 - Accident. | NaN | I-77 N | R | Akron | Summit | OH | 44311 | US | US/Eastern | KAKR | 2016-02-08 06:54:00 | 39.0 | NaN | 55.0 | 29.65 | 10.0 | Calm | NaN | NaN | Overcast | False | False | False | False | False | False | False | False | False | False | False | False | False | Night | Night | Day | Day | . Premilinary analysis of dataset . us_accidents.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1516064 entries, 0 to 1516063 Data columns (total 47 columns): # Column Non-Null Count Dtype -- -- 0 ID 1516064 non-null object 1 Severity 1516064 non-null int64 2 Start_Time 1516064 non-null object 3 End_Time 1516064 non-null object 4 Start_Lat 1516064 non-null float64 5 Start_Lng 1516064 non-null float64 6 End_Lat 1516064 non-null float64 7 End_Lng 1516064 non-null float64 8 Distance(mi) 1516064 non-null float64 9 Description 1516064 non-null object 10 Number 469969 non-null float64 11 Street 1516064 non-null object 12 Side 1516064 non-null object 13 City 1515981 non-null object 14 County 1516064 non-null object 15 State 1516064 non-null object 16 Zipcode 1515129 non-null object 17 Country 1516064 non-null object 18 Timezone 1513762 non-null object 19 Airport_Code 1511816 non-null object 20 Weather_Timestamp 1485800 non-null object 21 Temperature(F) 1473031 non-null float64 22 Wind_Chill(F) 1066748 non-null float64 23 Humidity(%) 1470555 non-null float64 24 Pressure(in) 1479790 non-null float64 25 Visibility(mi) 1471853 non-null float64 26 Wind_Direction 1474206 non-null object 27 Wind_Speed(mph) 1387202 non-null float64 28 Precipitation(in) 1005515 non-null float64 29 Weather_Condition 1472057 non-null object 30 Amenity 1516064 non-null bool 31 Bump 1516064 non-null bool 32 Crossing 1516064 non-null bool 33 Give_Way 1516064 non-null bool 34 Junction 1516064 non-null bool 35 No_Exit 1516064 non-null bool 36 Railway 1516064 non-null bool 37 Roundabout 1516064 non-null bool 38 Station 1516064 non-null bool 39 Stop 1516064 non-null bool 40 Traffic_Calming 1516064 non-null bool 41 Traffic_Signal 1516064 non-null bool 42 Turning_Loop 1516064 non-null bool 43 Sunrise_Sunset 1515981 non-null object 44 Civil_Twilight 1515981 non-null object 45 Nautical_Twilight 1515981 non-null object 46 Astronomical_Twilight 1515981 non-null object dtypes: bool(13), float64(13), int64(1), object(20) memory usage: 412.1+ MB . us_accidents.describe() . Severity Start_Lat Start_Lng End_Lat End_Lng Distance(mi) Number Temperature(F) Wind_Chill(F) Humidity(%) Pressure(in) Visibility(mi) Wind_Speed(mph) Precipitation(in) . count 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 1.516064e+06 | 4.699690e+05 | 1.473031e+06 | 1.066748e+06 | 1.470555e+06 | 1.479790e+06 | 1.471853e+06 | 1.387202e+06 | 1.005515e+06 | . mean 2.238630e+00 | 3.690056e+01 | -9.859919e+01 | 3.690061e+01 | -9.859901e+01 | 5.872617e-01 | 8.907533e+03 | 5.958460e+01 | 5.510976e+01 | 6.465960e+01 | 2.955495e+01 | 9.131755e+00 | 7.630812e+00 | 8.477855e-03 | . std 6.081481e-01 | 5.165653e+00 | 1.849602e+01 | 5.165629e+00 | 1.849590e+01 | 1.632659e+00 | 2.242190e+04 | 1.827316e+01 | 2.112735e+01 | 2.325986e+01 | 1.016756e+00 | 2.889112e+00 | 5.637364e+00 | 1.293168e-01 | . min 1.000000e+00 | 2.457022e+01 | -1.244976e+02 | 2.457011e+01 | -1.244978e+02 | 0.000000e+00 | 0.000000e+00 | -8.900000e+01 | -8.900000e+01 | 1.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | . 25% 2.000000e+00 | 3.385422e+01 | -1.182076e+02 | 3.385420e+01 | -1.182077e+02 | 0.000000e+00 | 1.212000e+03 | 4.700000e+01 | 4.080000e+01 | 4.800000e+01 | 2.944000e+01 | 1.000000e+01 | 4.600000e+00 | 0.000000e+00 | . 50% 2.000000e+00 | 3.735113e+01 | -9.438100e+01 | 3.735134e+01 | -9.437987e+01 | 1.780000e-01 | 4.000000e+03 | 6.100000e+01 | 5.700000e+01 | 6.800000e+01 | 2.988000e+01 | 1.000000e+01 | 7.000000e+00 | 0.000000e+00 | . 75% 2.000000e+00 | 4.072593e+01 | -8.087469e+01 | 4.072593e+01 | -8.087449e+01 | 5.940000e-01 | 1.010000e+04 | 7.300000e+01 | 7.100000e+01 | 8.400000e+01 | 3.004000e+01 | 1.000000e+01 | 1.040000e+01 | 0.000000e+00 | . max 4.000000e+00 | 4.900058e+01 | -6.711317e+01 | 4.907500e+01 | -6.710924e+01 | 1.551860e+02 | 9.999997e+06 | 1.706000e+02 | 1.130000e+02 | 1.000000e+02 | 5.804000e+01 | 1.400000e+02 | 9.840000e+02 | 2.400000e+01 | . Number of null values in the dataset, column vise . null_val_cols = us_accidents.isnull().sum() null_val_cols = null_val_cols[null_val_cols &gt; 0].sort_values(ascending = False) null_val_cols . Number 1046095 Precipitation(in) 510549 Wind_Chill(F) 449316 Wind_Speed(mph) 128862 Humidity(%) 45509 Visibility(mi) 44211 Weather_Condition 44007 Temperature(F) 43033 Wind_Direction 41858 Pressure(in) 36274 Weather_Timestamp 30264 Airport_Code 4248 Timezone 2302 Zipcode 935 City 83 Sunrise_Sunset 83 Civil_Twilight 83 Nautical_Twilight 83 Astronomical_Twilight 83 dtype: int64 . missing_percentage = null_val_cols / len(us_accidents) * 100 missing_percentage.plot(kind = &#39;barh&#39;, figsize = (14, 8)) plt.xlabel(&quot;Percentage of missing values&quot;) plt.ylabel(&quot;Columns&quot;) plt.show() . The Number column represents the street number in address record. It has over a million null values. Because we already have the location data from the Start_Lat, Start_Lng, County and State features, we can choose to drop this column from the dataset. With regards to columns representing weather conditions, we can choose to drop the columns having a significant number of null values, and impute other columns to the mean values from the same State and month, which should represent similar weather conditions. . us_accidents.drop(labels = [&quot;Number&quot;, &quot;Precipitation(in)&quot;], axis = 1, inplace = True) us_accidents.shape . (1516064, 45) . We&#39;ll also convert the Start_Time column to datetime dtype, so that we can use use it to apply datetime functions later on. . us_accidents.Start_Time = us_accidents.Start_Time.astype(&quot;datetime64&quot;) . Exploratory Analysis and Visualization . With these columns in mind, we will try to answer the following questions from the dataset: . Are there more accidents in warmer or colder areas? | Which 5 states have the highest number of accidents? | Among the top 100 cities in number of accidents, which states do they belong to most frequently. | What time of the day are accidents most frequent in? | Which days of the week have the most accidents? | Which months have the most accidents? | What is the trend of accidents year over year (decreasing/increasing?) | How does accident severity change with change in precipitation? | We can start with plotting longitude and lattitude data to get a sense of where is the data concentrated . lat, long = us_accidents.Start_Lat, us_accidents.Start_Lng . plt.figure(figsize = (10, 6)) sns.scatterplot(x = long, y = lat, s = 0.5) . &lt;AxesSubplot:xlabel=&#39;Start_Lng&#39;, ylabel=&#39;Start_Lat&#39;&gt; . It appears that accidents are more concentrated near the coasts, which are more populated areas of US. There seems to be a sharp decline in the mid - mid-western US. Apart from lack of accidents in those areas, it could also be a signal of poor data collection in those states. . Q1. Are there more accidents in warmer or colder areas? . First, we&#39;ll plot a histogram for the temperature data. . us_accidents[&quot;Temperature(F)&quot;].plot(kind = &quot;hist&quot;, logy = True) plt.xlabel(&quot;Temperature(F)&quot;) plt.show() . print(f&#39;&#39;&#39;Mean: {us_accidents[&quot;Temperature(F)&quot;].mean()} Skewness: {us_accidents[&quot;Temperature(F)&quot;].skew()} Kurtosis: {us_accidents[&quot;Temperature(F)&quot;].kurtosis()}&#39;&#39;&#39;) . Mean: 58.5816432896377 Skewness: -0.30533266112446805 Kurtosis: -0.13261978426888454 . We can see that the data is normally distributed with very low skewness and kurtosis values. Thus, the data doesn&#39;t support the idea that warmer areas have more accidents than colder ones or vice versa. . Q2. Which 5 states have the highest number of accidents? . accidents_by_states = us_accidents.State.value_counts() accidents_by_states[:5] . CA 291196 FL 105691 OR 60876 MN 41114 NY 38290 Name: State, dtype: int64 . plt.figure(figsize = (16, 5)) sns.barplot(x = accidents_by_states.index, y = accidents_by_states.values) . &lt;AxesSubplot:&gt; . Q3. Among the top 100 cities in number of accidents, which states do they belong to most frequently. . city_accidents = us_accidents.City.value_counts() city_accidents . Los Angeles 27760 Miami 26831 Orlando 10772 Dallas 10522 Charlotte 10312 ... Wilkeson 1 Culloden 1 Crestview Hills 1 Dinosaur 1 American Fork-Pleasant Grove 1 Name: City, Length: 8769, dtype: int64 . city_states = us_accidents.groupby(&quot;City&quot;)[&quot;State&quot;].aggregate(pd.Series.mode) city_states . City Aaronsburg PA Abbeville SC Abbotsford WI Abbottstown PA Aberdeen MD .. Zortman MT Zumbro Falls MN Zumbrota MN Zuni VA Zwingle IA Name: State, Length: 8769, dtype: object . top_100_cities = pd.concat([city_accidents, city_states], axis = 1)[:100] top_100_cities . City State . Los Angeles 27760 | CA | . Miami 26831 | FL | . Orlando 10772 | FL | . Dallas 10522 | TX | . Charlotte 10312 | NC | . ... ... | ... | . Flint 1465 | MI | . Hollywood 1431 | FL | . Eugene 1427 | OR | . Silver Spring 1425 | MD | . Birmingham 1422 | AL | . 100 rows × 2 columns . most_freq_states = top_100_cities.State.value_counts() most_freq_states . CA 35 FL 13 TX 5 NY 4 OR 4 MI 3 VA 3 LA 3 PA 3 SC 2 MO 2 UT 2 AZ 2 TN 2 MN 2 NC 2 OH 2 OK 1 NJ 1 MD 1 KY 1 IN 1 CO 1 DC 1 WA 1 IL 1 GA 1 AL 1 Name: State, dtype: int64 . most_freq_states.plot(kind = &quot;bar&quot;, figsize = (15,5)) plt.xlabel(&quot;States&quot;) plt.ylabel(&quot;Frequency in top 100 cities by number of accidents&quot;) plt.title(&quot;Most frequent states in the top 100 cities in the number of accidents&quot;) plt.show() . Q4. What time of the day are accidents most frequent in? . Do more accidents tend to occur at a particular time of the day? . us_accidents.Start_Time.dt.hour.plot(kind = &#39;hist&#39;, bins = 24) plt.xlabel(&quot;Hour of the day&quot;) plt.show() . It appears that accidents tend to occur more in the morning between 7-9 and in the afternoon between 13-17. Office hours could be the reason behind this trend. We can examine this by separating the data for weekdays and weekends. . weekdays = us_accidents.Start_Time.dt.dayofweek &lt; 5 us_accidents.loc[weekdays].Start_Time.dt.hour.plot(kind = &#39;hist&#39;, bins = 24) plt.xlabel(&quot;Hour of the day&quot;) plt.title(&quot;Accidents frequency of Weekdays&quot;) plt.show() . us_accidents.loc[~weekdays].Start_Time.dt.hour.plot(kind = &#39;hist&#39;, bins = 24) plt.xlabel(&quot;Hour of the day&quot;) plt.title(&quot;Accidents frequency of Weekends&quot;) plt.show() . On weekends, there seems to be a breakaway from the pattern that we saw earlier, which supports the idea that the trend we saw earlier could be due to the traffic resulting from office timings, which are usually closed on weekends. On weekends, accidents tend to occur more during daylight, which should be due to more traffic during those hours. . Q5. Which days of the week have the most accidents? . sns.histplot(us_accidents.Start_Time.dt.dayofweek, bins = 7) plt.xlabel(&quot;Day of Week&quot;) plt.show() . Accidents occur more on weekdays and there is a sharp drop in their count on the weekends. . Q6. Which months have the most accidents? . sns.histplot(us_accidents.Start_Time.dt.month, bins = 12) plt.xlabel(&quot;Month&quot;) plt.show() . Later months of the year appear to witness more accidents. This cannot be due to temperature(winter season) because the count drops suddenly in the starting months of the year. Due to there being no other apparent cause, it demands further analysis. . We can start by looking at the trend year over year. . fig, axes = plt.subplots(2, 3, figsize = (15,10), ) year = 2016 for r in range(2): for c in range(3): if year &lt; 2021: year_data = us_accidents.loc[us_accidents.Start_Time.dt.year == year] sns.histplot(year_data.Start_Time.dt.month, bins = 12, ax = axes[r, c]) axes[r, c].set_title(year) axes[r, c].set_xlabel(&quot;Month of the year&quot;) year += 1 . We can see that probably because the data was started being collected in the year 2016, the starting months have a much lower number of datapoints. Also, in the year 2020, because of the coronavirus pandemic lockdown restrictions, there was a suddent drop in the middle of the year. And as the restrictions eased, more accidents started to occur. But this data requires further more analysis on the month wise trends. . Q7. What is the trend of accidents year over year (decreasing/increasing?) . us_accidents.Start_Time.dt.year.plot(kind = &#39;hist&#39;, bins = 5, title = &quot;Year wise trend&quot;, xticks = np.arange(2016, 2021), figsize = (7, 5)) plt.show() . The number of accidents year over year looks to be increasing. This can be attributed to better data collection in the later years. Thus, it need further analysis. . Summary and Conclusion .",
            "url": "https://ncitshubham.github.io/blogs/2021/04/15/exploratory-data-analysis-of-road-accidents-in-usa.html",
            "relUrl": "/2021/04/15/exploratory-data-analysis-of-road-accidents-in-usa.html",
            "date": " • Apr 15, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "EDA and Data Visualization of Zomato Dataset",
            "content": "Introduction . [Note: This notebook was originally published on Kaggle here] . Zomato is an Indian multinational restaurant aggregator and food delivery company. In this project, we&#39;re going to study and analyze the Zomato Dataset shared by Himanshu Podder on Kaggle. This dataset contains information on restaurants in the city of Bengaluru, India. We can use this dataset to get an idea of different factors affecting the restaurants in different parts of the city and also answer questions like which type of food is most popular in the city, how does the location of the restuarant affects its rating on the Zomato platform, and what relation does the rating of the restaurant and the number of cuisines it offers has? . We will use the approach of Exploratory Data Analysis (EDA) for studying this data, which is used to analyse datasets to summarize their main characteristics, often using statistical graphics and other data visualization methods. EDA can help in seeing what the data can tell us beyond the formal modeling or hypothesis testing task.3 . The dataset can be used to answer a lot of questions but for this project, we are going to limit ourselves to 5-10 questions and then try to answer these questions. . Setup . Import the required libraries and get the file path. . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt # Data Visualization import seaborn as sns # Data Visualization import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . file_path = &quot;/kaggle/input/zomato-bangalore-restaurants/zomato.csv&quot; df = pd.read_csv(file_path, thousands = &#39;,&#39;) # the thousands parameter helps to remove commas from numeric values. . Data Preparation and Cleaning . Premilinary Analysis . Evaluate the structure of the dataset . df.head() . df.info() . Change the structure . We&#39;ll drop the columns which don&#39;t inform us much about the restuarants. . df.drop([&quot;url&quot;, &quot;name&quot;, &quot;phone&quot;, &quot;reviews_list&quot;, &quot;address&quot;, &quot;menu_item&quot;], axis = 1, inplace = True) df.columns . We&#39;ll also rename some of the columns. . df.rename(mapper = {&quot;listed_in(type)&quot;: &quot;type&quot;, &quot;approx_cost(for two people)&quot;: &quot;cost_for_two_people&quot;, &quot;rate&quot;: &quot;rating&quot;}, axis = 1, inplace = True) df.columns . Correcting datatypes . Now we&#39;ll look at the dtypes of the DataFrame and see if that needs any change. . df.dtypes . We need to change the rating column to numeric dtype. . df.rating.unique() . df = df.loc[df.rating != &quot;NEW&quot;] df = df.loc[df.rating != &quot;-&quot;] # Select the first 3 characters and convert the column to numeric df.rating = pd.to_numeric(df.rating.str[:3]) . df.rating.head() . Deal with null values . df.isnull().sum().sort_values(ascending = False) . It appears that in all the columns with null values, absence of values neither indicates the value of zero nor informs us on something useful. Thus, it&#39;s better to drop the rows with null values. In the dish_liked column, because the null values account for about half of the data, it&#39;s better to drop the whole column. . df.dropna(subset = [&quot;location&quot;, &quot;rating&quot;, &quot;rest_type&quot;, &quot;cuisines&quot;, &quot;cost_for_two_people&quot;], inplace = True) df.drop([&quot;dish_liked&quot;], axis = 1, inplace = True) df.info() . df.head() . Extras . We&#39;ll also convert the values in rest_type and cuisines columns to lists. . df.cuisines = df.cuisines.str.split(&quot;,&quot;) df.rest_type = df.rest_type.str.split(&quot;,&quot;) . df.head() . Exploratory Analysis and Visualization . With these columns in mind, we will try to answer the following questions from the dataset: . What locations are most popular for restaurants in Benagluru? | Which locations have the best rated restaurants? | What relation does the rating and number of votes that a restaurant receives have? What about table booking and online order facility? | Is a restaurant which offers online order facility rated better? | Are restaurants offering expensive food rated better? Does a table booking facility make a difference? | Does the number of cuisines that a restaurant provides have a relation to the rating it recieves? | How do Casual Dining and Fine Dining restaurants differ in their rating? | How do Casual Dining and Fine Dining restaurants differ in their cost_for_two_people? | What are the number of different types of restaurants? | Q1. What locations are most popular for restaurants in Benagluru? . popular_locations = df.location.value_counts().head(15) popular_locations . plt.figure(figsize = (10, 8)) sns.barplot(x = popular_locations, y = popular_locations.index) . The 5 most popular locations for restaurants are BTM, Koramangala 5th Block, HSR, Indiranagar, and JP Nagar, with BTM boasting of nearly 4000 eateries. . Q2. Which locations have the best rated restaurants? . (We&#39;ll select only those locations which have a minimum of 50 restaurants.) . location_rating = df.groupby(by = [&quot;location&quot;])[&quot;rating&quot;].agg([&quot;count&quot;, &quot;mean&quot;]) location_rating.head() . rated_locations = location_rating.loc[location_rating[&quot;count&quot;] &gt;= 50].sort_values(by = &quot;mean&quot;, ascending = False) # select the top 20 locations. top20_rated_locations = rated_locations[:20] top20_rated_locations.head() . plt.figure(figsize = (15, 5)) sns.barplot(x = top20_rated_locations.index, y = top20_rated_locations[&quot;mean&quot;]) plt.xticks(rotation = 45) plt.ylim(3.5, 4.3) plt.show() . These are the top 20 locations in Bengaluru based on the eatries&#39; ratings. The average rating in these locations range from 4.14 and 3.8. . Q3. What relation does the rating and number of votes that a restaurant receives have? What about table booking and online order facility? . plt.figure(figsize = (8, 5)) sns.scatterplot(x = &quot;rating&quot;, y = &quot;votes&quot;, hue = &quot;book_table&quot;, data = df, s = 40) . The number of votes look to be directly correlated with the rating of the restaurant, and they look to increase exponentially with the rating after a critical point. This is to be expected because better restaurants would atrract more customers and thus more votes. . Also, the restaurants which don&#39;t provide booking facility are clustered in low ratings and less number of votes. In other words, more popular restaurants with high ratings are more likely to offer table booking facility, which can also be seen the following graph. . sns.boxplot(x = &quot;book_table&quot;, y = &quot;rating&quot;, data = df) . Now, we can look at the scatterplot for rating, votes and online_order . plt.figure(figsize = (8, 5)) sns.scatterplot(x = &quot;rating&quot;, y = &quot;votes&quot;, hue = &quot;online_order&quot;, data = df, s = 40) . The data points for online order facility are scattered in the graph, and thus the data doesn&#39;t reveal any relationship between these variables. But, it may be worth exploring its relationship with the votes and the rating individually, which we&#39;ll do in the following sections. . Q4. Is a restaurant which offers online order facility rated better? . sns.violinplot(x = &quot;online_order&quot;, y = &quot;rating&quot;, data = df) . As discussed in the previous question, the rating doesn&#39;t seem to be any different between the restaurants which offer online order facility and which don&#39;t. . Q5. Are restaurants offering expensive food rated better? Does a table booking facility make a difference? . plt.figure(figsize = (8, 5)) sns.scatterplot(x = &quot;rating&quot;, y = &quot;cost_for_two_people&quot;, data = df, hue = &#39;book_table&#39;) . Although there are hints of an exponential relationship between the cost for two people and the rating of a restaurant, most of the data is clustered in low cost, and thus because of lack of data points for expensive restaurants, the data is inconclusive for this relations. But we can study book table facility individuallly with the cost for two people, where restaurants which offer book_table facility seem to be more expensive than restaurants which don&#39;t. . sns.boxplot(x = &quot;book_table&quot;, y = &quot;cost_for_two_people&quot;, data = df) . This graph also supports the idea that table booking is correlated with the cost for two people. . Q6. Does the number of cuisines that a restaurant provides have a relation to the rating it recieves? . The cuisines column shows the all the cuisines that a restaurant offers. We can add column to the DataFrame to store the number of cuisines that a restaurant offers. . df[&quot;no_of_cuisines&quot;] = df.cuisines.str.len() df[&quot;no_of_cuisines&quot;].head() . plt.figure(figsize = (8, 5)) sns.stripplot(x = &quot;no_of_cuisines&quot;, y = &quot;rating&quot;, data = df) . The rating seems to become more concentrated towards mean as the no. of cuisines that a restaurant offers goes up. But, this could also be a artifact of low no. of restaurants offering higher no. of cuisines. In general, the mean of rating also seems to go up with the increase in no. of cuisines, but the graph is inconclusive. We&#39;ll explore this more in the boxplot. . sns.boxplot(x = &quot;no_of_cuisines&quot;, y = &quot;rating&quot;, data = df) . This graph reflects the relationship better and does support the idea that restaurants offering more no of cuisines are usually rated better. . Q7. How do Casual Dining and Fine Dining restaurants differ in their cost_for_two_people? . df.head() . We&#39;ll need to explode the rest_type column to extract information. . rest_type_exploded = df.explode(column = &quot;rest_type&quot;) rest_type_exploded[&quot;rest_type&quot;] = rest_type_exploded[&quot;rest_type&quot;].str.strip() rest_type_exploded.head() . fine_dining_rest = rest_type_exploded.loc[rest_type_exploded.rest_type == &quot;Fine Dining&quot;] casual_dining_rest = rest_type_exploded.loc[rest_type_exploded.rest_type == &quot;Casual Dining&quot;] . sns.kdeplot(fine_dining_rest.cost_for_two_people, fill = True) sns.kdeplot(casual_dining_rest.cost_for_two_people, fill = True) plt.legend([&quot;Fine Dining&quot;, &quot;Casual Dining&quot;]) plt.show() . As expected, fine dining restaurants are much more expensive than casual dining restaurants. . Q8. How do Casual Dining and Fine Dining restaurants differ in their rating? . sns.kdeplot(fine_dining_rest.rating, fill = True) sns.kdeplot(casual_dining_rest.rating, fill = True) plt.legend([&quot;Fine Dining&quot;, &quot;Casual Dining&quot;]) plt.show() . Fine dining restaurants are usually rated better and their ratings show much less variance than the ratings of casual dining restaurants. . Q9. What are the five most common types of restaurants? . most_common_types_of_restaurants = rest_type_exploded.rest_type.value_counts() most_common_types_of_restaurants.head() . sns.barplot(x = most_common_types_of_restaurants.head(), y = most_common_types_of_restaurants.index[:5]) plt.show() . Q10. What are the top 5 rated restaurants in type and no_of_cuisines combined? . In other words, which type and no_of_cuisines combinations gather the highest ratings? . rating_data = df.groupby(by = [&quot;type&quot;, &quot;no_of_cuisines&quot;])[&quot;rating&quot;].agg(&quot;mean&quot;) rating_data.head() . Now, we&#39;ll make a 2D datarame out of this multiindexed pandas Series. . rating_data_df = rating_data.unstack() rating_data_df.head() . plt.figure(figsize = (9, 7)) fig = sns.heatmap(data = rating_data_df, annot = True, cmap = &quot;rocket_r&quot;) fig.set(xlabel = &quot;No. of cuisines&quot;, ylabel = &quot;Type&quot;) plt.show() . From the plot, Pubs and bars which offer more than 3 cuisines are all rated high. Similarly, Drinks &amp; nightlife restaurants with multiple cuisines are also rated really high. . We can get the top 5 combinations from the rating_data Series. . rating_data.sort_values(ascending = False).head() . Q11. What is the relationship between the type and cost_for_two_people? . plt.figure(figsize = (8, 6)) sns.boxplot(x = &quot;type&quot;, y = &quot;cost_for_two_people&quot;, data = df) plt.xticks(rotation = 45) plt.show() . First thing to note is that there are quite a few outliers in the data, almost all of them offering much more expensive food from the rest of the distribution. Also buffet, drinks &amp; nighlife, and pubs are much more expensive than eateries of the type desserts and delivery. . Summary and Conclusion . Many questions could be asked and explored from the Zomato Dataset, but here we tried to answer 11 of them. We studied all the restaurants in Bengaluru, who have registered on Zomato, and tried to explore multiple variables&#39; relationship with the restaurants&#39; ratings. We also studied what factors go along with the food cost for two people in these restaurants. . There are two important things to note here before taking any conclusions. First, all the conclusions we made in our analysis might apply only to restaurants registered on Zomato and other similar online platforms, and might differ significantly if we explore the food industry offline. Second really important thing is all the relationships that we studied are correlational in nature. This project thus does not establish causal relationships, although it might suggest some and can be taken as an inspiration to conduct actual experimental studies to explore the variables discussed here. Keeping in mind this, we can look at what we did establish in the EDA of this Zomato Dataset. . j jj] j k k . Apart from thes inferences, many more interesting relationships can be studied and should be explored from this data. What we&#39;ll do is we&#39;ll try to predict the zomato rating of a restaurant from this dataset here. . [Note: This is Part 1 of a two-part project] .",
            "url": "https://ncitshubham.github.io/blogs/2021/02/23/eda-and-data-visualization-of-zomato-dataset.html",
            "relUrl": "/2021/02/23/eda-and-data-visualization-of-zomato-dataset.html",
            "date": " • Feb 23, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ncitshubham.github.io/blogs/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a Data Analyst and a Software Developer in Python. .",
          "url": "https://ncitshubham.github.io/blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ncitshubham.github.io/blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}